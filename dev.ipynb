{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "config.update(\"jax_platform_name\", \"cpu\")\n",
    "\n",
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\".8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import networkx as nx\n",
    "import jaxley as jx\n",
    "from jaxley.io.swc import read_swc\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODOS:\n",
    "\n",
    "### change order of nodes and edges\n",
    "- Nodes: idx, indices (int), groups (bool), morph_attrs, channel_attrs, other_attrs (set_by)\n",
    "- Edges: idx, indices (int), synapses (bool), morph_attrs, channel_attrs, other_attrs (set_by)\n",
    "\n",
    "### Reformat dataframes\n",
    "- rename `global_X_index` to `X_index`\n",
    "- use tuples of nodes to index edges\n",
    "- treat synapses like groups and channels, i.e. boolean indices\n",
    "\n",
    "### Pass tests\n",
    "- pass import / export cycle tests\n",
    "- pass `test_swc.py`\n",
    "\n",
    "### Remove group_names from global graph attrs\n",
    "- group_names can be inferred from nodes, i.e. names of boolean cols which are not channels.\n",
    "\n",
    "\n",
    "### Keep branchpoints and tip nodes in `nodes` and in graph\n",
    "- introduce `compartments` attribute that only shows compartment nodes? Nodes shows all nodes `cell.nodes.loc[is_comp]`\n",
    "\n",
    "### Refactor graph editing functions in `morph_utils`\n",
    "- with the new graph functionality, this should be much easier to do\n",
    "\n",
    "### Refactor `compute_xyz` function\n",
    "- replace old `compute_xyz` with new version\n",
    "\n",
    "### Refactor `set_ncomp`\n",
    "- `set_ncomp` should be much simpler via `branch_comps_from_nodes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jaxley.io.graph as graph_io_old\n",
    "import jaxley.io.tmp as graph_io_new\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import jax\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1458,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from matplotlib.patches import Circle\n",
    "\n",
    "def graph_vis(graph, dims=(0, 1), ax=None, show_radii=False, jitter=0.0, scale_radii=1.0, radii_kwargs={}, **kwargs):\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "\n",
    "    add_jitter = lambda x: x + np.random.uniform(-jitter, jitter, size=len(x))\n",
    "    pos = {\n",
    "        k: add_jitter(np.array([data[\"xyz\"[d]] for d in dims]))\n",
    "        for k, data in graph.nodes(data=True)\n",
    "    }\n",
    "\n",
    "    nx.draw(graph, pos=pos, ax=ax, **kwargs)\n",
    "\n",
    "    if show_radii:\n",
    "        patch_kwargs = dict(edgecolor=\"C3\", facecolor=\"none\", linewidth=1.0, zorder=3)\n",
    "        patch_kwargs.update(radii_kwargs)\n",
    "\n",
    "        for k, data in graph.nodes(data=True):\n",
    "            r = data.get(\"radius\", data.get(\"r\", None))\n",
    "            R = float(r) * float(scale_radii)\n",
    "            ax.add_patch(Circle(pos[k], R, **patch_kwargs))\n",
    "\n",
    "        ax.set_aspect(\"equal\", adjustable=\"box\")\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from neuron import h\n",
    "from neuron import rxd\n",
    "import jaxley.io.graph as graph_io_old\n",
    "import jaxley.io.tmp as graph_io_new\n",
    "from scipy.spatial.distance import cdist\n",
    "from typing import Callable, Optional\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import pandas as pd\n",
    "\n",
    "unpack_dict = lambda d, keys: np.array([d[k] for k in keys])\n",
    "\n",
    "def neuron_swc_reader(fname: str, ncomp: int = 1):\n",
    "    \"\"\"\n",
    "    Load a SWC file and return a NEURON cell object.\n",
    "\n",
    "    Uses `h.Import3d_SWC_read()` to load the SWC file.\n",
    "\n",
    "    Args:\n",
    "        fname: The path to the SWC file.\n",
    "        ncomp: The number of compartments per segment.\n",
    "    \"\"\"\n",
    "    # Load NEURON stdlib and import3d\n",
    "    h.load_file(\"stdlib.hoc\")\n",
    "    h.load_file(\"import3d.hoc\")\n",
    "\n",
    "    # Clear existing sections\n",
    "    for sec in h.allsec():\n",
    "        h.delete_section(sec=sec)\n",
    "\n",
    "    # Load and instantiate SWC\n",
    "    cell = h.Import3d_SWC_read()\n",
    "    cell.input(fname)\n",
    "    i3d = h.Import3d_GUI(cell, False)\n",
    "    i3d.instantiate(None)\n",
    "\n",
    "    for sec in h.allsec():\n",
    "        sec.nseg = ncomp\n",
    "\n",
    "    return cell\n",
    "\n",
    "\n",
    "def neuron_swc_graph() -> nx.DiGraph:\n",
    "    \"\"\"\n",
    "    Reads from the global `h.allsec()` and writes the attributes to a nx.DiGraph. The \n",
    "    edges are created by connecting the first point of a section to the last point of the\n",
    "    parent section and connecting up each point (line in the SWC file) within a section.\n",
    "    Each n3d point becomes a node in the graph. \n",
    "\n",
    "    The graph is comparable to Jaxley's SWC-Graph and can be used to compare / debug \n",
    "    Jaxley's and NEURON's SWC readers.\n",
    "    \n",
    "    The node attributes are:\n",
    "    - node: The node index.\n",
    "    - id: The SWC type ID.\n",
    "    - x: The x-coordinate.\n",
    "    - y: The y-coordinate.\n",
    "    - z: The z-coordinate.\n",
    "    - r: The radius.\n",
    "    - parent_id: The index of the parent node.\n",
    "    - sec_name: The name of the section.\n",
    "\n",
    "    Returns:\n",
    "        A nx.DiGraph with the node and edge attributes.\n",
    "    \"\"\"\n",
    "    points = []\n",
    "    id_counter = 1  # SWC IDs start at 1\n",
    "\n",
    "    # Map section types to SWC type IDs\n",
    "    type_map = {\n",
    "        'soma': 1,\n",
    "        'axon': 2,\n",
    "        'dend': 3,\n",
    "        'apic': 4\n",
    "    }\n",
    "\n",
    "    # Keep track of last point ID in each section for connectivity\n",
    "    section_last_point_id = {}\n",
    "\n",
    "    for sec in h.allsec():\n",
    "        sec_name = sec.name()\n",
    "        # Determine SWC type by checking the prefix of section name\n",
    "        swc_type = 3  # default dendrite\n",
    "        for key in type_map:\n",
    "            if sec_name.startswith(key):\n",
    "                swc_type = type_map[key]\n",
    "                break\n",
    "\n",
    "        n3d = int(h.n3d(sec=sec))\n",
    "        if n3d == 0:\n",
    "            continue\n",
    "\n",
    "        for i in range(n3d):\n",
    "            x = h.x3d(i, sec=sec)\n",
    "            y = h.y3d(i, sec=sec)\n",
    "            z = h.z3d(i, sec=sec)\n",
    "            radius = h.diam3d(i, sec=sec) / 2\n",
    "\n",
    "            if i == 0:\n",
    "                # Parent id is the last point id of the parent section if exists\n",
    "                sec_ref = h.SectionRef(sec=sec)\n",
    "                if sec_ref.has_parent():\n",
    "                    parent_sec = sec_ref.parent\n",
    "                    parent_point_id = section_last_point_id.get(parent_sec.name(), -1)\n",
    "                else:\n",
    "                    parent_point_id = -1\n",
    "            else:\n",
    "                # Parent is previous point in the same section\n",
    "                parent_point_id = id_counter - 1\n",
    "\n",
    "            points.append({\n",
    "                'node': id_counter,\n",
    "                'id': swc_type,\n",
    "                'x': x,\n",
    "                'y': y,\n",
    "                'z': z,\n",
    "                'r': radius,\n",
    "                'parent_id': parent_point_id,\n",
    "                'sec_name': sec_name,\n",
    "            })\n",
    "            id_counter += 1\n",
    "\n",
    "        # Save last point id of this section for parent linkage\n",
    "        section_last_point_id[sec_name] = id_counter - 1\n",
    "\n",
    "    nodes_df = pd.DataFrame(points)\n",
    "    nodes_df.set_index(\"node\", inplace=True)\n",
    "    nodes_df.index.name = None\n",
    "\n",
    "    edges_df = nodes_df[[\"parent_id\"]].copy()\n",
    "    edges_df[\"target\"] = nodes_df.index\n",
    "    edges_df = edges_df[edges_df[\"parent_id\"] != -1]\n",
    "    edges_df.set_index([\"parent_id\", \"target\"], inplace=True)\n",
    "    edges_df.index.names = (None, None)\n",
    "\n",
    "    nodes_df.drop(columns=[\"parent_id\"], inplace=True)\n",
    "\n",
    "    return graph_io_new.pandas_to_nx(nodes_df, edges_df, pd.Series())\n",
    "\n",
    "\n",
    "def neuron_comp_graph(ncomp: int = 1) -> nx.DiGraph:\n",
    "    \"\"\"\n",
    "    Reads from the global `h.allsec()` and constructs a compartment graph from the ingested\n",
    "    SWC file. Each section is divided into `ncomp` compartments and each compartment will\n",
    "    become a node in the graph. Each node will contain the attributes that are used to\n",
    "    simulate each compartment.\n",
    "\n",
    "    The graph is comparable to Jaxley's Compartment-Graph and can be used to compare / debug \n",
    "    Jaxley's and NEURON's SWC readers. Especially how NEURON and Jaxley handle computation\n",
    "    of the compartment / segment attributes, like radius, volume, surface area, etc.\n",
    "\n",
    "    The node attributes are:\n",
    "    - comp_index: The compartment index.\n",
    "    - seg_name: The name of the segment.\n",
    "    - sec_name: The name of the section.\n",
    "    - x: The x-coordinate.\n",
    "    - y: The y-coordinate.\n",
    "    - z: The z-coordinate.\n",
    "    - radius: The radius.\n",
    "    - area: The area.\n",
    "    - surface_area: The surface area.\n",
    "    - volume: The volume.\n",
    "    - length: The length.\n",
    "    - groups: The groups.\n",
    "\n",
    "    Returns:\n",
    "        A nx.DiGraph with the node and edge attributes.\n",
    "    \"\"\"\n",
    "    for sec in h.allsec():\n",
    "        sec.nseg = ncomp\n",
    "\n",
    "    data = []\n",
    "    segment_indices = {}\n",
    "    compartment_index = 0\n",
    "\n",
    "    type2group = {\n",
    "        \"soma\": \"soma\",\n",
    "        \"axon\": \"axon\",\n",
    "        \"dend\": \"basal\",\n",
    "        \"apic\": \"apical\"\n",
    "    }\n",
    "\n",
    "    # Dummy cytosolic region\n",
    "    cyt = rxd.Region(h.allsec(), name='cyt')\n",
    "\n",
    "    # Create a dummy species to get access to segment volumes\n",
    "    ca = rxd.Species(cyt, name='ca', d=0)\n",
    "\n",
    "    seg_surface_areas = {str(node.segment): node.surface_area for node in ca.nodes}\n",
    "    seg_volumes = {str(node.segment): node.volume for node in ca.nodes}\n",
    "\n",
    "    # First pass: collect node data and assign segment indices\n",
    "    for sec in h.allsec():\n",
    "        centers = np.linspace(0, 1, 2*sec.nseg+1)[1:-1:2]\n",
    "        sec_name = sec.name()\n",
    "\n",
    "        segment_indices[sec] = []\n",
    "\n",
    "        for seg_idx, seg in enumerate(sec):\n",
    "            x = seg.x\n",
    "            pt3d_sec = h.SectionRef(sec=sec).sec\n",
    "            n3d = int(h.n3d(sec=pt3d_sec))\n",
    "\n",
    "            if n3d < 2:\n",
    "                continue\n",
    "\n",
    "            arc_lengths = [h.arc3d(i, sec=pt3d_sec) for i in range(n3d)]\n",
    "            xs = [h.x3d(i, sec=pt3d_sec) for i in range(n3d)]\n",
    "            ys = [h.y3d(i, sec=pt3d_sec) for i in range(n3d)]\n",
    "            zs = [h.z3d(i, sec=pt3d_sec) for i in range(n3d)]\n",
    "\n",
    "            total_length = arc_lengths[-1]\n",
    "            target = x * total_length\n",
    "\n",
    "            for i in range(n3d - 1):\n",
    "                if arc_lengths[i] <= target <= arc_lengths[i + 1]:\n",
    "                    denom = arc_lengths[i + 1] - arc_lengths[i]\n",
    "                    if denom == 0:\n",
    "                        # Use midpoint or skip (depending on your preference)\n",
    "                        seg_x_pos = xs[i]\n",
    "                        seg_y_pos = ys[i]\n",
    "                        seg_z_pos = zs[i]\n",
    "                    else:\n",
    "                        frac = (target - arc_lengths[i]) / denom\n",
    "                        seg_x_pos = xs[i] + frac * (xs[i + 1] - xs[i])\n",
    "                        seg_y_pos = ys[i] + frac * (ys[i + 1] - ys[i])\n",
    "                        seg_z_pos = zs[i] + frac * (zs[i + 1] - zs[i])\n",
    "                    break\n",
    "            else:\n",
    "                seg_x_pos = xs[-1]\n",
    "                seg_y_pos = ys[-1]\n",
    "                seg_z_pos = zs[-1]\n",
    "\n",
    "            radius = seg.diam / 2\n",
    "            area = h.area(x, sec=sec)\n",
    "            length = sec.L / sec.nseg\n",
    "            type_name = sec_name.split(\"[\")[0]\n",
    "            seg_name = str(sec(centers[seg_idx]))\n",
    "\n",
    "            data.append({\n",
    "                \"comp_index\": compartment_index,\n",
    "                \"seg_name\": str(sec(centers[seg_idx])),\n",
    "                \"sec_name\": sec_name,\n",
    "                \"x\": seg_x_pos,\n",
    "                \"y\": seg_y_pos,\n",
    "                \"z\": seg_z_pos,\n",
    "                \"radius\": radius,\n",
    "                \"area\": area,\n",
    "                \"surface_area\": seg_surface_areas[seg_name],\n",
    "                \"volume\": seg_volumes[seg_name],\n",
    "                \"length\": length,\n",
    "                \"groups\": [type2group.get(type_name, \"unknown\")]\n",
    "            })\n",
    "\n",
    "            segment_indices[sec].append(compartment_index)\n",
    "            compartment_index += 1\n",
    "\n",
    "    # Build edge list\n",
    "    seg_edges = [(str(s.parentseg().sec) if s.parentseg() else None, str(s)) for s in h.allsec()]\n",
    "\n",
    "    # Create DataFrames\n",
    "    nodes_df = pd.DataFrame(data).set_index(\"comp_index\")\n",
    "    edges_df = pd.DataFrame(seg_edges, columns=[\"source\", \"target\"])\n",
    "    edges_df = edges_df[edges_df[\"source\"].notna()]\n",
    "\n",
    "    name_to_index = dict(zip(nodes_df[\"sec_name\"], nodes_df.index))\n",
    "    edges_df['source'] = edges_df['source'].map(name_to_index)\n",
    "    edges_df['target'] = edges_df['target'].map(name_to_index)\n",
    "    edges_df.set_index([\"source\", \"target\"], inplace=True)\n",
    "    edges_df.index.names = [None, None]\n",
    "\n",
    "    return graph_io_new.pandas_to_nx(nodes_df, edges_df, pd.Series())\n",
    "\n",
    "def neuron_list_branches(swc_graph: nx.DiGraph) -> list[list[int]]:\n",
    "    \"\"\"Creates a list of branches / sections from a NEURON SWC graph.\n",
    "\n",
    "    This returns the compartment indices (same as the node indices in the compartment graph)\n",
    "    for each branch / section in the NEURON SWC graph.\n",
    "\n",
    "    Args:\n",
    "        swc_graph: A NEURON SWC graph (created with `neuron_swc_graph()`).\n",
    "\n",
    "    Returns:\n",
    "        A list of lists, where each sublist contains the compartment indices for a branch\n",
    "        and the index of each list corresponds to the branch index.\n",
    "    \"\"\"\n",
    "    secs = nx.get_node_attributes(swc_graph, \"sec_name\")\n",
    "    branches = {k: [] for k in secs.values()}\n",
    "    for k, v in secs.items():\n",
    "        branches[v].append(k)\n",
    "    return list(branches.values())\n",
    "\n",
    "def old_list_branches(swc_graph, ignore_swc_tracing_interruptions=True):\n",
    "    traced_branches = []\n",
    "    mod_graph, branches, *_ = graph_io_old._trace_branches(swc_graph, ignore_swc_tracing_interruptions=ignore_swc_tracing_interruptions)\n",
    "    for branch in branches:\n",
    "        traced_branches.append(np.unique(branch[:,:-1].flatten().astype(int)).tolist())\n",
    "    return traced_branches, mod_graph\n",
    "\n",
    "def match_branches(input_swc_graph: nx.DiGraph, target_swc_graph: nx.DiGraph, input_branches: list[list[int]], target_branches: list[list[int]], match_on: str = \"xyz\", tol: float = 1e-3) -> list[list[int]]:\n",
    "    \"\"\"Matches the branches in the input graph to the branches in the target graph.\n",
    "\n",
    "    The returned branch list is ordered in the same way as the target branch list. In\n",
    "    addition, the branches are made to have the same direction as the target branches.\n",
    "\n",
    "    Args:\n",
    "        input_swc_graph: A graph representation of a SWC file.\n",
    "        target_swc_graph: A graph representation of a SWC file.\n",
    "        input_branches: A list of lists, where each sublist contains the compartment indices\n",
    "            for a branch in the input graph.\n",
    "        target_branches: A list of lists, where each sublist contains the compartment indices\n",
    "            for a branch in the target graph.\n",
    "        match_on: The attributes to match on, i.e. [\"x\", \"y\", \"z\"] to match on the x, y, \n",
    "            and z coordinates.\n",
    "        tol: The tolerance for the matching, i.e. 1e-3 considers two nodes to be the same \n",
    "            if the distance between them is less than 1e-3.\n",
    "\n",
    "    Returns:\n",
    "        The input branches reordered to match the target branches.\n",
    "    \"\"\"\n",
    "    assert len(input_branches) == len(target_branches), \"Number of branches is not the same\"\n",
    "    \n",
    "    input_node_xyz = np.array([unpack_dict(d, match_on) for n, d in input_swc_graph.nodes(data=True)])\n",
    "    target_node_xyz = np.array([unpack_dict(d, match_on) for n, d in target_swc_graph.nodes(data=True)])\n",
    "\n",
    "    # find the closest nodes in the input graph and target graph\n",
    "    dists = cdist(input_node_xyz, target_node_xyz)\n",
    "    is_close = np.isclose(dists, 0, atol=tol)\n",
    "    input_idx, target_idx = np.where(is_close)\n",
    "\n",
    "    # align input and target nodes\n",
    "    input_nodes = np.array(list(input_swc_graph.nodes()))[input_idx]\n",
    "    target_nodes = np.array(list(target_swc_graph.nodes()))[target_idx]\n",
    "    # create a mapping from the input nodes to the target nodes\n",
    "    input2target = {n_i: n_t for n_i, n_t in zip(input_nodes, target_nodes)}\n",
    "\n",
    "    # branch index of the target nodes\n",
    "    branch_of_target_node = {n: np.where([n in b for b in target_branches])[0] for n in sum(target_branches, [])}\n",
    "    # corresponding branch index of the input nodes in the target graph\n",
    "    branch_targets = [np.hstack([branch_of_target_node[input2target[n]] for n in b]) for b in input_branches]\n",
    "    # each node in the input graph, i.e. [[1,2,3,4,5], [...]] has corresponding \n",
    "    # branch index in the target graph, i.e. [[0,1,1,1,1,2], [...]] The \n",
    "    # corresponding branch index will be assigned based on the most common target branch\n",
    "    # index for the input nodes, i.e. branch 0 in the input will be assigned to branch 1\n",
    "    # in the target graph.\n",
    "\n",
    "    # reorder the input branches to match the target branches\n",
    "    new_order = np.argsort([max(set(b), key=b.tolist().count) for i, b in enumerate(branch_targets)])\n",
    "    input_branches = [input_branches[i] for i in new_order]\n",
    "\n",
    "    # reverse node order of the input branches if they are not in the same direction as \n",
    "    # the target branches\n",
    "    for i, (b_in, b_target) in enumerate(branch_of_target_node.items()):\n",
    "        b_in_start_xyz = unpack_dict(input_swc_graph.nodes[b_in[0]], \"xyz\")\n",
    "        b_in_end_xyz = unpack_dict(input_swc_graph.nodes[b_in[-1]], \"xyz\")\n",
    "        b_target_start_xyz = unpack_dict(target_swc_graph.nodes[b_target[0]], \"xyz\")\n",
    "        dist_start2start = np.linalg.norm(b_in_start_xyz - b_target_start_xyz)\n",
    "        dist_end2start = np.linalg.norm(b_in_end_xyz - b_target_start_xyz)\n",
    "        if dist_start2start > dist_end2start:\n",
    "            input_branches[i] = input_branches[i][::-1]\n",
    "\n",
    "    return input_branches\n",
    "\n",
    "def match_nodes(input_graph: nx.DiGraph, target_graph: nx.DiGraph, match_on: str = \"xyzr\", tol: float = 1e-3, assert_unique: bool = True) -> nx.DiGraph:\n",
    "    \"\"\"Matches the nodes in the input graph to the nodes in the target graph.\n",
    "\n",
    "    The returned graph is relabeled such that the corresponding nodes in the input and target\n",
    "    graph have the same node indices.\n",
    "\n",
    "    Args:\n",
    "        input_graph: A graph (either a SWC graph or a compartment graph).\n",
    "        target_graph: A graph (either a SWC graph or a compartment graph).\n",
    "        match_on: The attributes to match on, i.e. [\"x\", \"y\", \"z\"] to match on the x, y, \n",
    "            and z coordinates.\n",
    "        tol: The tolerance for the matching, i.e. 1e-3 considers two nodes to be the same \n",
    "            if the distance between them is less than 1e-3.\n",
    "        assert_unique: Whether to assert that the nodes are matched uniquely. If False,\n",
    "            the nodes not all nodes will have a unique match.\n",
    "\n",
    "    Returns:\n",
    "        The input graph relabeled to have the same node indices as the target graph.\n",
    "    \"\"\"\n",
    "    input_nodes = np.array(list(input_graph.nodes()))\n",
    "    target_nodes = np.array(list(target_graph.nodes()))\n",
    "\n",
    "    input_node_xyz = np.array([unpack_dict(d, match_on) for n, d in input_graph.nodes(data=True)])\n",
    "    target_node_xyz = np.array([unpack_dict(d, match_on) for n, d in target_graph.nodes(data=True)])\n",
    "\n",
    "    # compute pairwise distances between the nodes in the input and target graph\n",
    "    dists = cdist(input_node_xyz, target_node_xyz)\n",
    "    is_close = np.isclose(dists, 0, atol=tol)\n",
    "    input_idx, target_idx = np.where(is_close)\n",
    "\n",
    "    # find nodes that are not matched uniquely\n",
    "    counts = np.sum(is_close, axis=1)\n",
    "    inds = np.where(counts > 1)[0]\n",
    "\n",
    "    if assert_unique:\n",
    "        assert len(input_graph.nodes()) == len(target_graph.nodes()), \"Number of nodes is not the same\"\n",
    "        assert np.all(counts == 1), f\"Some input nodes were not matched uniquely: {input_nodes[inds[counts != 1]]}\"\n",
    "\n",
    "    # reorder and relabel nodes on input graph\n",
    "    new_labels = dict(zip(input_nodes[input_idx], target_nodes[target_idx]))\n",
    "    graph_input = nx.relabel_nodes(graph_input, new_labels)\n",
    "\n",
    "    return graph_input\n",
    "\n",
    "def compare_node_attrs(input_graph: nx.DiGraph, target_graph: nx.DiGraph, key: str, func: Callable = jnp.isclose, reduce: Callable = all, subset: Optional[list[int]] = None):\n",
    "    \"\"\"Compares the node attributes of the input and target graph.\n",
    "\n",
    "    Graphs are compared by their node indices / labels. This means corresponding nodes in the\n",
    "    input and target graph should have the same node indices. To do this you can use \n",
    "    `match_nodes()` to relabel the input graph to have the same node indices as the target \n",
    "    graph, before running this function.\n",
    "\n",
    "    Args:\n",
    "        input_graph: A graph.\n",
    "        target_graph: A graph.\n",
    "        key: The attribute to compare.\n",
    "        func: The function to use to compare the attributes.\n",
    "        reduce: The function to use to reduce the comparison results.\n",
    "        subset: A list of node indices to compare.\n",
    "\n",
    "    Returns:\n",
    "        The comparison results.\n",
    "    \"\"\"\n",
    "    if subset is not None:\n",
    "        input_graph = input_graph.subgraph(subset)\n",
    "        target_graph = target_graph.subgraph(subset)\n",
    "    input_attrs = nx.get_node_attributes(input_graph, key)\n",
    "    target_attrs = nx.get_node_attributes(target_graph, key)\n",
    "    is_equal = jax.tree_util.tree_map(lambda x, y: func(x, y), input_attrs, target_attrs)\n",
    "    if reduce is not None:\n",
    "        return reduce(jnp.array(jax.tree.flatten(is_equal)[0]))\n",
    "    else:\n",
    "        return is_equal\n",
    "    \n",
    "def print_or_assert(cond, msg, print_only=True):\n",
    "    if print_only:\n",
    "        if not cond:\n",
    "            print(msg)\n",
    "        else:\n",
    "            return\n",
    "    else:\n",
    "        assert cond, msg\n",
    "\n",
    "def compute_branch_features(graph, branches):\n",
    "    def path_lengths(xyz):\n",
    "        d = np.diff(xyz, axis=0)\n",
    "        seg = np.linalg.norm(d, axis=1)\n",
    "        s = np.concatenate([[0.0], np.cumsum(seg)])\n",
    "        return s\n",
    "\n",
    "    def length_weighted_mean(values, s):\n",
    "        L = s[-1]\n",
    "        return np.trapezoid(values, s) / L if L > 0 else 0.0\n",
    "\n",
    "    xyzrs = [np.array([unpack_dict(graph.nodes[n], \"xyzr\") for n in b]) for b in branches]\n",
    "    ls = [path_lengths(xyzr_i[:, :3]) for xyzr_i in xyzrs]\n",
    "    branch_lens = [np.sum(l) for l in ls]\n",
    "    avg_x = [length_weighted_mean(xyzr_i[:, 0], l_i) for xyzr_i, l_i in zip(xyzrs, ls)]\n",
    "    avg_y = [length_weighted_mean(xyzr_i[:, 1], l_i) for xyzr_i, l_i in zip(xyzrs, ls)]\n",
    "    avg_z = [length_weighted_mean(xyzr_i[:, 2], l_i) for xyzr_i, l_i in zip(xyzrs, ls)]\n",
    "    avg_radius = [length_weighted_mean(xyzr_i[:, 3], l_i) for xyzr_i, l_i in zip(xyzrs, ls)]\n",
    "\n",
    "    fts = [branch_lens, avg_x, avg_y, avg_z, avg_radius]\n",
    "    return np.array([np.hstack(f) for f in zip(*fts)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_swc_reader_matches_neuron(fname: str):\n",
    "    \"\"\"Tests that the SWC reader matches NEURON.\n",
    "\n",
    "    This test has 3 parts:\n",
    "    1. Test the SWC file is read correctly.\n",
    "    2. Test Jaxley's branches match NEURON's sections.\n",
    "    3. Test Jaxley's compartments match NEURON's segments.\n",
    "\n",
    "    Args:\n",
    "        fname: The path to the SWC file.\n",
    "    \"\"\"\n",
    "    ### SWC GRAPHS ###\n",
    "    neuron_swc_reader(fname)\n",
    "    swc_neuron = neuron_swc_graph(fname)\n",
    "    # swc_jaxley = graph_io_old.to_swc_graph(fname)\n",
    "\n",
    "    swc_jaxley = graph_io_new.swc_to_nx(fname)\n",
    "    swc_neuron_matched = match_nodes(swc_neuron.copy(), swc_jaxley, assert_unique=False)\n",
    "\n",
    "    for attr in [\"x\", \"y\", \"z\", \"r\"]:\n",
    "        nodes_in_both_graphs = set(swc_jaxley.nodes()) & set(swc_neuron_matched.nodes())\n",
    "        print_or_assert(compare_node_attrs(swc_jaxley, swc_neuron_matched, attr, subset=nodes_in_both_graphs), f\"Attribute {attr} is not equal\")\n",
    "\n",
    "    ### BRANCHES ###\n",
    "    branches_jaxley = graph_io_new.list_branches(swc_jaxley, ignore_swc_tracing_interruptions=False)\n",
    "    # branches_jaxley = old_list_branches(swc_jaxley, ignore_swc_tracing_interruptions=False)\n",
    "    branches_neuron = neuron_list_branches(swc_neuron)\n",
    "\n",
    "    branches_jaxley = match_branches(swc_jaxley, swc_neuron, branches_jaxley, branches_neuron)\n",
    "\n",
    "    # branch_fts_jaxley = compute_branch_features(swc_jaxley, branches_jaxley)\n",
    "    # branch_fts_neuron = compute_branch_features(swc_neuron, branches_neuron)\n",
    "\n",
    "    assert len(branches_jaxley) == len(branches_neuron), \"Number of branches is not the same\"\n",
    "\n",
    "    ### COMP GRAPHS ###\n",
    "    comp_graph_neuron = neuron_comp_graph(fname, ncomp=1)\n",
    "\n",
    "    comp_graph_jaxley = graph_io_new.build_compartment_graph(swc_jaxley, ncomp=1, max_len=2000.0, ignore_swc_tracing_interruptions=False)\n",
    "    comp_graph_jaxley = graph_io_new._add_jaxley_meta_data(comp_graph_jaxley)\n",
    "    comp_graph_jaxley = graph_io_new._replace_branchpoints_with_edges(comp_graph_jaxley)\n",
    "    groups_from_node_attrs = lambda d: \"\".join([d.get(g, False)*g for g in [\"soma\", \"basal\", \"apical\", \"axon\"]])\n",
    "    nx.set_node_attributes(comp_graph_jaxley, {k: [groups_from_node_attrs(d)] for k, d in comp_graph_jaxley.nodes(data=True)}, \"groups\")\n",
    "\n",
    "    # comp_graph_jaxley = graph_io_old.build_compartment_graph(comp_graph_jaxley, ncomp=1, max_len=2000.0, ignore_swc_tracing_interruptions=False)\n",
    "    # comp_graph_jaxley = graph_io_old._remove_branch_points_at_tips(comp_graph_jaxley)\n",
    "    # comp_graph_jaxley = graph_io_old._remove_branch_points(comp_graph_jaxley)\n",
    "\n",
    "    comp_graph_jaxley = match_nodes(comp_graph_jaxley, comp_graph_neuron, match_on=\"xyz\", tol=1e-3)\n",
    "\n",
    "    for key in [\"radius\", \"length\", \"area\", \"volume\", \"x\", \"y\", \"z\"]:\n",
    "        print_or_assert(compare_node_attrs(comp_graph_jaxley, comp_graph_neuron, key, func=lambda x, y: jnp.isclose(x, y, atol=1e-4), reduce=jnp.all), f\"|{key}-{key}'| > {1e-4} for some nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Checking morph_interrupted_soma.swc ##\n",
      "## Checking morph_ca1_n120_250.swc ##\n",
      "## Checking morph_3_types.swc ##\n",
      "## Checking morph_allen_485574832.swc ##\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1412], line 10\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m([\n\u001b[1;32m      6\u001b[0m     fname\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.swc\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mall\u001b[39m(x \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m fname \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretina\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiple_roots\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflywire\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      8\u001b[0m ]):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m## Checking \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ##\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m     \u001b[43mtest_swc_reader_matches_neuron\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1411], line 25\u001b[0m, in \u001b[0;36mtest_swc_reader_matches_neuron\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m     22\u001b[0m     print_or_assert(compare_node_attrs(swc_jaxley, swc_neuron_matched, attr, subset\u001b[38;5;241m=\u001b[39mnodes_in_both_graphs), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not equal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m### BRANCHES ###\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m branches_jaxley \u001b[38;5;241m=\u001b[39m \u001b[43mgraph_io_new\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_branches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mswc_jaxley\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_swc_tracing_interruptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# branches_jaxley = old_list_branches(swc_jaxley, ignore_swc_tracing_interruptions=False)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m branches_neuron \u001b[38;5;241m=\u001b[39m neuron_list_branches(swc_neuron)\n",
      "File \u001b[0;32m~/Uni/PhD/projects/jaxleyverse/jaxley/jaxley/io/tmp.py:364\u001b[0m, in \u001b[0;36mlist_branches\u001b[0;34m(swc_graph, source, return_branchpoints, ignore_swc_tracing_interruptions, max_len)\u001b[0m\n\u001b[1;32m    359\u001b[0m     source \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m    360\u001b[0m         n \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m nx\u001b[38;5;241m.\u001b[39mdfs_tree(undir_graph, source) \u001b[38;5;28;01mif\u001b[39;00m is_branchpoint_or_tip(n)\n\u001b[1;32m    361\u001b[0m     )\n\u001b[1;32m    363\u001b[0m single_soma \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(soma_nodes()) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 364\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m \u001b[43mnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdfs_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mundir_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m single_soma \u001b[38;5;129;01mand\u001b[39;00m is_soma(node):  \u001b[38;5;66;03m# a single soma is its own branch\u001b[39;00m\n\u001b[1;32m    366\u001b[0m         branches\u001b[38;5;241m.\u001b[39mappend([node])\n",
      "File \u001b[0;32m<class 'networkx.utils.decorators.argmap'> compilation 24:3\u001b[0m, in \u001b[0;36margmap_dfs_tree_21\u001b[0;34m(G, source, depth_limit, sort_neighbors, backend, **backend_kwargs)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbz2\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcollections\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgzip\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mitertools\u001b[39;00m\n",
      "File \u001b[0;32m~/Uni/PhD/projects/jaxleyverse/jaxley/.venv/lib/python3.12/site-packages/networkx/utils/backends.py:967\u001b[0m, in \u001b[0;36m_dispatchable.__call__\u001b[0;34m(self, backend, *args, **kwargs)\u001b[0m\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m backend \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnetworkx\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    966\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m backend is not installed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 967\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morig_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;66;03m# Use `backend_name` in this function instead of `backend`.\u001b[39;00m\n\u001b[1;32m    970\u001b[0m \u001b[38;5;66;03m# This is purely for aesthetics and to make it easier to search for this\u001b[39;00m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;66;03m# variable since \"backend\" is used in many comments and log/error messages.\u001b[39;00m\n\u001b[1;32m    972\u001b[0m backend_name \u001b[38;5;241m=\u001b[39m backend\n",
      "File \u001b[0;32m~/Uni/PhD/projects/jaxleyverse/jaxley/.venv/lib/python3.12/site-packages/networkx/algorithms/traversal/depth_first_search.py:162\u001b[0m, in \u001b[0;36mdfs_tree\u001b[0;34m(G, source, depth_limit, sort_neighbors)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     T\u001b[38;5;241m.\u001b[39madd_node(source)\n\u001b[0;32m--> 162\u001b[0m \u001b[43mT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_edges_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdfs_edges\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth_limit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort_neighbors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort_neighbors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m T\n",
      "File \u001b[0;32m~/Uni/PhD/projects/jaxleyverse/jaxley/.venv/lib/python3.12/site-packages/networkx/classes/digraph.py:810\u001b[0m, in \u001b[0;36mDiGraph.add_edges_from\u001b[0;34m(self, ebunch_to_add, **attr)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone cannot be a node\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 810\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_succ[v] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madjlist_inner_dict_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pred[v] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madjlist_inner_dict_factory()\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_node[v] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_attr_dict_factory()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fpath = \"../jaxley/tests/swc_files\"\n",
    "fname = \"morph_ca1_n120_250.swc\"\n",
    "\n",
    "for fname in os.listdir(fpath):\n",
    "    if all([\n",
    "        fname.endswith(\".swc\"),\n",
    "        all(x not in fname for x in [\"retina\", \"multiple_roots\", \"flywire\"])\n",
    "    ]):\n",
    "        print(f\"## Checking {fname} ##\")\n",
    "        test_swc_reader_matches_neuron(os.path.join(fpath, fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1523,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = \"../jaxley/tests/swc_files\"\n",
    "fname = \"morph_ca1_n120_250.swc\"\n",
    "\n",
    "swc_jaxley = graph_io_new.swc_to_nx(os.path.join(fpath, fname))\n",
    "\n",
    "comp_graph_jaxley = graph_io_new.build_compartment_graph(swc_jaxley, ncomp=1, max_len=2000.0, ignore_swc_tracing_interruptions=False)\n",
    "cell = graph_io_new.from_graph(comp_graph_jaxley)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<networkx.classes.digraph.DiGraph at 0x7fc16cbb1610>"
      ]
     },
     "execution_count": 1526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_graph(module):\n",
    "    edges = module.edges.set_index([\"pre_index\", \"post_index\"])\n",
    "    edges.index.names = (None, None)\n",
    "\n",
    "    G = graph_io_new.pandas_to_nx(module.nodes, edges, pd.Series())\n",
    "\n",
    "    global_attrs = [\"xyzr\", \"channels\", \"synapses\", \"group_names\", \"branchpoints_and_tips\"]\n",
    "    for attr in global_attrs:\n",
    "        G.graph[attr] = getattr(module, attr)\n",
    "\n",
    "    G = graph_io_new._replace_edges_with_branchpoints(G) # TODO: fix this\n",
    "    return G\n",
    "\n",
    "def set_ncomp(cell, branch, ncomp):\n",
    "    branch_xyzr = cell.xyzr[branch]\n",
    "    node_df = pd.DataFrame(branch_xyzr, columns=[\"x\", \"y\", \"z\", \"r\"])\n",
    "    node_df[\"id\"] = 0 # dummy id\n",
    "\n",
    "    # assert no synapses attached to branch\n",
    "    # assert attrs other than xyzr along branch are the same\n",
    "\n",
    "    branch_comps = graph_io_new.compartmentalize_branch(node_df, ncomp)\n",
    "    # TODO: Finish this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_ncomp(view, ncomp):\n",
    "    from jaxley.io.graph import branch_comps_from_nodes\n",
    "\n",
    "    new_branch_comps = []\n",
    "    for branch_idx in view._branches_in_view:\n",
    "        branch = view.branch(branch_idx)\n",
    "        node_attrs = pd.DataFrame(branch.xyzr[0], columns=[\"x\", \"y\", \"z\", \"r\"])\n",
    "        node_attrs[\"id\"] = 0\n",
    "        new_comps = branch_comps_from_nodes(node_attrs, ncomp)\n",
    "        comp_cols = [\"is_comp\",\"id\", \"length\", \"x\", \"y\", \"z\", \"radius\", \"area\", \"volume\"]\n",
    "        comp_df = pd.DataFrame(new_comps, columns=comp_cols)\n",
    "        comp_df = comp_df.loc[comp_df[\"is_comp\"].astype(bool)].drop(columns=[\"id\", \"is_comp\"])\n",
    "        \n",
    "        for name, is_group in branch.nodes[branch.group_names].all().items():\n",
    "            comp_df.loc[:, name] = is_group\n",
    "\n",
    "        comp_df[\"global_cell_index\"] = branch.nodes[\"global_cell_index\"].iloc[0]\n",
    "        comp_df[\"global_branch_index\"] = branch.nodes[\"global_branch_index\"].iloc[0]\n",
    "        comp_df[\"v\"] = np.mean(branch.nodes[\"v\"])\n",
    "        comp_df[\"axial_resistivity\"] = np.mean(branch.nodes[\"axial_resistivity\"])\n",
    "        comp_df[\"capacitance\"] = np.mean(branch.nodes[\"capacitance\"])\n",
    "            \n",
    "        comp_df.index = [-1]*len(comp_df.index)\n",
    "\n",
    "        new_branch_comps.append(comp_df)\n",
    "\n",
    "    for branch_idx, branch_comps in zip(view._branches_in_view, new_branch_comps):\n",
    "        branch_nodes = view.nodes[view.nodes[\"global_branch_index\"] == branch_idx].index\n",
    "        view.base.nodes = pd.concat([view.base.nodes.loc[:branch_nodes[0]], branch_comps, view.base.nodes.loc[branch_nodes[-1]:]], ignore_index=True, join=\"outer\")\n",
    "    \n",
    "    view.base.nodes[\"global_comp_index\"] = np.arange(len(view.base.nodes))\n",
    "    view.base._init_view()\n",
    "    view.base._update_local_indices()\n",
    "    \n",
    "\n",
    "set_ncomp(cell.branch(0), 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
