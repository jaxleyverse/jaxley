{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p> The official documentation for Jaxley has moved to jaxley.readthedocs.io.  The website you are currently on will be taken down in the future.</p>"},{"location":"tutorial/00_jaxley_api/","title":"Key concepts in Jaxley","text":"<p>In this tutorial, we will introduce you to the basic concepts of Jaxley. You will learn about:</p> <ul> <li>Modules (e.g., Cell, Network,\u2026)<ul> <li>nodes</li> <li>edges</li> </ul> </li> <li>Views<ul> <li>Groups</li> </ul> </li> <li>Channels</li> <li>Synapses</li> </ul> <p>Here is a code snippet which you will learn to understand in this tutorial: <pre><code>import jaxley as jx\nfrom jaxley.channels import Na, K, Leak\nfrom jaxley.synapses import IonotropicSynapse\nfrom jaxley.connect import connect\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\n# Assembling different Modules into a Network\ncomp = jx.Compartment()\nbranch = jx.Branch(comp, ncomp=1)\ncell = jx.Cell(branch, parents=[-1, 0, 0])\nnet = jx.Network([cell]*3)\n\n# Navigating and inspecting the Modules using Views\ncell0 = net.cell(0)\ncell0.nodes\n\n# How to group together parts of Modules\nnet.cell(1).add_to_group(\"cell1\")\n\n# inserting channels in the membrane\nwith net.cell(0) as cell0:\n    cell0.insert(Na())\n    cell0.insert(K())\n\n# connecting two cells using a Synapse\npre_comp = cell0.branch(1).comp(0)\npost_comp = net.cell1.branch(0).comp(0)\n\nconnect(pre_comp, post_comp)\n</code></pre></p> <p>First, we import the relevant libraries:</p> <pre><code>from jax import config\nconfig.update(\"jax_enable_x64\", True)\nconfig.update(\"jax_platform_name\", \"cpu\")\n\nimport jaxley as jx\nfrom jaxley.channels import Na, K, Leak\nfrom jaxley.synapses import IonotropicSynapse\nfrom jaxley.connect import connect\nimport matplotlib.pyplot as plt\nimport numpy as np\n</code></pre>"},{"location":"tutorial/00_jaxley_api/#modules","title":"Modules","text":"<p>In Jaxley, we heavily rely on the concept of Modules to build biophyiscal models of neural systems at various scales. Jaxley implements four types of Modules: - <code>Compartment</code> - <code>Branch</code> - <code>Cell</code> - <code>Network</code> </p> <p>Modules can be connected together to build increasingly detailed and complex models. <code>Compartment</code> -&gt; <code>Branch</code> -&gt; <code>Cell</code> -&gt; <code>Network</code>.</p> <p><code>Compartment</code>s are the atoms of biophysical models in Jaxley. All mechanisms and synaptic connections live on the level of <code>Compartment</code>s and can already be simulated using <code>jx.integrate</code> on their own. Everything you do in Jaxley starts with a <code>Compartment</code>.</p> <pre><code>comp = jx.Compartment() # single compartment model.\n</code></pre> <p>Multiple <code>Compartments</code> can be connected together to form longer, linear cables, which we call <code>Branch</code>es and are equivalent to sections in <code>NEURON</code>.</p> <pre><code>ncomp = 4\nbranch = jx.Branch([comp] * ncomp)\n</code></pre> <p>In order to construct cell morphologies in Jaxley, multiple <code>Branches</code> can to be connected together as a <code>Cell</code>:</p> <pre><code># -1 indicates that the first branch has no parent branch.\n# The other two branches both have the 0-eth branch as their parent.\nparents = [-1, 0, 0]\ncell = jx.Cell([branch] * len(parents), parents)\n</code></pre> <p>Finally, several <code>Cell</code>s can be grouped together to form a <code>Network</code>, which can than be connected together using <code>Synpase</code>s.</p> <pre><code>ncells = 2\nnet = jx.Network([cell]*ncells)\n\nnet.shape # shows you the num_cells, num_branches, num_comps\n</code></pre> <pre><code>(2, 6, 24)\n</code></pre> <p>Every module tracks information about its current state and parameters in two Dataframes called <code>nodes</code> and <code>edges</code>. <code>nodes</code> contains all the information that we associate with compartments in the model (each row corresponds to one compartment) and <code>edges</code> tracks all the information relevant to synapses.</p> <p>This means that you can easily keep track of the current state of your <code>Module</code> and how it changes at all times.</p> <pre><code>net.nodes\n</code></pre> local_cell_index local_branch_index local_comp_index length radius axial_resistivity capacitance v global_cell_index global_branch_index global_comp_index controlled_by_param 0 0 0 0 10.0 1.0 5000.0 1.0 -70.0 0 0 0 0 1 0 0 1 10.0 1.0 5000.0 1.0 -70.0 0 0 1 0 2 0 0 2 10.0 1.0 5000.0 1.0 -70.0 0 0 2 0 3 0 0 3 10.0 1.0 5000.0 1.0 -70.0 0 0 3 0 4 0 1 0 10.0 1.0 5000.0 1.0 -70.0 0 1 4 0 5 0 1 1 10.0 1.0 5000.0 1.0 -70.0 0 1 5 0 6 0 1 2 10.0 1.0 5000.0 1.0 -70.0 0 1 6 0 7 0 1 3 10.0 1.0 5000.0 1.0 -70.0 0 1 7 0 8 0 2 0 10.0 1.0 5000.0 1.0 -70.0 0 2 8 0 9 0 2 1 10.0 1.0 5000.0 1.0 -70.0 0 2 9 0 10 0 2 2 10.0 1.0 5000.0 1.0 -70.0 0 2 10 0 11 0 2 3 10.0 1.0 5000.0 1.0 -70.0 0 2 11 0 12 1 0 0 10.0 1.0 5000.0 1.0 -70.0 1 3 12 0 13 1 0 1 10.0 1.0 5000.0 1.0 -70.0 1 3 13 0 14 1 0 2 10.0 1.0 5000.0 1.0 -70.0 1 3 14 0 15 1 0 3 10.0 1.0 5000.0 1.0 -70.0 1 3 15 0 16 1 1 0 10.0 1.0 5000.0 1.0 -70.0 1 4 16 0 17 1 1 1 10.0 1.0 5000.0 1.0 -70.0 1 4 17 0 18 1 1 2 10.0 1.0 5000.0 1.0 -70.0 1 4 18 0 19 1 1 3 10.0 1.0 5000.0 1.0 -70.0 1 4 19 0 20 1 2 0 10.0 1.0 5000.0 1.0 -70.0 1 5 20 0 21 1 2 1 10.0 1.0 5000.0 1.0 -70.0 1 5 21 0 22 1 2 2 10.0 1.0 5000.0 1.0 -70.0 1 5 22 0 23 1 2 3 10.0 1.0 5000.0 1.0 -70.0 1 5 23 0 <pre><code>net.edges.head() # this is currently empty since we have not made any connections yet\n</code></pre> global_edge_index pre_global_comp_index post_global_comp_index pre_locs post_locs type type_ind"},{"location":"tutorial/00_jaxley_api/#views","title":"Views","text":"<p>Since these <code>Module</code>s can become very complex, Jaxley utilizes so called <code>View</code>s to make working with <code>Module</code>s easy and intuitive. </p> <p>The simplest way to navigate Modules is by navigating them via the hierarchy that we introduced above. A <code>View</code> is what you get when you index into the module. For example, for a <code>Network</code>:</p> <pre><code>net.cell(0)\n</code></pre> <pre><code>View with 0 different channels. Use `.nodes` for details.\n</code></pre> <p>Views behave very similarly to <code>Module</code>s, i.e. the <code>cell(0)</code> (the 0<sup>th</sup> cell of the network) behaves like the <code>cell</code> we instantiated earlier. As such, <code>cell(0)</code> also has a <code>nodes</code> attribute, which keeps track of it\u2019s part of the network:</p> <pre><code>net.cell(0).nodes\n</code></pre> local_cell_index local_branch_index local_comp_index length radius axial_resistivity capacitance v global_cell_index global_branch_index global_comp_index controlled_by_param 0 0 0 0 10.0 1.0 5000.0 1.0 -70.0 0 0 0 0 1 0 0 1 10.0 1.0 5000.0 1.0 -70.0 0 0 1 0 2 0 0 2 10.0 1.0 5000.0 1.0 -70.0 0 0 2 0 3 0 0 3 10.0 1.0 5000.0 1.0 -70.0 0 0 3 0 4 0 1 0 10.0 1.0 5000.0 1.0 -70.0 0 1 4 0 5 0 1 1 10.0 1.0 5000.0 1.0 -70.0 0 1 5 0 6 0 1 2 10.0 1.0 5000.0 1.0 -70.0 0 1 6 0 7 0 1 3 10.0 1.0 5000.0 1.0 -70.0 0 1 7 0 8 0 2 0 10.0 1.0 5000.0 1.0 -70.0 0 2 8 0 9 0 2 1 10.0 1.0 5000.0 1.0 -70.0 0 2 9 0 10 0 2 2 10.0 1.0 5000.0 1.0 -70.0 0 2 10 0 11 0 2 3 10.0 1.0 5000.0 1.0 -70.0 0 2 11 0 <p>Let\u2019s use <code>View</code>s to visualize only parts of the <code>Network</code>. Before we do that, we create x, y, and z coordinates for the <code>Network</code>:</p> <pre><code># Compute xyz coordinates of the cells.\nnet.compute_xyz()\n\n# Move cells (since they are placed on top of each other by default).\nnet.cell(0).move(y=30)\n</code></pre> <p>We can now visualize the entire <code>net</code> (i.e., the entire <code>Module</code>) with the <code>.vis()</code> method\u2026</p> <pre><code># We can use the vis function to visualize Modules.\nfig, ax = plt.subplots(1, 1, figsize=(3,3))\nnet.vis(ax=ax)\n</code></pre> <pre><code>&lt;Axes: &gt;\n</code></pre> <p></p> <p>\u2026but we can also create a <code>View</code> to visualize only parts of the <code>net</code>:</p> <pre><code># ... and Views\nfig, ax = plt.subplots(1,1, figsize=(3,3))\nnet.cell(0).vis(ax=ax, color=\"blue\") # View of the 0th cell of the network\nnet.cell(1).vis(ax=ax, color=\"red\") # View of the 1st cell of the network\n\nnet.cell(0).branch(0).vis(ax=ax, color=\"green\") # View of the 1st branch of the 0th cell of the network\nnet.cell(1).branch(1).comp(1).vis(ax=ax, color=\"black\", type=\"line\") # View of the 0th comp of the 1st branch of the 0th cell of the network\n</code></pre> <pre><code>&lt;Axes: &gt;\n</code></pre> <p></p>"},{"location":"tutorial/00_jaxley_api/#how-to-create-views","title":"How to create <code>View</code>s","text":"<p>Above, we used <code>net.cell(0)</code> to generate a <code>View</code> of the 0-eth cell. <code>Jaxley</code> supports many ways of performing such indexing:</p> <pre><code># several types of indices are supported (lists, ranges, ...)\nnet.cell([0,1]).branch(\"all\").comp(0)  # View of all 0th comps of all branches of cell 0 and 1\n\nbranch.loc(0.1)  # Equivalent to `NEURON`s `loc`. Assumes branches are continuous from 0-1.\n\nnet[0,0,0]  # Modules/Views can also be lazily indexed\n\ncell0 = net.cell(0)  # Views can be assigned to variables and only track the parts of the Module they belong to\ncell0.branch(1).comp(0)  # Views can be continually indexed\n</code></pre> <pre><code>View with 0 different channels. Use `.nodes` for details.\n</code></pre> <pre><code>cell0.nodes\n</code></pre> local_cell_index local_branch_index local_comp_index length radius axial_resistivity capacitance v global_cell_index global_branch_index global_comp_index controlled_by_param 0 0 0 0 10.0 1.0 5000.0 1.0 -70.0 0 0 0 0 1 0 0 1 10.0 1.0 5000.0 1.0 -70.0 0 0 1 0 2 0 0 2 10.0 1.0 5000.0 1.0 -70.0 0 0 2 0 3 0 0 3 10.0 1.0 5000.0 1.0 -70.0 0 0 3 0 4 0 1 0 10.0 1.0 5000.0 1.0 -70.0 0 1 4 0 5 0 1 1 10.0 1.0 5000.0 1.0 -70.0 0 1 5 0 6 0 1 2 10.0 1.0 5000.0 1.0 -70.0 0 1 6 0 7 0 1 3 10.0 1.0 5000.0 1.0 -70.0 0 1 7 0 8 0 2 0 10.0 1.0 5000.0 1.0 -70.0 0 2 8 0 9 0 2 1 10.0 1.0 5000.0 1.0 -70.0 0 2 9 0 10 0 2 2 10.0 1.0 5000.0 1.0 -70.0 0 2 10 0 11 0 2 3 10.0 1.0 5000.0 1.0 -70.0 0 2 11 0 <pre><code>net.shape\n</code></pre> <pre><code>(2, 6, 24)\n</code></pre> <p>Note: In case you need even more flexibility in how you select parts of a Module, Jaxley provides a <code>select</code> method, to give full control over the exact parts of the <code>nodes</code> and <code>edges</code> that are part of a <code>View</code>. On examples of how this can be used, see the tutorial on advanced indexing.</p> <p>You can also iterate over networks, cells, and branches:</p> <pre><code># We set the radiuses to random values...\nradiuses = np.random.rand((24))\nnet.set(\"radius\", radiuses)\n\n# ...and then we set the length to 100.0 um if the radius is &gt;0.5.\nfor cell in net:\n    for branch in cell:\n        for comp in branch:\n            if comp.nodes.iloc[0][\"radius\"] &gt; 0.5:\n                comp.set(\"length\", 100.0)\n\n# Show the first five compartments:\nnet.nodes[[\"radius\", \"length\"]][:5]\n</code></pre> radius length 0 0.537066 100.0 1 0.050138 10.0 2 0.913129 100.0 3 0.874596 100.0 4 0.048903 10.0 <p>Finally, you can also use <code>View</code>s in a context manager:</p> <pre><code>with net.cell(0).branch(0) as branch0:\n    branch0.set(\"radius\", 2.0)\n    branch0.set(\"length\", 2.5)\n\n# Show the first five compartments.\nnet.nodes[[\"radius\", \"length\"]][:5]\n</code></pre> radius length 0 2.000000 2.5 1 2.000000 2.5 2 2.000000 2.5 3 2.000000 2.5 4 0.048903 10.0"},{"location":"tutorial/00_jaxley_api/#channels","title":"Channels","text":"<p>The <code>Module</code>s that we have created above will not do anything interesting, since by default Jaxley initializes them without any mechanisms in the membrane. To change this, we have to insert channels into the membrane. For this purpose <code>Jaxley</code> implements <code>Channel</code>s that can be inserted into any compartment using the <code>insert</code> method of a <code>Module</code> or a <code>View</code>:</p> <pre><code># insert a Leak channel into all compartments in the Module.\nnet.insert(Leak())\nnet.nodes.head() # Channel parameters are now also added to `nodes`.\n</code></pre> local_cell_index local_branch_index local_comp_index length radius axial_resistivity capacitance v global_cell_index global_branch_index global_comp_index controlled_by_param Leak Leak_gLeak Leak_eLeak 0 0 0 0 2.5 2.000000 5000.0 1.0 -70.0 0 0 0 0 True 0.0001 -70.0 1 0 0 1 2.5 2.000000 5000.0 1.0 -70.0 0 0 1 0 True 0.0001 -70.0 2 0 0 2 2.5 2.000000 5000.0 1.0 -70.0 0 0 2 0 True 0.0001 -70.0 3 0 0 3 2.5 2.000000 5000.0 1.0 -70.0 0 0 3 0 True 0.0001 -70.0 4 0 1 0 10.0 0.048903 5000.0 1.0 -70.0 0 1 4 0 True 0.0001 -70.0 <p>This is also were <code>View</code>s come in handy, as it allows to easily target the insertion of channels to specific compartments.</p> <pre><code># inserting several channels into parts of the network\nwith net.cell(0) as cell0:\n    cell0.insert(Na())\n    cell0.insert(K())\n\n# # The above is equivalent to:\n# net.cell(0).insert(Na())\n# net.cell(0).insert(K())\n\n# K and Na channels were only insert into cell 0\nnet.cell(\"all\").branch(0).comp(0).nodes[[\"global_cell_index\", \"Na\", \"K\", \"Leak\"]]\n</code></pre> global_cell_index Na K Leak 0 0 True True True 12 1 False False True"},{"location":"tutorial/00_jaxley_api/#synapses","title":"Synapses","text":"<p>To connect different cells together, Jaxley implements a <code>connect</code> method, that can be used to couple 2 compartments together using a <code>Synapse</code>. Synapses in Jaxley work only on the compartment level, that means to be able to connect two cells, you need to specify the exact compartments on a given cell to make the connections between. Below is an example of this:</p> <pre><code># connecting two cells using a Synapse\npre_comp = cell0.branch(1).comp(0)\npost_comp = net.cell(1).branch(0).comp(0)\n\nconnect(pre_comp, post_comp, IonotropicSynapse())\n\nnet.edges\n</code></pre> global_edge_index pre_global_comp_index post_global_comp_index type type_ind pre_locs post_locs IonotropicSynapse_gS IonotropicSynapse_e_syn IonotropicSynapse_k_minus IonotropicSynapse_s controlled_by_param 0 0 4 12 IonotropicSynapse 0 0.125 0.125 0.0001 0.0 0.025 0.2 0 <p>As you can see above, now the <code>edges</code> dataframe is also updated with the information of the newly added synapse. </p> <p>Congrats! You should now have an intuitive understand of how to use Jaxley\u2019s API to construct, navigate and manipulate neuron models.</p>"},{"location":"tutorial/01_morph_neurons/","title":"Basics of Jaxley","text":"<p>In this tutorial, you will learn how to:</p> <ul> <li>build your first morphologically detailed cell or read it from SWC  </li> <li>stimulate the cell  </li> <li>record from the cell  </li> <li>visualize cells  </li> <li>run your first simulation  </li> </ul> <p>Here is a code snippet which you will learn to understand in this tutorial: <pre><code>import jaxley as jx\nfrom jaxley.channels import Na, K, Leak\nimport matplotlib.pyplot as plt\n\n\n# Build the cell.\ncomp = jx.Compartment()\nbranch = jx.Branch(comp, ncomp=2)\ncell = jx.Cell(branch, parents=[-1, 0, 0, 1, 1])\n\n# Insert channels.\ncell.insert(Leak())\ncell.branch(0).insert(Na())\ncell.branch(0).insert(K())\n\n# Change parameters.\ncell.set(\"axial_resistivity\", 200.0)\n\n# Visualize the morphology.\ncell.compute_xyz()\nfig, ax = plt.subplots(1, 1, figsize=(4, 4))\ncell.vis(ax=ax)\n\n# Stimulate.\ncurrent = jx.step_current(i_delay=1.0, i_dur=1.0, i_amp=0.1, delta_t=0.025, t_max=10.0)\ncell.branch(0).loc(0.0).stimulate(current)\n\n# Record.\ncell.branch(0).loc(0.0).record(\"v\")\n\n# Simulate and plot.\nv = jx.integrate(cell, delta_t=0.025)\nplt.plot(v.T)\n</code></pre></p> <p>First, we import the relevant libraries:</p> <pre><code>from jax import config\nconfig.update(\"jax_enable_x64\", True)\nconfig.update(\"jax_platform_name\", \"cpu\")\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport jax.numpy as jnp\nfrom jax import jit\n\nimport jaxley as jx\nfrom jaxley.channels import Na, K, Leak\nfrom jaxley.synapses import IonotropicSynapse\nfrom jaxley.connect import fully_connect\n</code></pre> <p>We will now build our first cell in <code>Jaxley</code>. You have two options to do this: you can either build a cell bottom-up by defining the morphology yourself, or you can load cells from SWC files.</p>"},{"location":"tutorial/01_morph_neurons/#define-the-cell-from-scratch","title":"Define the cell from scratch","text":"<p>To define a cell from scratch you first have to define a single compartment and then assemble those compartments into a branch:</p> <pre><code>comp = jx.Compartment()\nbranch = jx.Branch(comp, ncomp=2)\n</code></pre> <p>Next, we can assemble branches into a cell. To do so, we have to define for each branch what its parent branch is. A <code>-1</code> entry means that this branch does not have a parent.</p> <pre><code>parents = jnp.asarray([-1, 0, 0, 1, 1])\ncell = jx.Cell(branch, parents=parents)\n</code></pre> <p>To learn more about <code>Compartment</code>s, <code>Branch</code>es, and <code>Cell</code>s, see this tutorial.</p>"},{"location":"tutorial/01_morph_neurons/#read-the-cell-from-an-swc-file","title":"Read the cell from an SWC file","text":"<p>Alternatively, you could also load cells from SWC with </p> <p><code>cell = jx.read_swc(fname, ncomp=4)</code></p> <p>Details on handling SWC files can be found in this tutorial.</p>"},{"location":"tutorial/01_morph_neurons/#visualize-the-cells","title":"Visualize the cells","text":"<p>Cells can be visualized as follows:</p> <pre><code>cell.compute_xyz()  # Only needed for visualization.\n\nfig, ax = plt.subplots(1, 1, figsize=(4, 2))\n_ = cell.vis(ax=ax, color=\"k\")\n</code></pre> <p></p>"},{"location":"tutorial/01_morph_neurons/#insert-mechanisms","title":"Insert mechanisms","text":"<p>Currently, the cell does not contain any kind of ion channel (not even a <code>leak</code>). We can fix this by inserting a leak channel into the entire cell, and by inserting sodium and potassium into the zero-eth branch.</p> <pre><code>cell.insert(Leak())\ncell.branch(0).insert(Na())\ncell.branch(0).insert(K())\n</code></pre> <p>Once the cell is created, we can inspect its <code>.nodes</code> attribute which lists all properties of the cell:</p> <pre><code>cell.nodes\n</code></pre> local_cell_index local_branch_index local_comp_index length radius axial_resistivity capacitance v global_cell_index global_branch_index ... Na Na_gNa eNa vt Na_m Na_h K K_gK eK K_n 0 0 0 0 10.0 1.0 5000.0 1.0 -70.0 0 0 ... True 0.05 50.0 -60.0 0.2 0.2 True 0.005 -90.0 0.2 1 0 0 1 10.0 1.0 5000.0 1.0 -70.0 0 0 ... True 0.05 50.0 -60.0 0.2 0.2 True 0.005 -90.0 0.2 2 0 1 0 10.0 1.0 5000.0 1.0 -70.0 0 1 ... False NaN NaN NaN NaN NaN False NaN NaN NaN 3 0 1 1 10.0 1.0 5000.0 1.0 -70.0 0 1 ... False NaN NaN NaN NaN NaN False NaN NaN NaN 4 0 2 0 10.0 1.0 5000.0 1.0 -70.0 0 2 ... False NaN NaN NaN NaN NaN False NaN NaN NaN 5 0 2 1 10.0 1.0 5000.0 1.0 -70.0 0 2 ... False NaN NaN NaN NaN NaN False NaN NaN NaN 6 0 3 0 10.0 1.0 5000.0 1.0 -70.0 0 3 ... False NaN NaN NaN NaN NaN False NaN NaN NaN 7 0 3 1 10.0 1.0 5000.0 1.0 -70.0 0 3 ... False NaN NaN NaN NaN NaN False NaN NaN NaN 8 0 4 0 10.0 1.0 5000.0 1.0 -70.0 0 4 ... False NaN NaN NaN NaN NaN False NaN NaN NaN 9 0 4 1 10.0 1.0 5000.0 1.0 -70.0 0 4 ... False NaN NaN NaN NaN NaN False NaN NaN NaN <p>10 rows \u00d7 25 columns</p> <p>Note that <code>Jaxley</code> uses the same units as the <code>NEURON</code> simulator, which are listed here.</p> <p>You can also inspect just parts of the <code>cell</code>, for example its 1<sup>st</sup> branch:</p> <pre><code>cell.branch(1).nodes\n</code></pre> local_cell_index local_branch_index local_comp_index length radius axial_resistivity capacitance v Leak Leak_gLeak ... Na_m Na_h K K_gK eK K_n global_cell_index global_branch_index global_comp_index controlled_by_param 2 0 0 0 10.0 1.0 5000.0 1.0 -70.0 True 0.0001 ... NaN NaN False NaN NaN NaN 0 1 2 1 3 0 0 1 10.0 1.0 5000.0 1.0 -70.0 True 0.0001 ... NaN NaN False NaN NaN NaN 0 1 3 1 <p>2 rows \u00d7 25 columns</p> <p>The easiest way to know which branch is the 1<sup>st</sup> branch (or, e.g., the zero-eth compartment of the 1<sup>st</sup> branch) is to plot it in a different color:</p> <pre><code>fig, ax = plt.subplots(1, 1, figsize=(4, 2))\n_ = cell.vis(ax=ax, color=\"k\")\n_ = cell.branch(1).vis(ax=ax, color=\"r\")\n_ = cell.branch(1).comp(1).vis(ax=ax, color=\"b\")\n</code></pre> <p></p> <p>More background and features on indexing as <code>cell.branch(0)</code> is in this tutorial.</p>"},{"location":"tutorial/01_morph_neurons/#change-parameters-of-the-cell","title":"Change parameters of the cell","text":"<p>You can change properties of the cell with the <code>.set()</code> method:</p> <pre><code>cell.branch(1).set(\"axial_resistivity\", 200.0)\n</code></pre> <p>And we can again inspect the <code>.nodes</code> to make sure that the axial resistivity indeed changed:</p> <pre><code>cell.branch(1).nodes\n</code></pre> local_cell_index local_branch_index local_comp_index length radius axial_resistivity capacitance v Leak Leak_gLeak ... Na_m Na_h K K_gK eK K_n global_cell_index global_branch_index global_comp_index controlled_by_param 2 0 0 0 10.0 1.0 200.0 1.0 -70.0 True 0.0001 ... NaN NaN False NaN NaN NaN 0 1 2 1 3 0 0 1 10.0 1.0 200.0 1.0 -70.0 True 0.0001 ... NaN NaN False NaN NaN NaN 0 1 3 1 <p>2 rows \u00d7 25 columns</p> <p>In a similar way, you can modify channel properties or initial states (units are again here):</p> <pre><code>cell.branch(0).set(\"K_gK\", 0.01)  # modify potassium conductance.\ncell.set(\"v\", -65.0)  # modify initial voltage.\n</code></pre>"},{"location":"tutorial/01_morph_neurons/#stimulate-the-cell","title":"Stimulate the cell","text":"<p>We next stimulate one of the compartments with a step current. For this, we first define the step current (units are again here):</p> <pre><code>dt = 0.025\nt_max = 10.0\ntime_vec = np.arange(0, t_max+dt, dt)\ncurrent = jx.step_current(i_delay=1.0, i_dur=2.0, i_amp=0.08, delta_t=dt, t_max=t_max)\n\nfig, ax = plt.subplots(1, 1, figsize=(4, 2))\n_ = plt.plot(time_vec, current)\n</code></pre> <p></p> <p>We then stimulate one of the compartments of the cell with this step current:</p> <pre><code>cell.delete_stimuli()\ncell.branch(0).loc(0.0).stimulate(current)\n</code></pre> <pre><code>Added 1 external_states. See `.externals` for details.\n</code></pre>"},{"location":"tutorial/01_morph_neurons/#define-recordings","title":"Define recordings","text":"<p>Next, you have to define where to record the voltage. In this case, we will record the voltage at two locations:</p> <pre><code>cell.delete_recordings()\ncell.branch(0).loc(0.0).record(\"v\")\ncell.branch(3).loc(1.0).record(\"v\")\n</code></pre> <pre><code>Added 1 recordings. See `.recordings` for details.\nAdded 1 recordings. See `.recordings` for details.\n</code></pre> <p>We can again visualize these locations to understand where we inserted recordings:</p> <pre><code>fig, ax = plt.subplots(1, 1, figsize=(4, 2))\n_ = cell.vis(ax=ax)\n_ = cell.branch(0).loc(0.0).vis(ax=ax, color=\"b\")\n_ = cell.branch(3).loc(1.0).vis(ax=ax, color=\"g\")\n</code></pre> <p></p>"},{"location":"tutorial/01_morph_neurons/#simulate-the-cell-response","title":"Simulate the cell response","text":"<p>Having set up the cell, inserted stimuli and recordings, we are now ready to run a simulation with <code>jx.integrate</code>:</p> <pre><code>voltages = jx.integrate(cell, delta_t=dt)\nprint(\"voltages.shape\", voltages.shape)\n</code></pre> <pre><code>voltages.shape (2, 402)\n</code></pre> <p>The <code>jx.integrate</code> function returns an array of shape <code>(num_recordings, num_timepoints)</code>. In our case, we inserted <code>2</code> recordings and we simulated for 10ms at a 0.025 time step, which leads to 402 time steps.</p> <p>We can now visualize the voltage response:</p> <pre><code>fig, ax = plt.subplots(1, 1, figsize=(4, 2))\n_ = ax.plot(voltages[0], c=\"b\")\n_ = ax.plot(voltages[1], c=\"orange\")\n</code></pre> <p></p> <p>At the location of the first recording (in blue) the cell spiked, whereas at the second recording, it did not. This makes sense because we only inserted sodium and potassium channels into the first branch, but not in the entire cell.</p> <p>Congrats! You have just run your first morphologically detailed neuron simulation in <code>Jaxley</code>. We suggest to continue by learning how to build networks. If you are only interested in single cell simulations, you can directly jump to learning how to speed up simulations. If you want to simulate detailed morphologies from SWC files, checkout our tutorial on working with detailed morphologies.</p>"},{"location":"tutorial/02_small_network/","title":"Network simulations in Jaxley","text":"<p>In this tutorial, you will learn how to:</p> <ul> <li>connect neurons into a network  </li> <li>visualize networks  </li> <li>use the <code>.edges</code> attribute to inspect and change synaptic parameters</li> </ul> <p>Here is a code snippet which you will learn to understand in this tutorial: <pre><code>import jaxley as jx\nfrom jaxley.synapses import IonotropicSynapse\nfrom jaxley.connect import fully_connect\n\n\n# Define a network. `cell` is defined as in previous tutorial.\nnet = jx.Network([cell for _ in range(11)])\n\n# Define synapses.\nfully_connect(\n    net.cell(range(10)),\n    net.cell(10),\n    IonotropicSynapse(),\n)\n\n# Change synaptic parameters.\nnet.select(edges=[0, 1]).set(\"IonotropicSynapse_gS\", 0.1)  # nS\n\n# Visualize the network.\nnet.compute_xyz()\nfig, ax = plt.subplots(1, 1, figsize=(4, 4))\nnet.vis(ax=ax, detail=\"full\", layers=[10, 1])  # or `detail=\"point\"`.\n</code></pre></p> <p>In the previous tutorial, you learned how to build single cells with morphological detail, how to insert stimuli and recordings, and how to run a first simulation. In this tutorial, we will define networks of multiple cells and connect them with synapses. Let\u2019s get started:</p> <pre><code>from jax import config\nconfig.update(\"jax_enable_x64\", True)\nconfig.update(\"jax_platform_name\", \"cpu\")\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport jax.numpy as jnp\nfrom jax import jit\n\nimport jaxley as jx\nfrom jaxley.channels import Na, K, Leak\nfrom jaxley.synapses import IonotropicSynapse\nfrom jaxley.connect import fully_connect, connect\n</code></pre>"},{"location":"tutorial/02_small_network/#define-the-network","title":"Define the network","text":"<p>First, we define a cell as you saw in the previous tutorial.</p> <pre><code>comp = jx.Compartment()\nbranch = jx.Branch(comp, ncomp=4)\ncell = jx.Cell(branch, parents=[-1, 0, 0, 1, 1, 2, 2])\n</code></pre> <p>We can assemble multiple cells into a network by using <code>jx.Network</code>, which takes a list of <code>jx.Cell</code>s. Here, we assemble 11 cells into a network:</p> <pre><code>num_cells = 11\nnet = jx.Network([cell for _ in range(num_cells)])\n</code></pre> <p>At this point, we can already visualize this network:</p> <pre><code>net.compute_xyz()\nnet.rotate(180)\nnet.arrange_in_layers(layers=[10, 1], within_layer_offset=150, between_layer_offset=200)\n\nfig, ax = plt.subplots(1, 1, figsize=(3, 6))\n_ = net.vis(ax=ax, detail=\"full\")\n</code></pre> <p></p> <p>Note: you can use <code>move_to</code> to have more control over the location of cells, e.g.: <code>network.cell(i).move_to(x=0, y=200)</code>.</p> <p>As you can see, the neurons are not connected yet. Let\u2019s fix this by connecting neurons with synapses. We will build a network consisting of two layers: 10 neurons in the input layer and 1 neuron in the output layer.</p> <p>We can use <code>Jaxley</code>\u2019s <code>fully_connect</code> method to connect these layers:</p> <pre><code>pre = net.cell(range(10))\npost = net.cell(10)\nfully_connect(pre, post, IonotropicSynapse())\n</code></pre> <p>Let\u2019s visualize this again:</p> <pre><code>fig, ax = plt.subplots(1, 1, figsize=(3, 6))\n_ = net.vis(ax=ax, detail=\"full\")\n</code></pre> <p></p> <p>As you can see, the <code>fully_connect</code> method inserted one synapse (in blue) from every neuron in the first layer to the output neuron. The <code>fully_connect</code> method builds this synapse from the zero-eth compartment and zero-eth branch of the presynaptic neuron onto the zero-eth compartment and zero-eth branch of the postsynaptic neuron by default. Allowing the post-synaptic compartment to be randomly chosen is also possible by setting <code>random_post_comp=True</code> in the <code>fully_connect</code> call. If you want more control over the pre- and post-synaptic branches, you can use the <code>connect</code> method:</p> <pre><code>pre = net.cell(0).branch(5).loc(1.0)\npost = net.cell(10).branch(0).loc(0.0)\nconnect(pre, post, IonotropicSynapse())\n</code></pre> <pre><code>fig, ax = plt.subplots(1, 1, figsize=(3, 6))\n_ = net.vis(ax=ax, detail=\"full\")\n</code></pre> <p></p>"},{"location":"tutorial/02_small_network/#inspecting-and-changing-synaptic-parameters","title":"Inspecting and changing synaptic parameters","text":"<p>You can inspect synaptic parameters via the <code>.edges</code> attribute:</p> <pre><code>net.edges\n</code></pre> global_edge_index pre_global_comp_index post_global_comp_index type type_ind pre_locs post_locs IonotropicSynapse_gS IonotropicSynapse_e_syn IonotropicSynapse_k_minus IonotropicSynapse_s controlled_by_param 0 0 0 280 IonotropicSynapse 0 0.125 0.125 0.0004 0.0 0.025 0.2 0 1 1 28 280 IonotropicSynapse 0 0.125 0.125 0.0004 0.0 0.025 0.2 0 2 2 56 280 IonotropicSynapse 0 0.125 0.125 0.0003 0.0 0.025 0.2 0 3 3 84 280 IonotropicSynapse 0 0.125 0.125 0.0003 0.0 0.025 0.2 0 4 4 112 280 IonotropicSynapse 0 0.125 0.125 0.0003 0.0 0.025 0.2 0 5 5 140 280 IonotropicSynapse 0 0.125 0.125 0.0003 0.0 0.025 0.2 0 6 6 168 280 IonotropicSynapse 0 0.125 0.125 0.0003 0.0 0.025 0.2 0 7 7 196 280 IonotropicSynapse 0 0.125 0.125 0.0003 0.0 0.025 0.2 0 8 8 224 280 IonotropicSynapse 0 0.125 0.125 0.0003 0.0 0.025 0.2 0 9 9 252 280 IonotropicSynapse 0 0.125 0.125 0.0003 0.0 0.025 0.2 0 10 10 23 280 IonotropicSynapse 0 0.875 0.125 0.0003 0.0 0.025 0.2 0 11 11 23 280 IonotropicSynapse 0 0.875 0.125 0.0001 0.0 0.025 0.2 0 <p>To modify a parameter of all synapses you can again use <code>.set()</code>:</p> <pre><code>net.set(\"IonotropicSynapse_gS\", 0.0003)  # nS\n</code></pre> <p>To modify individual syanptic parameters, use the <code>.select()</code> method. Below, we change the values of the first two synapses:</p> <pre><code>net.select(edges=[0, 1]).set(\"IonotropicSynapse_gS\", 0.0004)  # nS\n</code></pre> <p>For more details on how to flexibly set synaptic parameters (e.g., by cell type, or by pre-synaptic cell index,\u2026), see this tutorial.</p>"},{"location":"tutorial/02_small_network/#stimulating-recording-and-simulating-the-network","title":"Stimulating, recording, and simulating the network","text":"<p>We will now set up a simulation of the network. This works exactly as it does for single neurons:</p> <pre><code># Stimulus.\ni_delay = 3.0  # ms\ni_amp = 0.05  # nA\ni_dur = 2.0  # ms\n\n# Duration and step size.\ndt = 0.025  # ms\nt_max = 50.0  # ms\n</code></pre> <pre><code>time_vec = jnp.arange(0.0, t_max + dt, dt)\n</code></pre> <p>As a simple example, we insert sodium, potassium, and leak into every compartment of every cell of the network.</p> <pre><code>net.insert(Na())\nnet.insert(K())\nnet.insert(Leak())\n</code></pre> <p>We stimulate every neuron in the input layer and record the voltage from the output neuron:</p> <pre><code>current = jx.step_current(i_delay, i_dur, i_amp, dt, t_max)\nnet.delete_stimuli()\nfor stim_ind in range(10):\n    net.cell(stim_ind).branch(0).loc(0.0).stimulate(current)\n\nnet.delete_recordings()\nnet.cell(10).branch(0).loc(0.0).record()\n</code></pre> <pre><code>Added 1 external_states. See `.externals` for details.\nAdded 1 external_states. See `.externals` for details.\nAdded 1 external_states. See `.externals` for details.\nAdded 1 external_states. See `.externals` for details.\nAdded 1 external_states. See `.externals` for details.\nAdded 1 external_states. See `.externals` for details.\nAdded 1 external_states. See `.externals` for details.\nAdded 1 external_states. See `.externals` for details.\nAdded 1 external_states. See `.externals` for details.\nAdded 1 external_states. See `.externals` for details.\nAdded 1 recordings. See `.recordings` for details.\n</code></pre> <p>Finally, we can again run the network simulation and plot the result:</p> <pre><code>s = jx.integrate(net, delta_t=dt)\n</code></pre> <pre><code>fig, ax = plt.subplots(1, 1, figsize=(4, 2))\n_ = ax.plot(s.T)\n</code></pre> <p></p> <p>That\u2019s it! You now know how to simulate networks of morphologically detailed neurons. We recommend that you now have a look at how you can speed up your simulation. To learn more about handling synaptic parameters, we recommend to check out this tutorial.</p>"},{"location":"tutorial/04_jit_and_vmap/","title":"Speeding up simulations","text":"<p>In this tutorial, you will learn how to:</p> <ul> <li>make parameter sweeps in <code>Jaxley</code> </li> <li>use <code>jit</code> to compile your simulations and make them faster  </li> <li>use <code>vmap</code> to parallelize simulations on GPUs  </li> </ul> <p>Here is a code snippet which you will learn to understand in this tutorial: <pre><code>from jax import jit, vmap\n\n\ncell = ...  # See tutorial on Basics of Jaxley.\n\ndef simulate(params):\n    param_state = None\n    param_state = cell.data_set(\"Na_gNa\", params[0], param_state)\n    param_state = cell.data_set(\"K_gK\", params[1], param_state)\n    return jx.integrate(cell, param_state=param_state, delta_t=0.025)\n\n# Define 100 sets of sodium and potassium conductances.\nall_params = jnp.asarray(np.random.rand(100, 2))\n\n# Fast for-loops with jit compilation.\njitted_simulate = jit(simulate)\nvoltages = [jitted_simulate(params) for params in all_params]\n\n# Using vmap for parallelization.\nvmapped_simulate = vmap(jitted_simulate, in_axes=(0,))\nvoltages = vmapped_simulate(all_params)\n</code></pre></p> <p>In the previous tutorials, you learned how to build single cells or networks and how to change their parameters. In this tutorial, you will learn how to speed up such simulations by many orders of magnitude. This can be achieved in to ways:</p> <ul> <li>by using JIT compilation  </li> <li>by using GPU parallelization  </li> </ul> <p>Let\u2019s get started!</p>"},{"location":"tutorial/04_jit_and_vmap/#using-gpu-or-cpu","title":"Using GPU or CPU","text":"<p>In <code>Jaxley</code> you can set whether you want to use <code>gpu</code> or <code>cpu</code> with the following lines at the beginning of your script:</p> <pre><code>from jax import config\nconfig.update(\"jax_platform_name\", \"cpu\")\n</code></pre> <p><code>JAX</code> (and <code>Jaxley</code>) also allow to choose between <code>float32</code> and <code>float64</code>. Especially on GPUs, <code>float32</code> will be faster, but we have experienced stability issues when simulating morphologically detailed neurons with <code>float32</code>.</p> <pre><code>config.update(\"jax_enable_x64\", True)  # Set to false to use `float32`.\n</code></pre> <p>Next, we will import relevant libraries:</p> <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nimport jax.numpy as jnp\nfrom jax import jit, vmap\n\nimport jaxley as jx\nfrom jaxley.channels import Na, K, Leak\n</code></pre>"},{"location":"tutorial/04_jit_and_vmap/#building-the-cell-or-network","title":"Building the cell or network","text":"<p>We first build a cell (or network) in the same way as we showed in the previous tutorials:</p> <pre><code>dt = 0.025\nt_max = 10.0\n\ncomp = jx.Compartment()\nbranch = jx.Branch(comp, ncomp=4)\ncell = jx.Cell(branch, parents=[-1, 0, 0, 1, 1, 2, 2])\n\ncell.insert(Na())\ncell.insert(K())\ncell.insert(Leak())\n\ncell.delete_stimuli()\ncurrent = jx.step_current(i_delay=1.0, i_dur=1.0, i_amp=0.1, delta_t=dt, t_max=t_max)\ncell.branch(0).loc(0.0).stimulate(current)\n\ncell.delete_recordings()\ncell.branch(0).loc(0.0).record()\n</code></pre> <pre><code>Added 1 external_states. See `.externals` for details.\nAdded 1 recordings. See `.recordings` for details.\n</code></pre>"},{"location":"tutorial/04_jit_and_vmap/#parameter-sweeps","title":"Parameter sweeps","text":"<p>Assume you want to run the same cell with many different values for the sodium and potassium conductance, for example for genetic algorithms or for parameter sweeps. To do this efficiently in <code>Jaxley</code>, you have to use the <code>data_set()</code> method (in combination with <code>jit</code> and <code>vmap</code>, as shown later):</p> <pre><code>def simulate(params):\n    param_state = None\n    param_state = cell.data_set(\"Na_gNa\", params[0], param_state)\n    param_state = cell.data_set(\"K_gK\", params[1], param_state)\n    return jx.integrate(cell, param_state=param_state, delta_t=dt)\n</code></pre> <p>The <code>.data_set()</code> method takes three arguments: </p> <p>1) the name of the parameter you want to set. <code>Jaxley</code> allows to set the following parameters: \u201cradius\u201d, \u201clength\u201d, \u201caxial_resistivity\u201d, as well as all parameters of channels and synapses. 2) the value of the parameter. 3) a <code>param_state</code> which is initialized as <code>None</code> and is modified by <code>.data_set()</code>. This has to be passed to <code>jx.integrate()</code>.  </p> <p>Having done this, the simplest (but least efficient) way to perform the parameter sweep is to run a for-loop over many parameter sets:</p> <pre><code># Define 5 sets of sodium and potassium conductances.\nall_params = jnp.asarray(np.random.rand(5, 2))\n\nvoltages = jnp.asarray([simulate(params) for params in all_params])\nprint(\"voltages.shape\", voltages.shape)\n</code></pre> <pre><code>voltages.shape (5, 1, 402)\n</code></pre> <p>The resulting voltages have shape <code>(num_simulations, num_recordings, num_timesteps)</code>.</p>"},{"location":"tutorial/04_jit_and_vmap/#stimulus-sweeps","title":"Stimulus sweeps","text":"<p>In addition to running sweeps across multiple parameters, you can also run sweeeps across multiple stimuli (e.g. step current stimuli of different amplitudes. You can achieve this with the <code>data_stimulate()</code> method: <pre><code>def simulate(i_amp):\n    current = jx.step_current(1.0, 1.0, i_amp, 0.025, 10.0)\n\n    data_stimuli = None\n    data_stimuli = cell.branch(0).comp(0).data_stimulate(current, data_stimuli)\n    return jx.integrate(cell, data_stimuli=data_stimuli)\n</code></pre></p>"},{"location":"tutorial/04_jit_and_vmap/#speeding-up-for-loops-via-jit-compilation","title":"Speeding up for loops via <code>jit</code> compilation","text":"<p>We can speed up such parameter sweeps (or stimulus sweeps) with <code>jit</code> compilation. <code>jit</code> compilation will compile the simulation when it is run for the first time, such that every other simulation will be must faster. This can be achieved by defining a new function which uses <code>JAX</code>\u2019s <code>jit()</code>:</p> <pre><code>jitted_simulate = jit(simulate)\n</code></pre> <pre><code># First run, will be slow.\nvoltages = jitted_simulate(all_params[0])\n</code></pre> <pre><code># More runs, will be much faster.\nvoltages = jnp.asarray([jitted_simulate(params) for params in all_params])\nprint(\"voltages.shape\", voltages.shape)\n</code></pre> <pre><code>voltages.shape (5, 1, 402)\n</code></pre> <p><code>jit</code> compilation can be up to 10k times faster, especially for small simulations with few compartments. For very large models, the gain obtained with <code>jit</code> will be much smaller (<code>jit</code> may even provide no speed up at all).</p>"},{"location":"tutorial/04_jit_and_vmap/#speeding-up-with-gpu-parallelization-via-vmap","title":"Speeding up with GPU parallelization via <code>vmap</code>","text":"<p>Another way to speed up parameter sweeps is with GPU parallelization. Parallelization in <code>Jaxley</code> can be achieved by using <code>vmap</code> of <code>JAX</code>. To do this, we first create a new function that handles multiple parameter sets directly:</p> <pre><code># Using vmap for parallelization.\nvmapped_simulate = vmap(jitted_simulate)\n</code></pre> <p>We can then run this method on all parameter sets (<code>all_params.shape == (100, 2)</code>), and <code>Jaxley</code> will automatically parallelize across them. Of course, you will only get a speed-up if you have a GPU available and you specified <code>gpu</code> as device in the beginning of this tutorial.</p> <pre><code>voltages = vmapped_simulate(all_params)\n</code></pre> <p>GPU parallelization with <code>vmap</code> can give a large speed-up, which can easily be 2-3 orders of magnitude.</p>"},{"location":"tutorial/04_jit_and_vmap/#combining-jit-and-vmap","title":"Combining <code>jit</code> and <code>vmap</code>","text":"<p>Finally, you can also combine using <code>jit</code> and <code>vmap</code>. For example, you can run multiple batches of many parallel simulations. Each batch can be parallelized with <code>vmap</code> and simulating each batch can be compiled with <code>jit</code>:</p> <pre><code>jitted_vmapped_simulate = jit(vmap(simulate))\n</code></pre> <pre><code>for batch in range(10):\n    all_params = jnp.asarray(np.random.rand(5, 2))\n    voltages_batch = jitted_vmapped_simulate(all_params)\n</code></pre> <p>That\u2019s all you have to know about <code>jit</code> and <code>vmap</code>! If you have worked through this and the previous tutorials, you should be ready to set up your first network simulations.</p>"},{"location":"tutorial/04_jit_and_vmap/#next-steps","title":"Next steps","text":"<p>If you want to learn more, we recommend you to read the tutorial on building channel and synapse models.</p> <p>Alternatively, you can also directly jump ahead to the tutorial on training biophysical networks which will teach you how you can optimize parameters of biophysical models with gradient descent.</p> <p>Finally, if you want to learn more about JAX, check out their tutorial on jit or their tutorial on vmap.</p>"},{"location":"tutorial/05_channel_and_synapse_models/","title":"Building ion channels and synapses","text":"<p>In this tutorial, you will learn how to:</p> <ul> <li>define your own ion channel models beyond the preconfigured channels in <code>Jaxley</code> </li> <li>define your own synapse models  </li> </ul> <p>This tutorial assumes that you have already learned how to build basic simulations.</p> <pre><code>from jax import config\nconfig.update(\"jax_enable_x64\", True)\nconfig.update(\"jax_platform_name\", \"cpu\")\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport jax\nimport jax.numpy as jnp\nfrom jax import jit, value_and_grad\n\nimport jaxley as jx\n</code></pre> <p>First, we define a cell as you saw in the previous tutorial:</p> <pre><code>comp = jx.Compartment()\nbranch = jx.Branch(comp, ncomp=4)\ncell = jx.Cell(branch, parents=[-1, 0, 0, 1, 1, 2, 2])\n</code></pre> <p>You have also already learned how to insert preconfigured channels into <code>Jaxley</code> models: <pre><code>cell.insert(Na())\ncell.insert(K())\ncell.insert(Leak())\n</code></pre></p> <p>In this tutorial, we will show you how to build your own channel and synapse models. Alternatively, the Python toolbox <code>DendroTweaks</code> has built a tool to convert <code>NMODL</code> files to <code>Jaxley</code> channels, available here. We would appreciate both positive and negative experiences with this conversion tool: Please post feedback in the discussion here.</p>"},{"location":"tutorial/05_channel_and_synapse_models/#your-own-channel","title":"Your own channel","text":"<p>Below is how you can define your own channel. We will go into detail about individual parts of the code in the next couple of cells.</p> <pre><code>import jax.numpy as jnp\nfrom jaxley.channels import Channel\nfrom jaxley.solver_gate import solve_gate_exponential\n\n\ndef exp_update_alpha(x, y):\n    return x / (jnp.exp(x / y) - 1.0)\n\nclass Potassium(Channel):\n    \"\"\"Potassium channel.\"\"\"\n\n    def __init__(self, name = None):\n        self.current_is_in_mA_per_cm2 = True\n        super().__init__(name)\n        self.channel_params = {\"gK_new\": 1e-4}\n        self.channel_states = {\"n_new\": 0.0}\n        self.current_name = \"i_K\"\n\n    def update_states(self, states, dt, v, params):\n        \"\"\"Update state.\"\"\"\n        ns = states[\"n_new\"]\n        alpha = 0.01 * exp_update_alpha(-(v + 55), 10)\n        beta = 0.125 * jnp.exp(-(v + 65) / 80)\n        new_n = solve_gate_exponential(ns, dt, alpha, beta)\n        return {\"n_new\": new_n}\n\n    def compute_current(self, states, v, params):\n        \"\"\"Return current.\"\"\"\n        ns = states[\"n_new\"]\n        kd_conds = params[\"gK_new\"] * ns**4  # S/cm^2\n\n        e_kd = -77.0        \n        return kd_conds * (v - e_kd)\n\n    def init_state(self, states, v, params, delta_t):\n        alpha = 0.01 * exp_update_alpha(-(v + 55), 10)\n        beta = 0.125 * jnp.exp(-(v + 65) / 80)\n        return {\"n_new\": alpha / (alpha + beta)}\n</code></pre> <p>Let\u2019s look at each part of this in detail. </p> <p>The below is simply a helper function for the solver of the gate variables: <pre><code>def exp_update_alpha(x, y):\n    return x / (jnp.exp(x / y) - 1.0)\n</code></pre></p> <p>Next, we define our channel as a class. It should inherit from the <code>Channel</code> class and define <code>channel_params</code>, <code>channel_states</code>, and <code>current_name</code>. You also need to set <code>self.current_is_in_mA_per_cm2=True</code> as the first line on your <code>__init__()</code> method. This is to acknowledge that your current is returned in <code>mA/cm2</code> (not in <code>uA/cm2</code>, as would have been required in Jaxley versions 0.4.0 or older). <pre><code>class Potassium(Channel):\n    \"\"\"Potassium channel.\"\"\"\n\n    def __init__(self, name=None):\n        self.current_is_in_mA_per_cm2 = True\n        super().__init__(name)\n        self.channel_params = {\"gK_new\": 1e-4}\n        self.channel_states = {\"n_new\": 0.0}\n        self.current_name = \"i_K\"\n</code></pre></p> <p>Next, we have the <code>update_states()</code> method, which updates the gating variables: <pre><code>    def update_states(self, states, dt, v, params):\n</code></pre></p> <p>Every channel you define must have an <code>update_states()</code> method which takes exactly these five arguments (self, states, dt, v, params). The inputs <code>states</code> to the <code>update_states</code> method is a dictionary which contains all states that are updated (including states of other channels). <code>v</code> is a <code>jnp.ndarray</code> which contains the voltage of a single compartment (shape <code>()</code>). Let\u2019s get the state of the potassium channel which we are building here: <pre><code>ns = states[\"n_new\"]\n</code></pre></p> <p>Next, we update the state of the channel. In this example, we do this with exponential Euler, but you can implement any solver yourself: <pre><code>alpha = 0.01 * exp_update_alpha(-(v + 55), 10)\nbeta = 0.125 * jnp.exp(-(v + 65) / 80)\nnew_n = solve_gate_exponential(ns, dt, alpha, beta)\nreturn {\"n_new\": new_n}\n</code></pre></p> <p>A channel also needs a <code>compute_current()</code> method which returns the current through the channel: <pre><code>    def compute_current(self, states, v, params):\n        ns = states[\"n_new\"]\n\n        # Multiply with 1000 to convert Siemens to milli Siemens.\n        kd_conds = params[\"gK_new\"] * ns**4  # S/cm^2\n\n        e_kd = -77.0        \n        current = kd_conds * (v - e_kd)\n        return current\n</code></pre></p> <p>Finally, the <code>init_state()</code> method can be implemented optionally. It can be used to automatically compute the initial state based on the voltage when <code>cell.init_states()</code> is run.</p> <p>Alright, done! We can now insert this channel into any <code>jx.Module</code> such as our cell:</p> <pre><code>cell.insert(Potassium())\n</code></pre> <pre><code>cell.delete_stimuli()\ncurrent = jx.step_current(1.0, 1.0, 0.1, 0.025, 10.0)\ncell.branch(0).comp(0).stimulate(current)\n\ncell.delete_recordings()\ncell.branch(0).comp(0).record()\n</code></pre> <pre><code>Added 1 external_states. See `.externals` for details.\nAdded 1 recordings. See `.recordings` for details.\n</code></pre> <pre><code>s = jx.integrate(cell)\n</code></pre> <pre><code>fig, ax = plt.subplots(1, 1, figsize=(4, 2))\n_ = ax.plot(s.T[:-1])\n_ = ax.set_ylim([-80, 50])\n_ = ax.set_xlabel(\"Time (ms)\")\n_ = ax.set_ylabel(\"Voltage (mV)\")\n</code></pre> <p></p> <p>If you want to set up detailed biophysical models using ion dynamics (e.g., ion pumps and ion diffusion), then we recommend reading this tutorial.</p>"},{"location":"tutorial/05_channel_and_synapse_models/#your-own-synapse","title":"Your own synapse","text":"<p>The parts below assume that you have already learned how to build network simulations in <code>Jaxley</code>.</p> <p>Note that again, a synapse needs to have the two functions <code>update_states</code> and <code>compute_current</code> with all input arguments shown below. </p> <p>The below is an example of how to define your own synapse model in <code>Jaxley</code>:</p> <pre><code>import jax.numpy as jnp\nfrom jaxley.synapses.synapse import Synapse\n\n\nclass TestSynapse(Synapse):\n    \"\"\"\n    Compute syanptic current and update syanpse state.\n    \"\"\"\n    def __init__(self, name = None):\n        super().__init__(name)\n        self.synapse_params = {\"gChol\": 0.001, \"eChol\": 0.0}\n        self.synapse_states = {\"s_chol\": 0.1}\n\n    def update_states(self, states, delta_t, pre_voltage, post_voltage, params):\n        \"\"\"Return updated synapse state and current.\"\"\"\n        s_inf = 1.0 / (1.0 + jnp.exp((-35.0 - pre_voltage) / 10.0))\n        exp_term = jnp.exp(-delta_t)\n        new_s = states[\"s_chol\"] * exp_term + s_inf * (1.0 - exp_term)\n        return {\"s_chol\": new_s}\n\n    def compute_current(self, states, pre_voltage, post_voltage, params):\n        g_syn = params[\"gChol\"] * states[\"s_chol\"]\n        return g_syn * (post_voltage - params[\"eChol\"])\n</code></pre> <p>As you can see above, synapses follow closely how channels are defined. The main difference is that the <code>compute_current</code> method takes two voltages: the pre-synaptic voltage (a <code>jnp.ndarray</code> of shape <code>()</code>) and the post-synaptic voltage (a <code>jnp.ndarray</code> of shape <code>()</code>).</p> <pre><code>net = jx.Network([cell for _ in range(3)])\n</code></pre> <pre><code>from jaxley.connect import connect\n\npre = net.cell(0).branch(0).loc(0.0)\npost = net.cell(1).branch(0).loc(0.0)\nconnect(pre, post, TestSynapse())\n</code></pre> <pre><code>net.cell(0).branch(0).loc(0.0).stimulate(jx.step_current(1.0, 2.0, 0.1, 0.025, 10.0))\nfor i in range(3):\n    net.cell(i).branch(0).loc(0.0).record()\n</code></pre> <pre><code>Added 1 external_states. See `.externals` for details.\nAdded 1 recordings. See `.recordings` for details.\nAdded 1 recordings. See `.recordings` for details.\nAdded 1 recordings. See `.recordings` for details.\n</code></pre> <pre><code>s = jx.integrate(net)\n</code></pre> <pre><code>fig, ax = plt.subplots(1, 1, figsize=(4, 2))\n_ = ax.plot(s.T[:-1])\n_ = ax.set_ylim([-80, 50])\n_ = ax.set_xlabel(\"Time (ms)\")\n_ = ax.set_ylabel(\"Voltage (mV)\")\n</code></pre> <p></p> <p>That\u2019s it! You are now ready to build your own custom simulations and equip them with channel and synapse models!</p> <p>If you want to set up detailed biophysical models using ion dynamics (e.g., ion pumps and ion diffusion), then we recommend reading this tutorial. If you have not done so already, you can check out our tutorial on training biophysical networks which will teach you how you can optimize parameters of biophysical models with gradient descent.</p>"},{"location":"tutorial/06_groups/","title":"Defining groups","text":"<p>In this tutorial, you will learn how to:</p> <ul> <li>define groups (aka sectionlists) to simplify interactions with <code>Jaxley</code> </li> </ul> <p>Here is a code snippet which you will learn to understand in this tutorial: <pre><code>from jax import jit, vmap\n\n\nnet = ...  # See tutorial on Basics of Jaxley.\n\nnet.cell(0).add_to_group(\"fast_spiking\")\nnet.cell(1).add_to_group(\"slow_spiking\")\n\ndef simulate(params):\n    param_state = None\n    param_state = net.fast_spiking.data_set(\"HH_gNa\", params[0], param_state)\n    param_state = net.slow_spiking.data_set(\"HH_gNa\", params[1], param_state)\n    return jx.integrate(net, param_state=param_state)\n\n# Define sodium for fast and slow spiking neurons.\nparams = jnp.asarray([1.0, 0.1])\n\n# Run simulation.\nvoltages = simulate(params)\n</code></pre></p> <p>In many cases, you might want to group several compartments (or branches, or cells) and assign a unique parameter or mechanism to this group. For example, you might want to define a couple of branches as basal and then assign a Hodgkin-Huxley mechanism only to those branches. Or you might define a couple of cells as fast spiking and assign them a high value for the sodium conductance. We describe how you can do this in this tutorial.</p> <pre><code>from jax import config\nconfig.update(\"jax_enable_x64\", True)\nconfig.update(\"jax_platform_name\", \"cpu\")\n\nimport time\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport jax\nimport jax.numpy as jnp\nfrom jax import jit, value_and_grad\n\nimport jaxley as jx\nfrom jaxley.channels import Na, K, Leak\nfrom jaxley.synapses import IonotropicSynapse\nfrom jaxley.connect import fully_connect\n</code></pre> <p>First, we define a network as you saw in the previous tutorial:</p> <pre><code>comp = jx.Compartment()\nbranch = jx.Branch(comp, ncomp=2)\ncell = jx.Cell(branch, parents=[-1, 0, 0, 1])\nnetwork = jx.Network([cell for _ in range(3)])\n\npre = network.cell([0, 1])\npost = network.cell([2])\nfully_connect(pre, post, IonotropicSynapse())\n\nnetwork.insert(Na())\nnetwork.insert(K())\nnetwork.insert(Leak())\n</code></pre>"},{"location":"tutorial/06_groups/#group-apical-dendrites","title":"Group: apical dendrites","text":"<p>Assume that, in each of the five neurons in this network, the second and forth branch are apical dendrites. We can define this as:</p> <pre><code>for cell_ind in range(3):\n    network.cell(cell_ind).branch(1).add_to_group(\"apical\")\n    network.cell(cell_ind).branch(3).add_to_group(\"apical\")\n</code></pre> <p>After this, we can access <code>network.apical</code> as we previously accesses anything else:</p> <pre><code>network.apical.set(\"radius\", 0.3)\n</code></pre> <pre><code>network.apical.nodes\n</code></pre> local_cell_index local_branch_index local_comp_index length radius axial_resistivity capacitance v Na Na_gNa ... eK K_n Leak Leak_gLeak Leak_eLeak apical global_cell_index global_branch_index global_comp_index controlled_by_param 2 0 0 0 10.0 0.3 5000.0 1.0 -70.0 True 0.05 ... -90.0 0.2 True 0.0001 -70.0 True 0 1 2 0 3 0 0 1 10.0 0.3 5000.0 1.0 -70.0 True 0.05 ... -90.0 0.2 True 0.0001 -70.0 True 0 1 3 0 6 0 1 0 10.0 0.3 5000.0 1.0 -70.0 True 0.05 ... -90.0 0.2 True 0.0001 -70.0 True 0 3 6 0 7 0 1 1 10.0 0.3 5000.0 1.0 -70.0 True 0.05 ... -90.0 0.2 True 0.0001 -70.0 True 0 3 7 0 10 1 0 0 10.0 0.3 5000.0 1.0 -70.0 True 0.05 ... -90.0 0.2 True 0.0001 -70.0 True 1 5 10 0 11 1 0 1 10.0 0.3 5000.0 1.0 -70.0 True 0.05 ... -90.0 0.2 True 0.0001 -70.0 True 1 5 11 0 14 1 1 0 10.0 0.3 5000.0 1.0 -70.0 True 0.05 ... -90.0 0.2 True 0.0001 -70.0 True 1 7 14 0 15 1 1 1 10.0 0.3 5000.0 1.0 -70.0 True 0.05 ... -90.0 0.2 True 0.0001 -70.0 True 1 7 15 0 18 2 0 0 10.0 0.3 5000.0 1.0 -70.0 True 0.05 ... -90.0 0.2 True 0.0001 -70.0 True 2 9 18 0 19 2 0 1 10.0 0.3 5000.0 1.0 -70.0 True 0.05 ... -90.0 0.2 True 0.0001 -70.0 True 2 9 19 0 22 2 1 0 10.0 0.3 5000.0 1.0 -70.0 True 0.05 ... -90.0 0.2 True 0.0001 -70.0 True 2 11 22 0 23 2 1 1 10.0 0.3 5000.0 1.0 -70.0 True 0.05 ... -90.0 0.2 True 0.0001 -70.0 True 2 11 23 0 <p>12 rows \u00d7 26 columns</p>"},{"location":"tutorial/06_groups/#group-fast-spiking","title":"Group: fast spiking","text":"<p>Similarly, you could define a group of fast-spiking cells. Assume that the first and second cell are fast-spiking:</p> <pre><code>network.cell(0).add_to_group(\"fast_spiking\")\nnetwork.cell(1).add_to_group(\"fast_spiking\")\n</code></pre> <pre><code>network.fast_spiking.set(\"Na_gNa\", 0.4)\n</code></pre> <pre><code>network.fast_spiking.nodes\n</code></pre> local_cell_index local_branch_index local_comp_index length radius axial_resistivity capacitance v Na Na_gNa ... K_n Leak Leak_gLeak Leak_eLeak apical fast_spiking global_cell_index global_branch_index global_comp_index controlled_by_param 0 0 0 0 10.0 1.0 5000.0 1.0 -70.0 True 0.4 ... 0.2 True 0.0001 -70.0 False True 0 0 0 0 1 0 0 1 10.0 1.0 5000.0 1.0 -70.0 True 0.4 ... 0.2 True 0.0001 -70.0 False True 0 0 1 0 2 0 1 0 10.0 0.3 5000.0 1.0 -70.0 True 0.4 ... 0.2 True 0.0001 -70.0 True True 0 1 2 0 3 0 1 1 10.0 0.3 5000.0 1.0 -70.0 True 0.4 ... 0.2 True 0.0001 -70.0 True True 0 1 3 0 4 0 2 0 10.0 1.0 5000.0 1.0 -70.0 True 0.4 ... 0.2 True 0.0001 -70.0 False True 0 2 4 0 5 0 2 1 10.0 1.0 5000.0 1.0 -70.0 True 0.4 ... 0.2 True 0.0001 -70.0 False True 0 2 5 0 6 0 3 0 10.0 0.3 5000.0 1.0 -70.0 True 0.4 ... 0.2 True 0.0001 -70.0 True True 0 3 6 0 7 0 3 1 10.0 0.3 5000.0 1.0 -70.0 True 0.4 ... 0.2 True 0.0001 -70.0 True True 0 3 7 0 8 1 0 0 10.0 1.0 5000.0 1.0 -70.0 True 0.4 ... 0.2 True 0.0001 -70.0 False True 1 4 8 0 9 1 0 1 10.0 1.0 5000.0 1.0 -70.0 True 0.4 ... 0.2 True 0.0001 -70.0 False True 1 4 9 0 10 1 1 0 10.0 0.3 5000.0 1.0 -70.0 True 0.4 ... 0.2 True 0.0001 -70.0 True True 1 5 10 0 11 1 1 1 10.0 0.3 5000.0 1.0 -70.0 True 0.4 ... 0.2 True 0.0001 -70.0 True True 1 5 11 0 12 1 2 0 10.0 1.0 5000.0 1.0 -70.0 True 0.4 ... 0.2 True 0.0001 -70.0 False True 1 6 12 0 13 1 2 1 10.0 1.0 5000.0 1.0 -70.0 True 0.4 ... 0.2 True 0.0001 -70.0 False True 1 6 13 0 14 1 3 0 10.0 0.3 5000.0 1.0 -70.0 True 0.4 ... 0.2 True 0.0001 -70.0 True True 1 7 14 0 15 1 3 1 10.0 0.3 5000.0 1.0 -70.0 True 0.4 ... 0.2 True 0.0001 -70.0 True True 1 7 15 0 <p>16 rows \u00d7 27 columns</p>"},{"location":"tutorial/06_groups/#groups-from-swc-files","title":"Groups from SWC files","text":"<p>If you are reading <code>.swc</code> morphologigies, you can automatically assign groups with  <pre><code>jx.read_swc(file_name, ncomp=n, assign_groups=True)  # assign_groups=True is the default\n</code></pre> After that, you can directly use <code>cell.soma</code>, <code>cell.apical</code>, <code>cell.basal</code>, or <code>cell.axon</code>.</p>"},{"location":"tutorial/06_groups/#how-groups-are-interpreted-by-make_trainable","title":"How groups are interpreted by <code>.make_trainable()</code>","text":"<p>If you make a parameter of a <code>group</code> trainable, then it will be treated as a single shared parameter for a given property:</p> <pre><code>network.fast_spiking.make_trainable(\"Na_gNa\")\n</code></pre> <pre><code>Number of newly added trainable parameters: 1. Total number of trainable parameters: 1\n</code></pre> <p>As such, <code>get_parameters()</code> returns only a single trainable parameter, which will be the sodium conductance for every compartment of every fast-spiking neuron:</p> <pre><code>network.get_parameters()\n</code></pre> <pre><code>[{'Na_gNa': Array([0.4], dtype=float64)}]\n</code></pre> <p>If, instead, you would want a separate parameter for every fast-spiking cell, you should not use the group, but instead do the following (remember that fast-spiking neurons had indices [0,1]):</p> <pre><code>network.cell([0,1]).make_trainable(\"axial_resistivity\")\n</code></pre> <pre><code>Number of newly added trainable parameters: 2. Total number of trainable parameters: 3\n</code></pre> <pre><code>network.get_parameters()\n</code></pre> <pre><code>[{'Na_gNa': Array([0.4], dtype=float64)},\n {'axial_resistivity': Array([5000., 5000.], dtype=float64)}]\n</code></pre> <p>This generated two parameters for the axial resistivitiy, each corresponding to one cell.</p>"},{"location":"tutorial/06_groups/#summary","title":"Summary","text":"<p>Groups allow you to organize your simulation in a more intuitive way, and they allow to perform parameter sharing with <code>make_trainable()</code>.</p>"},{"location":"tutorial/07_gradient_descent/","title":"Training biophysical models","text":"<p>In this tutorial, you will learn how to train biophysical models in <code>Jaxley</code>. This includes the following:</p> <ul> <li>compute the gradient with respect to parameters  </li> <li>use parameter transformations  </li> <li>use multi-level checkpointing  </li> <li>define optimizers  </li> <li>write dataloaders and parallelize across data  </li> </ul> <p>Here is a code snippet which you will learn to understand in this tutorial: <pre><code>from jax import jit, vmap, value_and_grad\nimport jaxley as jx\nimport jaxley.optimize.transforms as jt\n\nnet = ...  # See tutorial on the basics of `Jaxley`.\n\n# Define which parameters to train.\nnet.cell(\"all\").make_trainable(\"HH_gNa\")\nnet.IonotropicSynapse.make_trainable(\"IonotropicSynapse_gS\")\nparameters = net.get_parameters()\n\n# Define parameter transform and apply it to the parameters.\ntransform = jx.ParamTransform([\n    {\"HH_gNa\": jt.SigmoidTransform(0.0, 1.0)},\n    {\"IonotropicSynapse_gS\": jt.SigmoidTransform(0.0, 1.0)},\n])\n\nopt_params = transform.inverse(parameters)\n\n# Define simulation and batch it across stimuli.\ndef simulate(params, datapoint):\n    # Define stimuli.\n    currents = jx.datapoint_to_step_currents(i_delay=1.0, i_dur=1.0, i_amp=datapoint, delta_t=0.025, t_max=5.0)\n    data_stimuli = None\n    data_stimuli = net.cell(0).branch(0).comp(0).data_stimulate(currents[0], data_stimuli)\n    data_stimuli = net.cell(1).branch(0).comp(0).data_stimulate(currents[1], data_stimuli)\n    return jx.integrate(net, params=params, data_stimuli=data_stimuli, checkpoint_lengths=[15, 15], delta_t=0.025)\n\nbatch_simulate = vmap(simulate, in_axes=(None, 0))\n\n# Define loss function and its gradient.\ndef loss_fn(opt_params, datapoints, label):\n    params = transform.forward(opt_params)\n    voltages = batch_simulate(params, datapoints)\n    return jnp.mean(jnp.abs(jnp.mean(voltages) - label))\n\ngrad_fn = jit(value_and_grad(loss_fn, argnums=0))\n\n# Define data and labels.\ninput_dim = 2\ndata = jnp.asarray(np.random.randn(100, input_dim))\nnum_classes = 2\nlabels = jnp.asarray(np.random.randint(num_classes, size=100))\n\n# Define dataloader (see tutorial below for a minimal example of the `Dataset` class).\ndataloader = Dataset(inputs, labels)\ndataloader = dataloader.shuffle(seed=0).batch(4)\n\n# Define the optimizer.\noptimizer = optax.adam(learning_rate=0.01)\nopt_state = optimizer.init(opt_params)\n\nfor epoch in range(10):\n    for batch_ind, batch in enumerate(dataloader):\n        stim_batch, label_batch = batch\n        loss, gradient = grad_fn(opt_params, stim_batch, label_batch)\n\n        # Optimizer step.\n        updates, opt_state = optimizer.update(gradient, opt_state)\n        opt_params = optax.apply_updates(opt_params, updates)\n</code></pre></p> <p>Let\u2019s get started!</p> <pre><code>from jax import config\nconfig.update(\"jax_enable_x64\", True)\nconfig.update(\"jax_platform_name\", \"cpu\")\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport jax\nimport jax.numpy as jnp\nfrom jax import jit, vmap, value_and_grad\n\nimport jaxley as jx\nfrom jaxley.channels import Leak\nfrom jaxley.synapses import TanhRateSynapse\nfrom jaxley.connect import fully_connect\n</code></pre> <p>First, we define a network as you saw in the previous tutorial:</p> <pre><code>_ = np.random.seed(0)  # For synaptic locations.\n\ncomp = jx.Compartment()\nbranch = jx.Branch(comp, ncomp=2)\ncell = jx.Cell(branch, parents=[-1, 0, 0])\nnet = jx.Network([cell for _ in range(3)])\n\npre = net.cell([0, 1])\npost = net.cell([2])\nfully_connect(pre, post, TanhRateSynapse())\n\n# Change some default values of the tanh synapse.\nnet.TanhRateSynapse.set(\"TanhRateSynapse_x_offset\", -60.0)\nnet.TanhRateSynapse.set(\"TanhRateSynapse_gS\", 1e-3)\nnet.TanhRateSynapse.set(\"TanhRateSynapse_slope\", 0.1)\n\nnet.insert(Leak())\n</code></pre> <p>This network consists of three neurons arranged in two layers:</p> <pre><code>net.compute_xyz()\nnet.rotate(180)\nnet.arrange_in_layers(layers=[2, 1], within_layer_offset=100.0, between_layer_offset=100.0)\nfig, ax = plt.subplots(1, 1, figsize=(3, 2))\n_ = net.vis(ax=ax, detail=\"full\")\n</code></pre> <p></p> <p>We consider the last neuron as the output neuron and record the voltage from there:</p> <pre><code>net.delete_recordings()\nnet.cell(0).branch(0).loc(0.0).record()\nnet.cell(1).branch(0).loc(0.0).record()\nnet.cell(2).branch(0).loc(0.0).record()\n</code></pre> <pre><code>Added 1 recordings. See `.recordings` for details.\nAdded 1 recordings. See `.recordings` for details.\nAdded 1 recordings. See `.recordings` for details.\n</code></pre>"},{"location":"tutorial/07_gradient_descent/#defining-a-dataset","title":"Defining a dataset","text":"<p>We will train this biophysical network on a classification task. The inputs will be values and the label is binary:</p> <pre><code>inputs = jnp.asarray(np.random.rand(100, 2))\nlabels = jnp.asarray((inputs[:, 0] + inputs[:, 1]) &gt; 1.0)\n</code></pre> <pre><code>fig, ax = plt.subplots(1, 1, figsize=(3, 2))\n_ = ax.scatter(inputs[labels, 0], inputs[labels, 1])\n_ = ax.scatter(inputs[~labels, 0], inputs[~labels, 1])\n</code></pre> <p></p> <pre><code>labels = labels.astype(float)\n</code></pre>"},{"location":"tutorial/07_gradient_descent/#defining-trainable-parameters","title":"Defining trainable parameters","text":"<pre><code>net.delete_trainables()\n</code></pre> <p>This follows the same API as <code>.set()</code> seen in the previous tutorial. If you want to use a single parameter for all <code>radius</code>es in the entire network, do:</p> <pre><code>net.make_trainable(\"radius\")\n</code></pre> <pre><code>Number of newly added trainable parameters: 1. Total number of trainable parameters: 1\n</code></pre> <p>We can also define parameters for individual compartments. To do this, use the <code>\"all\"</code> key. The following defines a separate parameter the sodium conductance for every compartment in the entire network:</p> <pre><code>net.cell(\"all\").branch(\"all\").loc(\"all\").make_trainable(\"Leak_gLeak\")\n</code></pre> <pre><code>Number of newly added trainable parameters: 18. Total number of trainable parameters: 19\n</code></pre>"},{"location":"tutorial/07_gradient_descent/#making-synaptic-parameters-trainable","title":"Making synaptic parameters trainable","text":"<p>Synaptic parameters can be made trainable in the exact same way. To use a single parameter for all syanptic conductances in the entire network, do <pre><code>net.TanhRateSynapse.make_trainable(\"TanhRateSynapse_gS\")\n</code></pre></p> <p>Here, we use a different syanptic conductance for all syanpses. This can be done as follows:</p> <pre><code>net.TanhRateSynapse.edge(\"all\").make_trainable(\"TanhRateSynapse_gS\")\n</code></pre> <pre><code>Number of newly added trainable parameters: 2. Total number of trainable parameters: 21\n</code></pre>"},{"location":"tutorial/07_gradient_descent/#running-the-simulation","title":"Running the simulation","text":"<p>Once all parameters are defined, you have to use <code>.get_parameters()</code> to obtain all trainable parameters. This is also the time to check how many trainable parameters your network has:</p> <pre><code>params = net.get_parameters()\n</code></pre> <p>You can now run the simulation with the trainable parameters by passing them to the <code>jx.integrate</code> function.</p> <pre><code>s = jx.integrate(net, params=params, t_max=10.0)\n</code></pre>"},{"location":"tutorial/07_gradient_descent/#stimulating-the-network","title":"Stimulating the network","text":"<p>The network above does not yet get any stimuli. We will use the 2D inputs from the dataset to stimulate the two input neurons. The amplitude of the step current corresponds to the input value. Below is the simulator that defines this:</p> <pre><code>def simulate(params, inputs):\n    currents = jx.datapoint_to_step_currents(i_delay=1.0, i_dur=1.0, i_amp=inputs / 10, delta_t=0.025, t_max=10.0)\n\n    data_stimuli = None\n    data_stimuli = net.cell(0).branch(2).loc(1.0).data_stimulate(currents[0], data_stimuli=data_stimuli)\n    data_stimuli = net.cell(1).branch(2).loc(1.0).data_stimulate(currents[1], data_stimuli=data_stimuli)\n\n    return jx.integrate(net, params=params, data_stimuli=data_stimuli, delta_t=0.025)\n\nbatched_simulate = vmap(simulate, in_axes=(None, 0))\n</code></pre> <p>We can also inspect some traces:</p> <pre><code>traces = batched_simulate(params, inputs[:4])\n</code></pre> <pre><code>fig, ax = plt.subplots(1, 1, figsize=(4, 2))\n_ = ax.plot(traces[:, 2, :].T)\n</code></pre> <p></p>"},{"location":"tutorial/07_gradient_descent/#defining-a-loss-function","title":"Defining a loss function","text":"<p>Let us define a loss function to be optimized:</p> <pre><code>def loss(params, inputs, labels):\n    traces = batched_simulate(params, inputs)  # Shape `(batchsize, num_recordings, timepoints)`.\n    prediction = jnp.mean(traces[:, 2], axis=1)  # Use the average over time of the output neuron (2) as prediction.\n    prediction = (prediction + 72.0) / 5  # Such that the prediction is roughly in [0, 1].\n    losses = jnp.abs(prediction - labels)  # Mean absolute error loss.\n    return jnp.mean(losses)  # Average across the batch.\n</code></pre> <p>And we can use <code>JAX</code>\u2019s inbuilt functions to take the gradient through the entire ODE:</p> <pre><code>jitted_grad = jit(value_and_grad(loss, argnums=0))\n</code></pre> <pre><code>value, gradient = jitted_grad(params, inputs[:4], labels[:4])\n</code></pre>"},{"location":"tutorial/07_gradient_descent/#defining-parameter-transformations","title":"Defining parameter transformations","text":"<p>Before training, however, we will enforce for all parameters to be within a prespecified range (such that, e.g., conductances can not become negative)</p> <pre><code>import jaxley.optimize.transforms as jt\n</code></pre> <pre><code># Define a function to create appropriate transforms for each parameter\ndef create_transform(name):\n    if name == \"axial_resistivity\":\n        # Must be positive; apply Softplus and scale to match initialization\n        return jt.ChainTransform([jt.SoftplusTransform(0), jt.AffineTransform(5000, 0)])\n    elif name == \"length\":\n        # Apply Softplus and affine transform for the 'length' parameter\n        return jt.ChainTransform([jt.SoftplusTransform(0), jt.AffineTransform(10, 0)])\n    else:\n        # Default to a Softplus transform for other parameters\n        return jt.SoftplusTransform(0)\n\n# Apply the transforms to the parameters\ntransforms = [{k: create_transform(k) for k in param} for param in params]\ntf = jt.ParamTransform(transforms)\n</code></pre> <pre><code>transform = jx.ParamTransform([{\"radius\": jt.SigmoidTransform(0.1, 5.0)},\n                               {\"Leak_gLeak\":jt.SigmoidTransform(1e-5, 1e-3)},\n                               {\"TanhRateSynapse_gS\" : jt.SigmoidTransform(1e-5, 1e-2)}])\n</code></pre> <p>With these  modify the loss function acocrdingly:</p> <pre><code>def loss(opt_params, inputs, labels):\n    transform.forward(opt_params)\n\n    traces = batched_simulate(params, inputs)  # Shape `(batchsize, num_recordings, timepoints)`.\n    prediction = jnp.mean(traces[:, 2], axis=1)  # Use the average over time of the output neuron (2) as prediction.\n    prediction = (prediction + 72.0)  # Such that the prediction is around 0.\n    losses = jnp.abs(prediction - labels)  # Mean absolute error loss.\n    return jnp.mean(losses)  # Average across the batch.\n</code></pre>"},{"location":"tutorial/07_gradient_descent/#using-checkpointing","title":"Using checkpointing","text":"<p>Checkpointing allows to vastly reduce the memory requirements of training biophysical models (see also JAX\u2019s full tutorial on checkpointing).</p> <pre><code>t_max = 5.0\ndt = 0.025\n\nlevels = 2\ntime_points = t_max // dt + 2\ncheckpoints = [int(np.ceil(time_points**(1/levels))) for _ in range(levels)]\n</code></pre> <p>To enable checkpointing, we have to modify the <code>simulate</code> function appropriately and use <pre><code>jx.integrate(..., checkpoint_inds=checkpoints)\n</code></pre> as done below:</p> <pre><code>def simulate(params, inputs):\n    currents = jx.datapoint_to_step_currents(i_delay=1.0, i_dur=1.0, i_amp=inputs / 10.0, delta_t=dt, t_max=t_max)\n\n    data_stimuli = None\n    data_stimuli = net.cell(0).branch(2).loc(1.0).data_stimulate(currents[0], data_stimuli=data_stimuli)\n    data_stimuli = net.cell(1).branch(2).loc(1.0).data_stimulate(currents[1], data_stimuli=data_stimuli)\n\n    return jx.integrate(net, params=params, data_stimuli=data_stimuli, checkpoint_lengths=checkpoints)\n\nbatched_simulate = vmap(simulate, in_axes=(None, 0))\n\n\ndef predict(params, inputs):\n    traces = simulate(params, inputs)  # Shape `(batchsize, num_recordings, timepoints)`.\n    prediction = jnp.mean(traces[2])  # Use the average over time of the output neuron (2) as prediction.\n    return prediction + 72.0  # Such that the prediction is around 0.\n\nbatched_predict = vmap(predict, in_axes=(None, 0))\n\n\ndef loss(opt_params, inputs, labels):\n    params = transform.forward(opt_params)\n\n    predictions = batched_predict(params, inputs)\n    losses = jnp.abs(predictions - labels)  # Mean absolute error loss.\n    return jnp.mean(losses)  # Average across the batch.\n\njitted_grad = jit(value_and_grad(loss, argnums=0))\n</code></pre>"},{"location":"tutorial/07_gradient_descent/#training","title":"Training","text":"<p>We will use the ADAM optimizer from the optax library to optimize the free parameters (you have to install the package with <code>pip install optax</code> first):</p> <pre><code>import optax\n</code></pre> <pre><code>opt_params = transform.inverse(params)\noptimizer = optax.adam(learning_rate=0.01)\nopt_state = optimizer.init(opt_params)\n</code></pre>"},{"location":"tutorial/07_gradient_descent/#writing-a-dataloader","title":"Writing a dataloader","text":"<p>Below, we just write our own (very simple) dataloader. Alternatively, you could use the dataloader from any deep learning library such as pytorch or tensorflow:</p> <pre><code>class Dataset:\n    def __init__(self, inputs: np.ndarray, labels: np.ndarray):\n        \"\"\"Simple Dataloader.\n\n        Args:\n            inputs: Array of shape (num_samples, num_dim)\n            labels: Array of shape (num_samples,)\n        \"\"\"\n        assert len(inputs) == len(labels), \"Inputs and labels must have same length\"\n        self.inputs = inputs\n        self.labels = labels\n        self.num_samples = len(inputs)\n        self._rng_state = None\n        self.batch_size = 1\n\n    def shuffle(self, seed=None):\n        \"\"\"Shuffle the dataset in-place\"\"\"\n        self._rng_state = np.random.get_state()[1][0] if seed is None else seed\n        np.random.seed(self._rng_state)\n        indices = np.random.permutation(self.num_samples)\n        self.inputs = self.inputs[indices]\n        self.labels = self.labels[indices]\n        return self\n\n    def batch(self, batch_size):\n        \"\"\"Create batches of the data\"\"\"\n        self.batch_size = batch_size\n        return self\n\n    def __iter__(self):\n        self.shuffle(seed=self._rng_state)\n        for start in range(0, self.num_samples, self.batch_size):\n            end = min(start + self.batch_size, self.num_samples)\n            yield self.inputs[start:end], self.labels[start:end]\n        self._rng_state += 1\n</code></pre>"},{"location":"tutorial/07_gradient_descent/#training-loop","title":"Training loop","text":"<pre><code>batch_size = 4\ndataloader = Dataset(inputs, labels)\ndataloader = dataloader.shuffle(seed=0).batch(batch_size)\n\nfor epoch in range(10):\n    epoch_loss = 0.0\n\n    for batch_ind, batch in enumerate(dataloader):\n        current_batch, label_batch = batch\n        loss_val, gradient = jitted_grad(opt_params, current_batch, label_batch)\n        updates, opt_state = optimizer.update(gradient, opt_state)\n        opt_params = optax.apply_updates(opt_params, updates)\n        epoch_loss += loss_val\n\n    print(f\"epoch {epoch}, loss {epoch_loss}\")\n\nfinal_params = transform.forward(opt_params)\n</code></pre> <pre><code>epoch 0, loss 22.65623100330507\nepoch 1, loss 17.646464026707072\nepoch 2, loss 11.067483916489753\nepoch 3, loss 7.757597445307745\nepoch 4, loss 6.991337400856902\nepoch 5, loss 6.614704674024435\nepoch 6, loss 6.559548399999545\nepoch 7, loss 6.477586586556327\nepoch 8, loss 6.427426195029805\nepoch 9, loss 6.4088411856655165\n</code></pre> <pre><code>ntest = 32\npredictions = batched_predict(final_params, inputs[:ntest])\n</code></pre> <pre><code>fig, ax = plt.subplots(1, 1, figsize=(3, 2))\n_ = ax.scatter(labels[:ntest], predictions)\n_ = ax.set_xlabel(\"Label\")\n_ = ax.set_ylabel(\"Prediction\")\n</code></pre> <p>Indeed, the loss goes down and the network successfully classifies the patterns.</p>"},{"location":"tutorial/07_gradient_descent/#summary","title":"Summary","text":"<p>Puh, this was a pretty dense tutorial with a lot of material. You should have learned how to:</p> <ul> <li>compute the gradient with respect to parameters  </li> <li>use parameter transformations  </li> <li>use multi-level checkpointing  </li> <li>define optimizers  </li> <li>write dataloaders and parallelize across data  </li> </ul> <p>This was the last \u201cbasic\u201d tutorial of the <code>Jaxley</code> toolbox. If you want to learn more, check out our Advanced Tutorials. If anything is still unclear please create a discussion. If you find any bugs, please open an issue. Happy coding!</p>"},{"location":"tutorial/08_importing_morphologies/","title":"Working with morphologies","text":"<p>In this tutorial, you will learn how to:</p> <ul> <li>Load morphologies and make them compatible with <code>Jaxley</code> </li> <li>Use the visualization features  </li> <li>Assemble a small network of morphologically accurate cells.  </li> <li>Delete parts of a morphology.</li> <li>Connect two morphologies into one.</li> </ul> <p>Here is a code snippet which you will learn to understand in this tutorial:</p> <pre><code>import matplotlib.pyplot as plt\nimport jaxley as jx\nfrom jaxley.morphology import morph_delete, morph_connect\n\n# Read cell from SWC.\ncell = jx.read_swc(\"my_cell.swc\", ncomp=1)\n\n# Plot the cell morphology.\ncell.vis()\nplt.axis(\"square\")\n\n# If needed, Jaxley provides utilities to edit cells, as shown below.\n\n# Delete the apical dendrite.\ncell = morph_delete(cell.apical)\n\n# Attach a \"stub\" to the cell.\nstub = jx.Cell()\ncell = morph_connect(cell.branch(0).loc(0.0), stub.branch(0).loc(0.0))\n\n# Modify the number of compartments of a branch.\ncell.branch(2).set_ncomp(4)\n</code></pre> <p>Let\u2019s get started!</p>"},{"location":"tutorial/08_importing_morphologies/#importing-swc-files","title":"Importing SWC files","text":"<p>To work with <code>.swc</code> files, <code>Jaxley</code> implements a custom <code>.swc</code> reader. The reader traces the morphology and identifies all uninterrupted sections. These uninterrupted sections are called <code>branches</code> in <code>Jaxley</code>. Each <code>branch</code> is then further partitioned into <code>compartments</code>.</p> <p>To demonstrate this, let\u2019s import an example morphology of a Layer 5 pyramidal cell and visualize it.</p> <pre><code>import jaxley as jx\n\n# import swc file into jx.Cell object\nfname = \"data/morph.swc\"\ncell = jx.read_swc(fname, ncomp=1)  # Use one compartment per branch. We modify this below.\n\n# print shape (num_branches, num_comps)\nprint(cell.shape)\n\ncell.show()\n</code></pre> <pre><code>(155, 155)\n</code></pre> local_comp_index global_comp_index local_branch_index global_branch_index local_cell_index global_cell_index 0 0 0 0 0 0 0 1 0 1 1 1 0 0 2 0 2 2 2 0 0 3 0 3 3 3 0 0 4 0 4 4 4 0 0 ... ... ... ... ... ... ... 150 0 150 150 150 0 0 151 0 151 151 151 0 0 152 0 152 152 152 0 0 153 0 153 153 153 0 0 154 0 154 154 154 0 0 <p>155 rows \u00d7 6 columns</p> <p>As we can see, this yields a morphology that is approximated by 157 compartments. The above assigns one compartment to every branch (<code>ncomp=1</code>). To use a different number of compartments in individual branches, you can use <code>.set_ncomp()</code>:</p> <pre><code>cell.branch(1).set_ncomp(4)\n</code></pre> <p>As you can see below, branch <code>0</code> has two compartments (because this is what was passed to <code>jx.read_swc(..., ncomp=2)</code>), but branch <code>1</code> has four compartments:</p> <pre><code>cell.branch([0, 1]).nodes\n</code></pre> local_cell_index local_branch_index local_comp_index length radius axial_resistivity capacitance v x y z apical basal soma global_cell_index global_branch_index global_comp_index controlled_by_param 0 0 0 0 244.227045 0.257375 5000.0 1.0 -70.0 -38.81406 -467.221741 61.146923 True False False 0 0 0 0 1 0 1 0 1.068485 0.400000 5000.0 1.0 -70.0 -34.76659 -399.802795 12.197484 True False False 0 1 1 1 2 0 1 1 1.068485 0.400000 5000.0 1.0 -70.0 -34.76659 -399.802795 12.197484 True False False 0 1 2 1 3 0 1 2 1.068485 0.400000 5000.0 1.0 -70.0 -34.76659 -399.802795 12.197484 True False False 0 1 3 1 4 0 1 3 1.068485 0.400000 5000.0 1.0 -70.0 -34.76659 -399.802795 12.197484 True False False 0 1 4 1 <p>To automatically choose the number of compartments, see the how-to guide on the d_lambda rule.</p>"},{"location":"tutorial/08_importing_morphologies/#visualization","title":"Visualization","text":"<p>Once imported the compartmentalized morphology can be viewed using <code>vis</code>.  </p> <pre><code>import matplotlib.pyplot as plt\n\ncell.vis()\nplt.axis(\"off\")\nplt.title(\"L5PC\")\nplt.axis(\"square\")\nplt.show()\n</code></pre> <p></p> <p><code>vis</code> can be called on any <code>jx.Module</code> and every <code>View</code> of the module. This means we can also for example use <code>vis</code> to highlight each branch. This can be done by iterating over each branch index and calling <code>cell.branch(i).vis()</code>. Within the loop.</p> <pre><code>fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n# define colorwheel with 10 colors\ncolors = plt.cm.tab10.colors\nfor i, branch in enumerate(cell.branches):\n    branch.vis(ax=ax, color=colors[i % 10])\nplt.axis(\"off\")\nplt.title(\"Branches\")\nplt.axis(\"square\")\nplt.show()\n</code></pre> <p></p> <p>While we only use two compartments to approximate each branch in this example, we can see the morphology is still plotted in great detail. This is because we always plot the full <code>.swc</code> reconstruction irrespective of the number of compartments used. The morphology lives separately in the <code>cell.xyzr</code> attribute in a per branch fashion. </p> <p>In addition to plotting the full morphology of the cell using points <code>vis(type=\"scatter\")</code> or lines <code>vis(type=\"line\")</code>, <code>Jaxley</code> also supports plotting a detailed morphological <code>vis(type=\"morph\")</code> or approximate compartmental reconstruction <code>vis(type=\"comp\")</code> that correctly considers the thickness of the neurite. Note that <code>\"comp\"</code> plots the lengths of each compartment which is equal to the length of the traced neurite. While neurites can be zigzaggy, the compartments that approximate them are straight lines. This can lead to miss-alignment of the compartment ends. For details see the documentation of <code>vis</code>. </p> <p>The morphologies can either be projected onto 2D or also rendered in 3D. </p> <pre><code># visualize the cell\nfig, ax = plt.subplots(1, 4, figsize=(7, 4), layout=\"constrained\", sharex=True, sharey=True)\ncell.vis(ax=ax[0], type=\"morph\", dims=[0,1])\ncell.vis(ax=ax[1], type=\"comp\", dims=[0,1])\ncell.vis(ax=ax[2], type=\"scatter\", dims=[0,1], s=1)\ncell.vis(ax=ax[3], type=\"line\", dims=[0,1])\nfig.suptitle(\"Comparison of plot types\")\nfor i in range(4):\n    ax[i].set_aspect(\"equal\", adjustable=\"box\")\nplt.show()\n</code></pre> <p></p> <pre><code># plot in 3D\nfig = plt.figure()\nax = fig.add_subplot(111, projection=\"3d\")\ncell.vis(ax=ax, type=\"line\", dims=[2,0,1])\nax.view_init(elev=20, azim=5)\nax.set_aspect(\"equal\", adjustable=\"box\")\nplt.show()\n</code></pre> <p></p> <p>Since <code>Jaxley</code> supports grouping different branches or compartments together, we can also use the <code>id</code> labels provided by the <code>.swc</code> file to assign group labels to the <code>jx.Cell</code> object.</p> <pre><code>print(cell.group_names)\n\nfig, ax = plt.subplots(1, 1, figsize=(5, 5))\ncell.soma.vis(ax=ax, color=\"orange\")\ncell.apical.vis(ax=ax, color=\"blue\")\ncell.basal.vis(ax=ax, color=\"green\")\nax.set_aspect(\"equal\")\nplt.axis(\"off\")\nplt.show()\n</code></pre> <pre><code>['apical', 'basal', 'soma']\n</code></pre> <p></p>"},{"location":"tutorial/08_importing_morphologies/#editing-morphologies","title":"Editing morphologies","text":"<p><code>Jaxley</code> provides functionality to edit morphologies. In particular, it provides functions to delete parts of a morphology or to connect two cell morphologies into a single cell.</p> <p>\u26a0\ufe0f IMPORTANT! If you edit morphologies, please do so before you change the number of compartments per branch (via, e.g., <code>cell.branch(0).set_ncomp(4)</code>). In addition, you must delete all recordings, stimuli, and trainable parameters before running <code>morph_delete()</code> or <code>morph_connect()</code>.</p> <p>Below, we will show how you can delete all apical branches of a morphology. To do so, we first import the morphology:</p> <pre><code>import jaxley as jx\n\nfname = \"data/morph.swc\"\ncell = jx.read_swc(fname, ncomp=1)\n</code></pre> <p>We then use the <code>morph_delete</code> method to delete parts of the morphology:</p> <pre><code>from jaxley.morphology import morph_delete\n\n# Creates a new cell which has the apical dendrite deleted.\ncell = morph_delete(cell.apical)\n</code></pre> <p>To check if everything worked, we visualize the resulting morphology (soma highlighted in red):</p> <pre><code>fig, ax = plt.subplots(1, 1, figsize=(4, 4))\nax = cell.vis(ax=ax)\nax = cell.soma.vis(ax=ax, color=\"r\")\nax.set_aspect(\"equal\")\nplt.axis(\"off\")\n</code></pre> <pre><code>(-130.92000000000002, 152.44, -21.766, 205.186)\n</code></pre> <p></p> <p><code>Jaxley</code> also provides functionality to attach two morphologies. This is useful to, for example, replace the axon with a \u201cstub\u201d. Below, we show how one can attach two morphologies. To this end, we first create a small \u201cstub\u201d consisting of just a single compartment, which we will attach to the morphology above.</p> <p>\u26a0\ufe0f IMPORTANT! You must use the same <code>ncomp</code> for both morphologies! You can always modify the number of compartments per branch with <code>cell.branch(0).set_ncomp(n)</code> afterwards.</p> <pre><code># Create a \"stub\" of 50um length from scratch.\nstub = jx.Cell()\nstub.set(\"length\", 50.0)\nstub.add_to_group(\"stub\")  # Such that we can do `cell.stub` after `morph_connect`.\n\n# Rotate the stub. This is only used for visualization.\nstub.compute_xyz()\nstub.rotate(90)\n</code></pre> <p>Once the two morphologies are defined, we can use <code>morph_attach</code> to combine two cells into one:</p> <pre><code>from jaxley.morphology import morph_connect\n\nnew_cell = morph_connect(cell.branch(0).loc(0.0), stub.branch(0).loc(0.0))\n</code></pre> <p>Let\u2019s visualize the cell before and after having added the stub:</p> <pre><code>import matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 2, figsize=(7, 3))\n_ = cell.vis(ax=ax[0])\n_ = new_cell.vis(ax=ax[1])\n_ = new_cell.stub.vis(ax=ax[1], color=\"r\")  # .stub only works because we did `add_to_group(\"stub\")` above.\n_ = ax[0].set_aspect(\"equal\")\n_ = ax[1].set_aspect(\"equal\")\n</code></pre> <p></p> <p>Indeed, the new morphology has an additional stub, which we highlighted in red.</p>"},{"location":"tutorial/08_importing_morphologies/#assembling-cells-into-a-network","title":"Assembling cells into a network","text":"<p>To build a network of morphologically detailed cells, we can now connect several reconstructed cells together and also visualize the network. However, since all cells are going to have the same center, <code>Jaxley</code> will naively plot all of them on top of each other. To separate out the cells, we therefore have to move them to a new location first.</p> <pre><code>from jaxley.synapses import IonotropicSynapse\n\nfname = \"data/morph.swc\"\ncell = jx.read_swc(fname, ncomp=1)\n\nnet = jx.Network([cell]*5)\njx.connect(net.cell(0).soma.branch(0).comp(0), net[2,0,0], IonotropicSynapse())\njx.connect(net.cell(0).soma.branch(0).comp(0), net[3,0,0], IonotropicSynapse())\njx.connect(net.cell(0).soma.branch(0).comp(0), net[4,0,0], IonotropicSynapse())\n\njx.connect(net.cell(1).soma.branch(0).comp(0), net[2,0,0], IonotropicSynapse())\njx.connect(net.cell(1).soma.branch(0).comp(0), net[3,0,0], IonotropicSynapse())\njx.connect(net.cell(1).soma.branch(0).comp(0), net[4,0,0], IonotropicSynapse())\n\nnet.rotate(-90)\n\nnet.cell(0).move(0, 900)\nnet.cell(1).move(0, 1500)\n\nnet.cell(2).move(900, 600)\nnet.cell(3).move(900, 1200)\nnet.cell(4).move(900, 1800)\n\nnet.vis()\nplt.axis(\"off\")\nplt.axis(\"square\")\nplt.show()\n</code></pre> <p></p> <p>Congrats! You have now learned how to visualize and build networks out of very complex morphologies. To simulate this network, you can follow the steps in the tutorial on how to build a network. If you are a power-user and need more flexibility for editing or inspecting cell morphologies (e.g., trim dendrites), check out the tutorial on Jaxley\u2019s graph backend.</p>"},{"location":"tutorial/09_advanced_indexing/","title":"Customizing synaptic parameters","text":"<p>In this tutorial, you will learn how to:</p> <ul> <li>use the <code>select()</code> method to fully customize network simulations with <code>Jaxley</code>.  </li> <li>use the <code>copy_node_property_to_edges()</code> method to flexibly modify synapses.  </li> </ul> <p>Here is a code snippet which you will learn to understand in this tutorial: <pre><code>net = ...  # See tutorial on Basics of Jaxley.\n\n# Set synaptic conductance of the synapse with index 0 and 1.\nnet.select(edges=[0, 1]).set(\"Ionotropic_gS\", 0.1)\n\n# Set synaptic conductance of all synapses that have cells 3 or 4 as presynaptic neuron.\nnet.copy_node_property_to_edges(\"global_cell_index\")\ndf = net.edges\ndf = df.query(\"pre_global_cell_index in [3, 4]\")\nnet.select(edges=df.index).set(\"Ionotropic_gS\", 0.2)\n\n# Set synaptic conductance of all synapses that\n# 1) have cells 2 or 3 as presynaptic neuron and\n# 2) has cell 5 as postsynaptic neuron\ndf = net.edges\ndf = df.query(\"pre_global_cell_index in [2, 3]\")\ndf = df.query(\"post_global_cell_index == 5\")\nnet.select(edges=df.index).set(\"Ionotropic_gS\", 0.3)\n</code></pre></p> <p>In a previous tutorial you learned how to set parameters of a <code>jx.Network</code>. In that tutorial, we briefly mentioned the <code>select()</code> method which allowed to set individual synapses to particular values. In this tutorial, we will go into detail in how you can fully customize your <code>Jaxley</code> simulation.</p> <p>Let\u2019s go!</p> <pre><code>import jaxley as jx\nfrom jaxley.channels import Na, K, Leak\nfrom jaxley.connect import fully_connect\nfrom jaxley.synapses import IonotropicSynapse\n</code></pre>"},{"location":"tutorial/09_advanced_indexing/#preface-building-the-network","title":"Preface: Building the network","text":"<p>We first build a network consisting of six neurons, in the same way as we showed in the previous tutorials:</p> <pre><code>dt = 0.025\nt_max = 10.0\n\ncomp = jx.Compartment()\nbranch = jx.Branch(comp, ncomp=2)\ncell = jx.Cell(branch, parents=[-1, 0])\nnet = jx.Network([cell for _ in range(6)])\nfully_connect(net.cell([0, 1, 2]), net.cell([3, 4, 5]), IonotropicSynapse())\n</code></pre>"},{"location":"tutorial/09_advanced_indexing/#setting-individual-synapse-parameters","title":"Setting individual synapse parameters","text":"<p>As always, you can use the <code>.edges</code> table to inspect synaptic parameters of the network:</p> <pre><code>net.edges\n</code></pre> global_edge_index pre_index post_index type type_ind pre_locs post_locs IonotropicSynapse_gS IonotropicSynapse_e_syn IonotropicSynapse_k_minus IonotropicSynapse_v_th IonotropicSynapse_delta IonotropicSynapse_s controlled_by_param 0 0 0 15 IonotropicSynapse 0 0.25 0.25 0.0001 0.0 0.025 -35.0 10.0 0.2 0 1 1 0 20 IonotropicSynapse 0 0.25 0.25 0.0001 0.0 0.025 -35.0 10.0 0.2 0 2 2 0 25 IonotropicSynapse 0 0.25 0.25 0.0001 0.0 0.025 -35.0 10.0 0.2 0 3 3 5 15 IonotropicSynapse 0 0.25 0.25 0.0001 0.0 0.025 -35.0 10.0 0.2 0 4 4 5 20 IonotropicSynapse 0 0.25 0.25 0.0001 0.0 0.025 -35.0 10.0 0.2 0 5 5 5 25 IonotropicSynapse 0 0.25 0.25 0.0001 0.0 0.025 -35.0 10.0 0.2 0 6 6 10 15 IonotropicSynapse 0 0.25 0.25 0.0001 0.0 0.025 -35.0 10.0 0.2 0 7 7 10 20 IonotropicSynapse 0 0.25 0.25 0.0001 0.0 0.025 -35.0 10.0 0.2 0 8 8 10 25 IonotropicSynapse 0 0.25 0.25 0.0001 0.0 0.025 -35.0 10.0 0.2 0 <p>This table has nine rows, each corresponding to one synapse. This makes sense because we fully connected three neurons (0, 1, 2) to three other neurons (3, 4, 5), giving a total of <code>3x3=9</code> synapses.</p> <p>You can modify parameters of individual synapses as follows:</p> <pre><code>net.select(edges=[3, 4, 5]).set(\"IonotropicSynapse_gS\", 0.2)\n</code></pre> <p>Above, we are modifying the synapses with indices <code>[3, 4, 5]</code> (i.e., the indices of the <code>net.edges</code> DataFrame). The resulting values are indeed changed:</p> <pre><code>net.edges.IonotropicSynapse_gS\n</code></pre> <pre><code>0    0.0001\n1    0.0001\n2    0.0001\n3    0.2000\n4    0.2000\n5    0.2000\n6    0.0001\n7    0.0001\n8    0.0001\nName: IonotropicSynapse_gS, dtype: float64\n</code></pre>"},{"location":"tutorial/09_advanced_indexing/#example-1-setting-synaptic-parameters-which-connect-particular-neurons","title":"Example 1: Setting synaptic parameters which connect particular neurons","text":"<p>This is great, but setting synaptic parameters just by their index can be exhausting, in particular in very large networks. Instead, we would want to, for example, set the maximal conductance of all synapses that connect from cell 0 or 1 to any other neuron.</p> <p>In <code>Jaxley</code>, such customization can be achieved by filtering the <code>.edges</code> dataframe accordingly, as shown below:</p> <pre><code>net = jx.Network([cell for _ in range(6)])\nfully_connect(net.cell([0, 1, 2]), net.cell([3, 4, 5]), IonotropicSynapse())\n\nnet.copy_node_property_to_edges(\"global_cell_index\")\ndf = net.edges\ndf = df.query(\"pre_global_cell_index in [0, 1]\")\nnet.select(edges=df.index).set(\"IonotropicSynapse_gS\", 0.23)\n</code></pre> <pre><code>net.edges.IonotropicSynapse_gS\n</code></pre> <pre><code>0    0.2300\n1    0.2300\n2    0.2300\n3    0.2300\n4    0.2300\n5    0.2300\n6    0.0001\n7    0.0001\n8    0.0001\nName: IonotropicSynapse_gS, dtype: float64\n</code></pre> <p>Indeed, the first six synapses now have the value <code>0.23</code>! Let\u2019s look at the individual lines to understand how this worked:</p> <p>We want to set parameter by cell index. However, by default, the pre- or post-synaptic cell-indices are not listed in <code>net.edges</code>. We can add the cell index to the <code>.edges</code> dataframe by calling <code>.copy_node_property_to_edges()</code>: <pre><code>net.copy_node_property_to_edges(\"global_cell_index\")\n</code></pre></p> <p>After this, the pre- and post-synaptic cell indices are listed in <code>net.edges</code> as <code>pre_global_cell_index</code> and <code>post_global_cell_index</code>.</p> <p>Next, we take <code>.edges</code>, which is a pandas DataFrame: <pre><code>df = net.edges\n</code></pre></p> <p>We then modify this DataFrame to only contain those rows where the global cell index is in 0 or 1: <pre><code>df = df.query(\"pre_global_cell_index in [0, 1]\")\n</code></pre></p> <p>For the above step, you use any column of the DataFrame to filter it (you can see all columns with <code>df.columns</code>). Note that, while we used <code>.query()</code> here, you can really filter the pandas DataFrame however you want. For example, the <code>query</code> above is identical to <code>df = df[df[\"pre_global_cell_index\"].isin([0, 1])]</code>.</p> <p>Finally, we use the <code>.select()</code> method, which returns a subset of the <code>Network</code> at the specified indices. This subset of the network can be modified with <code>.set()</code>: <pre><code>net.select(edges=df.index).set(\"IonotropicSynapse_gS\", 0.23)\n</code></pre></p>"},{"location":"tutorial/09_advanced_indexing/#example-2-setting-parameters-given-pre-and-post-synaptic-cell-indices","title":"Example 2: Setting parameters given pre- and post-synaptic cell indices","text":"<p>Say you want to select all synapses that have cells 1 or 2 as presynaptic neuron and cell 4 or 5 as postsynaptic neuron.</p> <pre><code>net = jx.Network([cell for _ in range(6)])\nfully_connect(net.cell([0, 1, 2]), net.cell([3, 4, 5]), IonotropicSynapse())\n</code></pre> <p>Just like before, we can simply use <code>.query()</code> as already shown above. However, this time, call <code>.query()</code> to twice to filter by pre- and post-synaptic cell indices:</p> <pre><code>net.copy_node_property_to_edges(\"global_cell_index\")\n\ndf = net.edges\ndf = df.query(\"pre_global_cell_index in [1, 2]\")\ndf = df.query(\"post_global_cell_index in [4, 5]\")\nnet.select(edges=df.index).set(\"IonotropicSynapse_gS\", 0.3)\n</code></pre> <pre><code>net.edges.IonotropicSynapse_gS\n</code></pre> <pre><code>0    0.0001\n1    0.0001\n2    0.0001\n3    0.0001\n4    0.3000\n5    0.3000\n6    0.0001\n7    0.3000\n8    0.3000\nName: IonotropicSynapse_gS, dtype: float64\n</code></pre>"},{"location":"tutorial/09_advanced_indexing/#example-3-applying-this-strategy-to-cell-level-parameters","title":"Example 3: Applying this strategy to cell level parameters","text":"<p>You had previously seen that you can modify parameters with, e.g., <code>net.cell(0).set(...)</code>. However, if you need more flexibility than this, you can also use the above strategy to modify cell-level parameters:</p> <pre><code>net = jx.Network([cell for _ in range(6)])\nfully_connect(net.cell([0, 1, 2]), net.cell([3, 4, 5]), IonotropicSynapse())\n\ndf = net.nodes\ndf = df.query(\"global_cell_index in [0, 1]\")\nnet.select(nodes=df.index).set(\"radius\", 0.1)\n</code></pre>"},{"location":"tutorial/09_advanced_indexing/#example-4-flexibly-setting-parameters-based-on-their-groups","title":"Example 4: Flexibly setting parameters based on their <code>groups</code>","text":"<p>If you are using groups, as shown in this tutorial, then you can also use this for querying synapses. To demonstrate this, let\u2019s create a group of excitatory neurons (e.g., cells 0, 3, 5):</p> <pre><code># Redefine network.\nnet = jx.Network([cell for _ in range(6)])\nfully_connect(net.cell([0, 1, 2]), net.cell([3, 4, 5]), IonotropicSynapse())\n\nnet.cell([0, 3, 5]).add_to_group(\"exc\")\n</code></pre> <p>Now, say we want all synapses that start from these excitatory neurons. You can do this as follows:</p> <pre><code># First, we have to identify which cells are in the `exc` group.\nindices_of_excitatory_cells = net.exc.nodes[\"global_cell_index\"].unique().tolist()  # [0, 3, 5]\n\n# Then we can proceed as before:\nnet.copy_node_property_to_edges(\"global_cell_index\")\ndf = net.edges\ndf = df.query(f\"pre_global_cell_index in {indices_of_excitatory_cells}\")\nnet.select(edges=df.index).set(\"IonotropicSynapse_gS\", 0.4)\n</code></pre>"},{"location":"tutorial/09_advanced_indexing/#example-5-setting-synaptic-parameters-based-on-properties-of-the-presynaptic-cell","title":"Example 5: Setting synaptic parameters based on properties of the presynaptic cell","text":"<p>Let\u2019s discuss one more example: Imagine we only want to modify those synapses whose presynaptic compartment has a sodium channel. Let\u2019s first add a sodium channel to some of the cells:</p> <pre><code>net = jx.Network([cell for _ in range(6)])\nfully_connect(net.cell([0, 1, 2]), net.cell([3, 4, 5]), IonotropicSynapse())\n\nnet.cell(0).branch(0).comp(0).insert(Na())\nnet.cell(2).branch(1).comp(1).insert(Na())\n</code></pre> <p>Now, let us query which cells have the desired synapses:</p> <pre><code>net.copy_node_property_to_edges(\"global_comp_index\")\n\ndf = net.nodes\ndf = df.query(\"Na\")\nindices_of_sodium_compartments = df[\"global_comp_index\"].unique().tolist()\n</code></pre> <p><code>indices_of_sodium_compartments</code> lists all compartments which contained sodium:</p> <pre><code>print(indices_of_sodium_compartments)\n</code></pre> <pre><code>[0, 11]\n</code></pre> <p>Then, we can proceed as always and filter for the global pre-synaptic compartment index:</p> <pre><code>df = net.edges\ndf = df.query(f\"pre_global_comp_index in {indices_of_sodium_compartments}\")\nnet.select(edges=df.index).set(\"IonotropicSynapse_gS\", 0.6)\n</code></pre> <pre><code>net.edges.IonotropicSynapse_gS\n</code></pre> <pre><code>0    0.6000\n1    0.6000\n2    0.6000\n3    0.0001\n4    0.0001\n5    0.0001\n6    0.0001\n7    0.0001\n8    0.0001\nName: IonotropicSynapse_gS, dtype: float64\n</code></pre> <p>Indeed, only synapses coming from the first neuron were modified (as its presynaptic compartment contained sodium), in contrast to synapses from neuron 2 (whose presynaptic compartment did not).</p>"},{"location":"tutorial/09_advanced_indexing/#summary","title":"Summary","text":"<p>In this tutorial, you learned how to fully customize your <code>Jaxley</code> simulation. This works by querying rows from the <code>.edges</code> DataFrame.</p>"},{"location":"tutorial/10_advanced_parameter_sharing/","title":"Synaptic parameter sharing","text":"<p>In this tutorial, you will learn how to:</p> <ul> <li>flexibly share parameters of synapses  </li> </ul> <p>Here is a code snippet which you will learn to understand in this tutorial: <pre><code>net = ...  # See tutorial on Basics of Jaxley.\n\n# The same parameter for all synapses\nnet.make_trainable(\"Ionotropic_gS\")\n\n# An individual parameter for every synapse.\nnet.select(edges=\"all\").make_trainable(\"Ionotropic_gS\")\n\n# Share synaptic conductances emerging from the same neurons.\nnet.copy_node_property_to_edges(\"cell_index\")\nsub_net = net.select(edges=[0, 1, 2])\nsub_net.edges[\"controlled_by_param\"] = sub_net.edges[\"pre_global_cell_index\"]\nsub_net.make_trainable(\"Ionotropic_gS\")\n</code></pre></p> <p>In a previous tutorial about training networks, we briefly touched on parameter sharing. In this tutorial, we will show you how you can flexibly share parameters within a network.</p> <pre><code>import jaxley as jx\nfrom jaxley.channels import Na, K, Leak\nfrom jaxley.connect import fully_connect\nfrom jaxley.synapses import IonotropicSynapse\n</code></pre>"},{"location":"tutorial/10_advanced_parameter_sharing/#preface-building-the-network","title":"Preface: Building the network","text":"<p>We first build a network consisting of six neurons, in the same way as we showed in the previous tutorials:</p> <pre><code>dt = 0.025\nt_max = 10.0\n\ncomp = jx.Compartment()\nbranch = jx.Branch(comp, ncomp=2)\ncell = jx.Cell(branch, parents=[-1, 0])\nnet = jx.Network([cell for _ in range(6)])\nfully_connect(net.cell([0, 1, 2]), net.cell([3, 4, 5]), IonotropicSynapse())\n</code></pre>"},{"location":"tutorial/10_advanced_parameter_sharing/#sharing-parameters-by-modifying-controlled_by_param","title":"Sharing parameters by modifying <code>controlled_by_param</code>","text":"<pre><code>net.copy_node_property_to_edges(\"global_cell_index\")\n\ndf = net.edges\ndf = df.query(\"pre_global_cell_index in [0, 1, 2]\")\nsubnetwork = net.select(edges=df.index)\n\ndf = subnetwork.edges\ndf[\"controlled_by_param\"] = df[\"pre_global_cell_index\"]\nsubnetwork.make_trainable(\"IonotropicSynapse_gS\")\n</code></pre> <pre><code>Number of newly added trainable parameters: 3. Total number of trainable parameters: 3\n</code></pre> <p>Let\u2019s look at this line by line. First, we exactly follow the previous tutorial in selecting the synapses which we are interested in training (i.e., the ones whose presynaptic neuron has index 0, 1, 2):</p> <pre><code>df = net.edges\ndf = df.query(\"pre_global_cell_index in [0, 1, 2]\")\nsubnetwork = net.select(edges=df.index)\n</code></pre> <p>As second step, we enable parameter sharing. This is done by setting the <code>controlled_by_param</code>. Synapses that have the same value in <code>controlled_by_param</code> will be shared. Let\u2019s inspect <code>controlled_by_param</code> before we modify it:</p> <pre><code>subnetwork.edges[[\"pre_global_cell_index\", \"controlled_by_param\"]]\n</code></pre> pre_global_cell_index controlled_by_param 0 0 0 1 0 1 2 0 2 3 1 3 4 1 4 5 1 5 6 2 6 7 2 7 8 2 8 <p>Every synapse has a different value. Because of this, no synaptic parameters will be shared. To enable parameter sharing we override the <code>controlled_by_param</code> column with the presynaptic cell index:</p> <pre><code>df = subnetwork.edges\ndf[\"controlled_by_param\"] = df[\"pre_global_cell_index\"]\n</code></pre> <pre><code>df[[\"pre_global_cell_index\", \"controlled_by_param\"]]\n</code></pre> pre_global_cell_index controlled_by_param 0 0 0 1 0 0 2 0 0 3 1 1 4 1 1 5 1 1 6 2 2 7 2 2 8 2 2 <p>Now, all we have to do is to make these synaptic parameters trainable with the <code>make_trainable()</code> method:</p> <pre><code>subnetwork.make_trainable(\"IonotropicSynapse_gS\")\n</code></pre> <pre><code>Number of newly added trainable parameters: 3. Total number of trainable parameters: 6\n</code></pre> <p>It correctly says that we added three parameters (because we have three cells, and we share individual synaptic parameters). We now have 6 trainable parameters in total (because we already added 3 trainable parameters above).</p>"},{"location":"tutorial/10_advanced_parameter_sharing/#a-more-involved-example-sharing-by-pre-and-post-synaptic-cell-type","title":"A more involved example: sharing by pre- and post-synaptic cell type","text":"<p>As an example, consider the following: We have a fully connected network of six cells. Each cell falls into one of three cell types:</p> <pre><code>from typing import Union, List\n</code></pre> <pre><code>net = jx.Network([cell for _ in range(6)])\nfully_connect(net.cell(\"all\"), net.cell(\"all\"), IonotropicSynapse())\n\nnet.cell([0, 1]).add_to_group(\"exc\")\nnet.cell([2, 3]).add_to_group(\"inh\")\nnet.cell([4, 5]).add_to_group(\"unknown\")\n</code></pre> <p>We want to make all synapses that start from excitatory or inhibitory neurons trainable. In addition, we want to use the same parameter for synapses if they have the same pre- and post-synaptic cell type.</p> <p>To achieve this, we will first want a column in <code>net.nodes</code> which indicates the cell type. </p> <pre><code>net.nodes[\"cell_type\"] = net.nodes[[\"exc\", \"inh\", \"unknown\"]].idxmax(axis=1)\n</code></pre> <pre><code>net.nodes[\"cell_type\"]\n</code></pre> <pre><code>0         exc\n1         exc\n2         exc\n3         exc\n4         exc\n5         exc\n6         exc\n7         exc\n8         inh\n9         inh\n10        inh\n11        inh\n12        inh\n13        inh\n14        inh\n15        inh\n16    unknown\n17    unknown\n18    unknown\n19    unknown\n20    unknown\n21    unknown\n22    unknown\n23    unknown\nName: cell_type, dtype: object\n</code></pre> <p>The <code>cell_type</code> is now part of the <code>net.nodes</code>. However, we would like to do parameter sharing of synapses based on the pre- and post-synaptic node values. To do so, we import the <code>cell_type</code> column into <code>net.edges</code>. To do this, we use the <code>.copy_node_property_to_edges()</code> which the name of the property you are copying from nodes: </p> <pre><code>net.copy_node_property_to_edges(\"cell_type\")\n</code></pre> <p>After this, you have columns in the <code>.edges</code> which indicate the pre- and post-synaptic cell type:</p> <pre><code>net.edges[[\"pre_cell_type\", \"post_cell_type\"]]\n</code></pre> pre_cell_type post_cell_type 0 exc exc 1 exc exc 2 exc inh 3 exc inh 4 exc unknown 5 exc unknown 6 exc exc 7 exc exc 8 exc inh 9 exc inh 10 exc unknown 11 exc unknown 12 inh exc 13 inh exc 14 inh inh 15 inh inh 16 inh unknown 17 inh unknown 18 inh exc 19 inh exc 20 inh inh 21 inh inh 22 inh unknown 23 inh unknown 24 unknown exc 25 unknown exc 26 unknown inh 27 unknown inh 28 unknown unknown 29 unknown unknown 30 unknown exc 31 unknown exc 32 unknown inh 33 unknown inh 34 unknown unknown 35 unknown unknown <p>Next, we specify which parts of the network we actually want to change (in this case, all synapses which have excitatory or inhibitory presynaptic neurons):</p> <pre><code>df = net.edges\ndf = df.query(f\"pre_cell_type in ['exc', 'inh']\")\nprint(f\"There are {len(df)} synapses to be changed.\")\n\nsubnetwork = net.select(edges=df.index)\n</code></pre> <pre><code>There are 24 synapses to be changed.\n</code></pre> <p>As the last step, we again have to specify parameter sharing by setting <code>controlled_by_param</code>. In this case, we want to share parameters that have the same pre- and post-synaptic neuron. We achieve this by grouping the synpases by their pre- and post-synaptic cell type (see pd.DataFrame.groupby for details):</p> <pre><code># Step 6: use groupby to specify parameter sharing and make the parameters trainable.\nsubnetwork.edges[\"controlled_by_param\"] = subnetwork.edges.groupby([\"pre_cell_type\", \"post_cell_type\"]).ngroup()\nsubnetwork.make_trainable(\"IonotropicSynapse_gS\")\n</code></pre> <pre><code>Number of newly added trainable parameters: 6. Total number of trainable parameters: 6\n</code></pre> <p>This created six trainable parameters, which makes sense as we have two types of pre-synaptic neurons (excitatory and inhibitory) and each has three options for the postsynaptic neuron (pre, post, unknown).</p>"},{"location":"tutorial/10_advanced_parameter_sharing/#summary","title":"Summary","text":"<p>In this tutorial, you learned how you can flexibly share synaptic parameters. This works by first using <code>select()</code> to identify which synapses to make trainable, and by then modifying <code>controlled_by_param</code> to customize parameter sharing.</p>"},{"location":"tutorial/11_ion_dynamics/","title":"Ion dynamics","text":"<p>In this tutorial, you will learn how to:</p> <ul> <li>define ion pumps  </li> <li>update reversal potentials with the Nernst equation  </li> <li>diffuse ions within the cell  </li> </ul> <p>Here is a code snippet which you will learn to understand in this tutorial: <pre><code>import jaxley as jx\nfrom jaxley.pumps import CaPump, CaNernstReversal\nfrom jaxley_mech.channels.l5pc import CaHVA\n\n\nbranch = jx.Branch()\ncell = jx.Cell(branch, parents=[-1, 0, 0])\n\n# Insert a voltage-gated calcium channel.\ncell.insert(CaHVA())\n\n# Insert a mechanism which modifies the intracellular calcium based on the calcium current.\ncell.insert(CaPump())\n\n# Insert a mechanism that updates the calcium reversal potential based on the intracellular calcium level.\ncell.insert(CaNernstReversal())\n\n# Let the intracellular calcium diffuse within the cell.\ncell.diffuse(\"CaCon_i\")\ncell.set(\"axial_diffusion_CaCon_i\", 2.0)\n\n# Record the intracellular calcium concentration and simulate.\ncell.record(\"CaCon_i\")\ncacon_i = jx.integrate(cell, t_max=100.0, delta_t=0.025)\n</code></pre></p> <p>This tutorial assumes that you have already learned how to build basic simulations. It is also helpful to have read the tutorial on how to build ion channel models in Jaxley.</p> <pre><code>from jax import config\nconfig.update(\"jax_enable_x64\", True)\nconfig.update(\"jax_platform_name\", \"cpu\")\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport jax\nimport jax.numpy as jnp\nfrom jax import jit\n\nimport jaxley as jx\nfrom jaxley.channels import Na, K, Leak\n</code></pre> <p>First, we define a cell as you saw in the previous tutorial and insert sodium, potassium, and leak ion channels:</p> <pre><code>comp = jx.Compartment()\nbranch = jx.Branch(comp, ncomp=4)\ncell = jx.Cell(branch, parents=[-1, 0, 0, 1, 1, 2, 2])\ncell.insert(Na())\ncell.insert(K())\ncell.insert(Leak())\n</code></pre> <p>In this tutorial, we will set up a neuron with detailed calcium dynamcis. We will define all channels and pumps from scratch, but you could also just import these channels and run: <pre><code>from jaxley.channels import CaCurrentToConcentrationChange, CaNernstPotential\nfrom jaxley_mech.channels.l5pc import CaHVA\n\ncell.insert(CaHVA())  # Insert a voltage-gated calcium channel.\ncell.insert(CaCurrentToConcentrationChange())  # Modify intracellular calcium based on Ca-current.\ncell.insert(CaNernstPotential())  # Insert a mechanism that updates the calcium reversal potential.\n</code></pre></p>"},{"location":"tutorial/11_ion_dynamics/#a-voltage-gated-calcium-channel","title":"A voltage-gated calcium channel","text":"<p>Let\u2019s start by defining a voltage-gated calcium ion channel. This is done as is described in the tutorial on building channel models:</p> <pre><code>from jaxley.channels import Channel\nfrom jaxley.solver_gate import solve_gate_exponential, save_exp\n\nclass CaHVA(Channel):\n    \"\"\"High-Voltage-Activated (HVA) Ca2+ channel\"\"\"\n\n    def __init__(self, name=None):\n        self.current_is_in_mA_per_cm2 = True\n        super().__init__(name)\n        self.channel_params = {\n            f\"{self._name}_gCaHVA\": 0.00001,  # S/cm^2\n        }\n        self.channel_states = {\n            f\"{self._name}_m\": 0.1,  # Initial value for m gating variable\n            f\"{self._name}_h\": 0.1,  # Initial value for h gating variable\n            \"eCa\": 0.0,  # Initial value for reversal potential, mV.\n        }\n        self.current_name = f\"i_Ca\"\n\n    def update_states(self, states, dt, voltages, params):\n        \"\"\"Update state of gating variables.\"\"\"\n        prefix = self._name\n        ms, hs = states[f\"{prefix}_m\"], states[f\"{prefix}_h\"]\n        m_new = solve_gate_exponential(ms, dt, *self.m_gate(voltages))\n        h_new = solve_gate_exponential(hs, dt, *self.h_gate(voltages))\n        return {f\"{prefix}_m\": m_new, f\"{prefix}_h\": h_new, \"eCa\": states[\"eCa\"]}\n\n    def compute_current(self, states, voltages, params):\n        \"\"\"Compute the current through the channel.\"\"\"\n        prefix = self._name\n        ms, hs = states[f\"{prefix}_m\"], states[f\"{prefix}_h\"]\n        ca_cond = params[f\"{prefix}_gCaHVA\"] * (ms**2) * hs\n        current = ca_cond * (voltages - states[\"eCa\"])\n        return current\n\n    def init_state(self, states, voltages, params, delta_t):\n        \"\"\"Initialize the state such at fixed point of gate dynamics.\"\"\"\n        prefix = self._name\n        alpha_m, beta_m = self.m_gate(voltages)\n        alpha_h, beta_h = self.h_gate(voltages)\n        return {\n            f\"{prefix}_m\": alpha_m / (alpha_m + beta_m),\n            f\"{prefix}_h\": alpha_h / (alpha_h + beta_h),\n        }\n\n    @staticmethod\n    def m_gate(v):\n        \"\"\"Voltage-dependent dynamics for the m gating variable.\"\"\"\n        alpha = (0.055 * (-27 - v + 1e-6)) / (save_exp((-27.0 - v + 1e-6) / 3.8) - 1.0)\n        beta = 0.94 * save_exp((-75.0 - v + 1e-6) / 17.0)\n        return alpha, beta\n\n    @staticmethod\n    def h_gate(v):\n        \"\"\"Voltage-dependent dynamics for the h gating variable.\"\"\"\n        alpha = 0.000457 * save_exp((-13.0 - v) / 50.0)\n        beta = 0.0065 / (save_exp((-v - 15.0) / 28.0) + 1.0)\n        return alpha, beta\n</code></pre> <p>Note two things: - we named the current through this channel as <code>i_Ca</code>. This happens by defining <code>self.current_name = \"i_Ca\"</code> and will be useful to define the pump (which depends on the calcium current) - the current through this channel depends on the reversal potential (named <code>eCa</code>): <code>current = ca_cond * (voltages - u[\"eCa\"])</code>. The reversal potential will later on be updated based on the intracellular calcium concentration.</p> <p>Let\u2019s insert this channel into our cell:</p> <pre><code>cell.insert(CaHVA())\n</code></pre>"},{"location":"tutorial/11_ion_dynamics/#a-calcium-ion-pump","title":"A calcium ion pump","text":"<p>Next, we would like to modify the intracellular calcium ion concentration and then update the calcium reversal potential. In <code>Jaxley</code>, everything that modifies intracellular ion concentration is called a <code>Pump</code>. A <code>Pump</code> is very similar to a <code>Channel</code>, with two differences: - it has to define which ion concentration (Ca, Na,\u2026) should be modified (by setting <code>self.ion_name = \"CaCon_i\"</code>) - its <code>compute_current</code> method will modify the ion concentration, not the membrane voltage (as would be the case for a <code>Channel</code>).</p> <p>Let\u2019s define a <code>Pump</code> for calcium ions:</p> <pre><code>from jaxley.pumps import Pump\n\n\nclass CaPump(Pump):\n    \"\"\"Calcium dynamics tracking inside calcium concentration.\"\"\"\n\n    def __init__(self, name=None):\n        super().__init__(name)\n        self.channel_params = {\n            f\"{self._name}_gamma\": 0.05,  # Fraction of free calcium (not buffered).\n            f\"{self._name}_decay\": 80,  # Buffering time constant in ms.\n            f\"{self._name}_depth\": 0.1,  # Depth of shell in um.\n            f\"{self._name}_minCai\": 1e-4,  # Minimum intracell. ca concentration in mM.\n        }\n        self.channel_states = {\"i_Ca\": 1e-8, \"CaCon_i\": 5e-05}\n        self.ion_name = \"CaCon_i\"\n        self.current_name = \"i_CaPump\"\n\n    def update_states(self, states, dt, v, params):\n        \"\"\"Update states if necessary (but this pump has no states to update).\"\"\"\n        return {\"CaCon_i\": states[\"CaCon_i\"], \"i_Ca\": states[\"i_Ca\"]}\n\n    def compute_current(self, states, modified_state, params):\n        \"\"\"Return change of calcium concentration based on calcium current and decay.\"\"\"\n        prefix = self._name\n        ica = states[\"i_Ca\"]\n        gamma = params[f\"{prefix}_gamma\"]\n        decay = params[f\"{prefix}_decay\"]\n        depth = params[f\"{prefix}_depth\"]\n        minCai = params[f\"{prefix}_minCai\"]\n\n        FARADAY = 96485  # Coulombs per mole.\n\n        # Calculate the contribution of calcium currents to cai change.\n        drive_channel = -10_000.0 * ica * gamma / (2 * FARADAY * depth)\n        state_decay = (modified_state - minCai) / decay\n        diff = drive_channel - state_decay\n        return -diff\n\n    def init_state(self, states, v, params, delta_t):\n        \"\"\"Initialize states of channel.\"\"\"\n        return {}\n</code></pre> <p>As you can see, the <code>CaPump</code> defines an <code>ion_name</code>: <pre><code>self.ion_name = \"CaCon_i\"\n</code></pre> As we had hinted at before, the current that modifies the intracellular calcium concentration depends on the current through the <code>CaHVA</code> channel. In the <code>CaHVA</code> channel, we had defined <code>self.current_name = \"i_Ca\"</code>, so we can access this current in the <code>compute_current()</code> method of the <code>CaPump</code>: <pre><code>ica = states[\"i_Ca\"]\n</code></pre></p> <p>Let\u2019s add the calcium pump to the cell:</p> <pre><code>cell.insert(CaPump())\n</code></pre> <p>If you do not want mechanisms such as calcium buffering (which are included in the model above), but you simply want to convert the calcium current to a change in intracellular concentration, use: <pre><code>from jaxley.pumps import CaFaradayConcentrationChange\ncell.insert(CaFaradayConcentrationChange())\n</code></pre></p>"},{"location":"tutorial/11_ion_dynamics/#updating-the-calcium-reversal-potential","title":"Updating the calcium reversal potential","text":"<p>The <code>CaPump</code> modifies the intracellular calcium ion concentration. So far, however, updating the intracellular calcium ion concentration does not impact the voltage dynamics at all (feel free to check the <code>CaHVA</code> channel above: neither its <code>update_states()</code> nor its <code>compute_current()</code> directly depend on <code>states[\"CaCon_i\"]</code>). To change this, we would like to modify the calcium reversal potential <code>eCa</code> based on the intracellular calcium concentration (again, check the <code>CaHVA</code> channel: its <code>compute_current()</code> does depend on <code>states[\"eCa\"]</code>).</p> <p>The update to the reversal potential is done via the Nernst equation, which itself can be implemented via a <code>Channel</code> in <code>Jaxley</code>. For example:</p> <pre><code>class CaNernstReversal(Channel):\n    \"\"\"Compute Calcium reversal from inner and outer concentration of calcium.\"\"\"\n\n    def __init__(self, name=None):\n        self.current_is_in_mA_per_cm2 = True\n        super().__init__(name)\n        self.channel_constants = {\n            \"F\": 96485.3329,  # C/mol (Faraday's constant)\n            \"T\": 279.45,  # Kelvin (temperature)\n            \"R\": 8.314,  # J/(mol K) (gas constant)\n        }\n        self.channel_params = {}\n        self.channel_states = {\"eCa\": 0.0, \"CaCon_i\": 5e-05, \"CaCon_e\": 2.0}\n        self.current_name = f\"i_Ca\"\n\n    def update_states(self, states, dt, voltages, params):\n        \"\"\"Update internal calcium concentration based on calcium current and decay.\"\"\"\n        R, T, F = (\n            self.channel_constants[\"R\"],\n            self.channel_constants[\"T\"],\n            self.channel_constants[\"F\"],\n        )\n        Cai = states[\"CaCon_i\"]\n        Cao = states[\"CaCon_e\"]\n        C = R * T / (2 * F) * 1000  # mV\n        vCa = C * jnp.log(Cao / Cai)\n        return {\"eCa\": vCa, \"CaCon_i\": Cai, \"CaCon_e\": Cao}\n\n    def compute_current(self, states, voltages, params):\n        \"\"\"This dynamics model does not directly contribute to the membrane current.\"\"\"\n        return 0\n\n    def init_state(self, states, voltages, params, delta_t):\n        \"\"\"Initialize the state at fixed point of gate dynamics.\"\"\"\n        return {}\n</code></pre> <p>The <code>CaNernstReversal</code> modifies the reversal potential <code>eCa</code> in its <code>update_states</code> method.</p> <p>We can insert this mechanism as always:</p> <pre><code>cell.insert(CaNernstReversal())\n</code></pre>"},{"location":"tutorial/11_ion_dynamics/#ion-diffusion","title":"Ion diffusion","text":"<p>In principle, we could already run this simulation. Optionally, we can add one more feature: ion diffusion. Ion diffusion can be turned on with the <code>.diffuse()</code> method in <code>Jaxley</code> (if you do not run the <code>.diffuse()</code> method, then ions do not diffuse). In this case, we would like to diffuse the intracellular calcium concentration:</p> <pre><code>cell.diffuse(\"CaCon_i\")\n</code></pre> <p>We can define how strongly calcium diffuses by setting its axial resistivity:</p> <pre><code>cell.set(\"axial_diffusion_CaCon_i\", 1.0)  # Higher values -&gt; More diffusion.\n</code></pre> <p>\u26a0\ufe0f IMPORTANT! <code>axial_diffusion_CaCon_i</code> must be strictly positive in the entire cell. We do not allow <code>0.0</code>, but you can use small values like <code>1e-8</code>.</p>"},{"location":"tutorial/11_ion_dynamics/#running-the-simulation","title":"Running the simulation","text":"<p>We can now record voltage or intracellular calcium concentration from our cell, stimulate it with a step current, and simulate:</p> <pre><code>t_max = 20.0\ndt = 0.025\n\ncell.delete_recordings()\ncell.delete_stimuli()\n\ncell.branch(0).comp(0).record(\"v\")\ncell.branch(0).comp(0).record(\"CaCon_i\")\ncell.branch(0).comp(0).stimulate(jx.step_current(1.0, 10.0, 0.03, dt, t_max))\nv_and_ca = jx.integrate(cell, delta_t=dt, t_max=t_max)\n</code></pre> <pre><code>Added 1 recordings. See `.recordings` for details.\nAdded 1 recordings. See `.recordings` for details.\nAdded 1 external_states. See `.externals` for details.\n</code></pre> <pre><code>time_vec = np.arange(0, t_max + dt * 2, dt)\nfig, ax = plt.subplots(1, 2, figsize=(8, 2))\n_ = ax[0].plot(v_and_ca[0])\n_ = ax[1].plot(v_and_ca[1])\n_ = ax[0].set_ylabel(\"Voltage\")\n_ = ax[1].set_ylabel(\"Intracellular Ca\")\n</code></pre> <p></p> <p>That\u2019s it! In this tutorial, you should have learned how to model detailed intracellular ion (in particular calcium) dynamics.</p>"},{"location":"tutorial/12_simplified_models/","title":"Simplified models","text":"<p>In this tutorial, you will learn how to:</p> <ul> <li>use <code>Jaxley</code> to simulate simplified neuron models such as:  </li> <li>Leaky-integrate-and-fire neuron models </li> <li>Izhikevich neuron models</li> <li>Rate-based neuron models (unit-free)  </li> <li>define your own simplified neuron models  </li> </ul> <p>Here is a code snippet which you will learn to understand in this tutorial: <pre><code># Leaky integrate-and-fire neurons.\nfrom jaxley.channels import Leak, Fire\ncell = jx.Cell()\ncell.insert(Leak())\ncell.insert(Fire())\ncell.record(\"v\")\ncell.record(\"Fire_spikes\")\n\n# Izhikevich neuron models.\nfrom jaxley.channels import Izhikevich\ncell = jx.Cell()\ncell.insert(Izhikevich())\ncell.record(\"v\")\n\n# Rate-based neuron models (unit-free).\nfrom jaxley.channels import Rate\ncell = jx.Cell()\ncell.set(\"length\", 1.0 / (2 * pi * 1e-5))  # Make external current and synapses unit-free.\ncell.insert(Rate())\ncell.record(\"v\")\n</code></pre></p> <p><code>Jaxley</code> focuses on biophysical, Hodgkin-Huxley-type models. These models follow the equation</p> \\[ \\frac{1}{C} \\frac{\\text{d}V}{\\text{d}t} = i, \\] <p>where \\(C\\) is the capacitance, \\(V\\) is the membrane voltage, and \\(i\\) is a membrane current which is linear in the voltage, for example \\(i=g_{\\text{Leak}} (E_{\\text{Leak}} - V)\\). All Hodkgin-Huxley-type models fall into this category, but many simplified neuron models do not follow these equations. Nonetheless, <code>Jaxley</code> can also simulate such simpler neuron models. This tutorial will teach you how to do this.</p> <pre><code>import matplotlib.pyplot as plt\nimport jax.numpy as jnp\n\nimport jaxley as jx\n</code></pre>"},{"location":"tutorial/12_simplified_models/#pre-configured-simplified-models","title":"Pre-configured simplified models","text":"<p>We will first go over three popular, pre-configured neuron models (LIF, Izhikevich, rate-based).</p>"},{"location":"tutorial/12_simplified_models/#leaky-integrate-and-fire-neuron-models","title":"Leaky integrate-and-fire neuron models","text":"<p>Leaky integrate-and-fire (LIF) neurons follow the equation:</p> \\[ \\frac{1}{C} \\frac{\\text{d}V}{\\text{d}t} = g_{\\text{Leak}} (E_{\\text{Leak}} - V), \\] <p>with the reset condition:</p> \\[ \\text{if } V \\geq 30 \\text{ mV, then }  V \\leftarrow V_{\\text{reset}} \\\\ \\] <p>The first equation is consistent with the Hodgkin-Huxley mechanism (with only a leak channel), but the reset is not. To implement LIF neurons, run:</p> <pre><code>from jaxley.channels import Leak, Fire\ncell = jx.Cell()\ncell.insert(Leak())\ncell.insert(Fire())\ncell.record(\"v\")\ncell.record(\"Fire_spikes\")\n</code></pre> <pre><code>Added 1 recordings. See `.recordings` for details.\nAdded 1 recordings. See `.recordings` for details.\n\n\n/Users/michaeldeistler/Documents/phd/jaxley/jaxley/channels/non_capacitive/spike.py:29: UserWarning: The `Fire` channel does not support surrogate gradients. Its gradient will be zero after every spike.\n  warn(\n</code></pre> <p>As the warning says, <code>Jaxley</code> does not yet support optimizing LIF neuron models with gradient descent because it does not yet implement surrogate gradients. We can still simulate LIF neurons as always:</p> <pre><code>dt = 0.1\nt_max = 40.0\n\ncell.stimulate(jx.step_current(5.0, 20.0, 0.005, dt, t_max))\nv = jx.integrate(cell, delta_t=dt)\ntime_vec = jnp.arange(0, t_max + 2 * dt, dt)\nfig, ax = plt.subplots(2, 1, figsize=(6, 4))\n_ = ax[0].plot(time_vec, v[0])\n_ = ax[1].plot(time_vec, v[1])\n_ = ax[0].set_ylabel(\"Voltage (mV)\")\n_ = ax[1].set_ylabel(\"Spikes\")\n_ = ax[1].set_xlabel(\"Time (ms)\")\n</code></pre> <pre><code>Added 1 external_states. See `.externals` for details.\n</code></pre> <p></p>"},{"location":"tutorial/12_simplified_models/#izhikevich-neuron-models","title":"Izhikevich neuron models","text":"<p>Finally, Izhikevich neuron models follow the equation:</p> \\[ \\frac{\\text{d}V}{\\text{d}t} = 0.04V^2 + 5V + 140 - u + I \\] \\[ \\frac{\\text{d}u}{\\text{d}t} = a (bV - u) \\] <p>with the reset condition:</p> \\[ \\text{if } V \\geq 30 \\text{ mV, then }  \\begin{cases}  V \\leftarrow c \\\\ u \\leftarrow u + d \\end{cases} \\] <p>The voltage is not consistent with the standard Hodgkin-Huxley equations because: - it does not include a capacitance - the update is not linear in the voltage (\\(0.04 V^2\\))</p> <p>To implement this equation in <code>Jaxley</code>, run:</p> <pre><code>from jaxley.channels import Izhikevich\ncell = jx.Cell()\ncell.insert(Izhikevich())\ncell.record(\"v\")\n</code></pre> <pre><code>Added 1 recordings. See `.recordings` for details.\n\n\n/Users/michaeldeistler/Documents/phd/jaxley/jaxley/channels/non_capacitive/izhikevich.py:28: UserWarning: The `Izhikevich` channel does not support surrogate gradients. Its gradient will be zero after every spike.\n  warn(\n</code></pre> <p>Again, <code>Jaxley</code> does not yet support optimizing Izhikevich neuron models with gradient descent (because of the non-differentiable reset). Let\u2019s simulate this model:</p> <pre><code>dt = 0.1\nt_max = 200.0\n\ncell.stimulate(jx.step_current(10.0, 180.0, 0.012, dt, t_max))\nv = jx.integrate(cell, delta_t=dt)\ntime_vec = jnp.arange(0, t_max + 2 * dt, dt)\nfig, ax = plt.subplots(1, 1, figsize=(5, 3))\n_ = plt.plot(time_vec, v.T)\n</code></pre> <pre><code>Added 1 external_states. See `.externals` for details.\n</code></pre> <p></p>"},{"location":"tutorial/12_simplified_models/#rate-based-neuron-models","title":"Rate-based neuron models","text":"<p>Rate-based neuron models follow the equation</p> \\[ \\frac{1}{\\tau} \\frac{\\text{d}V}{\\text{d}t} = -V. \\] <p>A rate-based neuron model is, in principle, consistent with Hodgkin-Huxley equations, and it could be implemented with a <code>Leak</code> channel in <code>Jaxley</code>. However, rate-based neuron models are sometimes assumed to be unit free. To implement a unit-free rate-based neuron model in <code>Jaxley</code>, use the <code>Rate</code> mechanism and define <code>radius</code> and <code>length</code> of the cell such that stimuli or synaptic inputs become unit free:</p> <pre><code>from jaxley.channels import Rate\nfrom math import pi\n\ncell = jx.Cell()\n# If `length = 1 / (2 * pi * 1e-5) um` and `radius = 1.0um`, then `area = 1e5 um^2 = 1e-3 cm2`.\n# The `1e-3` corrects for stimuli being in `nA`, but voltage updates being in `mV * ms = uA`.\n# As such, setting `length = 1 / (2 * pi * 1e-5) um` and `radius = 1.0 um` effectively makes\n# the external currents or synaptic currents unit-free.\ncell.set(\"length\", 1.0 / (2 * pi * 1e-5))\ncell.set(\"radius\", 1.0)  # 1.0 is also the default.\ncell.insert(Rate())\n</code></pre> <p>Let\u2019s simulate this cell:</p> <pre><code>cell.stimulate(2.0 * jnp.ones((5,)))\ncell.set(\"v\", 2.0)\ncell.record(\"v\")\n\ndt = 1.0\nt_max = 10.0\n\nv = jx.integrate(cell, t_max=t_max, delta_t=dt)\ntime_vec = jnp.arange(0, t_max + 2 * dt, dt)\nfig, ax = plt.subplots(1, 1, figsize=(5, 2))\n_ = plt.plot(time_vec, v.T, marker=\"o\")\n</code></pre> <pre><code>Added 1 external_states. See `.externals` for details.\nAdded 1 recordings. See `.recordings` for details.\n</code></pre> <p></p> <p>The voltage increases during the the first 5 timesteps, because we are stimulating the cell during that time, and then quickly drops off to zero.</p>"},{"location":"tutorial/12_simplified_models/#how-to-implement-your-own-simplified-models","title":"How to implement your own simplified models","text":"<p>If the models above do not offer enough flexibility for your usecase, you can also implement mechanisms yourself. For this, it is helpful if you first read the tutorial on how to build channel models in <code>Jaxley</code>.</p> <p>As you learned in this tutorial, every channel returns a current in its <code>compute_current</code> method. This is the current that turns up on the right side of the Hodgkin-Huxley equation</p> \\[ \\frac{1}{C} \\frac{\\text{d}V}{\\text{d}t} = i. \\] <p>If you specifically want to update voltages in a way that does not fall within this equation, you should instead update voltages in the <code>update_states()</code> method. Let\u2019s have a look at how the reset in a leaky integrate-and-fire neuron is implemented:</p> <pre><code>from jaxley.channels import Channel\n\n\nclass Fire(Channel):\n    \"\"\"Mechanism to reset the voltage when it crosses a threshold.\"\"\"\n\n    def __init__(self, name = None):\n        self.current_is_in_mA_per_cm2 = True\n        super().__init__(name)\n        self.channel_params = {f\"{self.name}_vth\": -50, f\"{self.name}_vreset\": -70}\n        self.channel_states = {}\n        self.current_name = f\"{self.name}_fire\"\n\n    def update_states(self, states, dt, v, params):\n        \"\"\"Reset the voltage when a spike occurs and log the spike\"\"\"\n        prefix = self._name\n        vreset = params[f\"{prefix}_vreset\"]\n        vth = params[f\"{prefix}_vth\"]\n        v = jax.lax.select(v &gt; vth, vreset, v)\n        return {\"v\": v}\n\n    def compute_current(self, states, v, params):\n        return jnp.zeros((1,))\n\n    def init_state(self, states, v, params, delta_t):\n        return {}\n</code></pre> <p>As you can see, this channel directly modifies voltages <code>v</code> in the <code>update_states</code> method. Beyond this, it returns zero current for the Hodgkin-Huxley update (but the <code>Leak</code> channel does perform a Hodgkin-Huxley-type update).</p> <p>That\u2019s it, you should now be able to define your own simplified neuron models in <code>Jaxley</code>! For each of these models, you can now connect multiple neurons in a network. You can also build networks in which some neurons are simplified and others follow Hodgkin-Huxley dynamics (or even include morphological detail). Have fun!</p>"},{"location":"tutorial/13_graph_backend/","title":"The graph backend","text":"<p>In this tutorial, you will learn how to use <code>Jaxley</code>s graph pipeline, which offers interoperability with <code>networkX</code>. We will: - define morphologies via <code>networkX</code> graphs. - export morphologies to <code>networkX</code> graphs. - import morphologies using <code>Jaxley</code>\u2019s graph pipeline.</p> <p>Here is a code snippet which you will learn to understand in this tutorial:</p> <pre><code>import jaxley as jx\nimport networkx as nx\nfrom jaxley.io.graph import (\n    to_swc_graph, build_compartment_graph, vis_compartment_graph, from_graph\n)\nfrom jaxley.modules.base import to_graph\n\n# Import cell from SWC via the graph-backend.\nswc_graph = to_swc_graph(\"tests/swc_files/morph.swc\")\ncomp_graph = build_compartment_graph(swc_graph, ncomp=1)\ncell = from_graph(comp_graph)\n\n# Export cell to the graph-backend.\ncomp_graph = to_graph(cell)\nvis_compartment_graph(comp_graph)\n</code></pre> <p>While <code>swc</code> is a great way to save, load and specify complex morphologies, often more flexibility is needed. In these cases, graphs present a natural way to represent and work with neural morphologies, allowing for easy fixing, pruning, smoothing and traversal of neural morphologies. For this purpose, <code>Jaxley</code>  comes with a <code>networkX</code> toolset that allows for easy interoperability between <code>networkX</code> graphs and <code>Jaxley</code> Modules.</p> <p>To work with complex morphologies, <code>jaxley.io.graph</code> provides a way to import <code>swc</code> reconstructions as graphs:</p> <pre><code>from jaxley.io.graph import to_swc_graph\n\nfname = \"data/morph.swc\"\nswc_graph = to_swc_graph(fname)\n</code></pre> <p>A major advantage of this is that having imported an <code>swc</code> file as a graph allows to fix, prune, or smooth the morphology. As an example, we remove the apical dendrites of a morphology that we read from <code>swc</code>:</p> <pre><code>import networkx as nx\n\n# manipulate the graph\nids = nx.get_node_attributes(swc_graph, \"id\")\nids = {k: v for k, v in ids.items() if v != 4}  # Apical dendrite has `id=4`: http://www.neuronland.org/NLMorphologyConverter/MorphologyFormats/SWC/Spec.html \nswc_graph = nx.subgraph(swc_graph, ids).copy()\n</code></pre> <p>We can then visualize the remaining morphology:</p> <pre><code>pos = {k: (v[\"x\"], v[\"y\"]) for k, v in swc_graph.nodes.items()}\nnx.draw(swc_graph.to_undirected(), pos=pos, node_size=20)\n</code></pre> <p></p> <p>Next, we compartmentalize this graph. To this end, <code>build_compartment_graph()</code> segments the SWC graph into branches and divides the branches up into compartments.</p> <pre><code>from jaxley.io.graph import build_compartment_graph\n\ncomp_graph = build_compartment_graph(swc_graph, ncomp=2)\n</code></pre> <p>You can view the compartment structure as follows:</p> <pre><code>import matplotlib.pyplot as plt\nfrom jaxley.io.graph import vis_compartment_graph\n\nprint(f\"node attributes {comp_graph.nodes[0]}\")\nprint(f\"edge attributes {comp_graph.edges[(0, 1)]}\")\n\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\nvis_compartment_graph(comp_graph, ax=ax)\n</code></pre> <pre><code>node attributes {'x': 2.311556339263916, 'y': -8.913771629333496, 'z': 0.0, 'branch_index': 0, 'comp_index': 0, 'type': 'comp', 'xyzr': array([[  4.06      , -11.45      ,   0.        ,   2.6       ],\n       [  3.        , -10.35      ,   0.        ,   3.86      ],\n       [  2.74      ,  -9.85      ,   0.        ,   4.019     ],\n       [  2.2       ,  -8.67      ,   0.        ,   4.34      ],\n       [  2.17      ,  -8.49      ,   0.        ,   4.34      ],\n       [  1.99      ,  -7.26      ,   0.        ,   5.81      ],\n       [  1.98      ,  -6.        ,   0.        ,   6.28      ],\n       [  1.96899895,  -5.83329182,   0.        ,   6.36039227]]), 'groups': ['soma'], 'radius': 4.542392560157545, 'length': 6.241557163671718, 'cell_index': 0}\nedge attributes {}\n</code></pre> <p></p> <p>In the graph above, red points indicate compartments and orange points indicate branchpoints.</p> <p>Now, let\u2019s import this graph into <code>Jaxley</code>. We use the <code>from_graph()</code> function to convert the graph into a <code>Cell</code> object, which <code>Jaxley</code> can then simulate or optimize.</p> <pre><code>from jaxley.io.graph import build_compartment_graph, from_graph\nfrom jaxley.channels import HH\n\ncell = from_graph(comp_graph)\n\n# The resulting cell can be treated like any Jaxley cell.\n# As an example, we add HH and change parameters for visualization.\ncell.insert(HH())\nfor branch in cell:\n    y_pos = branch.xyzr[0][0,1]\n    branch.set(\"HH_gNa\", 0.5 + 0.5 * y_pos)\n</code></pre> <p><code>Jaxley</code> also offers the option to export any <code>Module</code> to a <code>networkX</code> graph object:</p> <pre><code>from jaxley.modules.base import to_graph\n\ncomp_graph = to_graph(cell, channels=True)\n</code></pre> <p>Exporting a <code>Jaxley</code> cell to a graph provides another way to store or share the current Module state, since <code>to_graph</code> attaches all relevant attributes to the nodes and eges of the graph. It can also be used to make more complex visualizations: for example, we can visualize the channel density of each compartment as below:</p> <pre><code># plot of the cell, coloring each node according to the sodium conductance\npos = {k: (v[\"x\"], v[\"y\"]) for k, v in comp_graph.nodes.items()}\ncolors = []\nfor n in comp_graph.nodes:\n    if \"HH_gNa\" in comp_graph.nodes[n]:\n        # Color of compartments should scale with HH_gNa.\n        colors.append(comp_graph.nodes[n][\"HH_gNa\"])\n    else:\n        # Branchpoints have no channels.\n        colors.append(0.0)\n\nnx.draw(comp_graph.to_undirected(), pos=pos, node_color=colors, cmap=\"viridis\", with_labels=False, node_size=50)\nplt.title(\"Sodium conductance\")\nplt.show()\n</code></pre> <p></p>"},{"location":"tutorial/13_graph_backend/#editing-morphologies","title":"Editing morphologies","text":"<p>To edit morphologies, <code>Jaxley</code> provides <code>delete_morph</code> and <code>attach_morph</code>. If these do not provide enough flexibility, you can use the graph-backend to modify morphologies. As an example, we will trim all tip dendrites that are shorter than 250 \\(\\mu\\)m.</p> <p>First, we import the SWC file as a compartment graph:</p> <pre><code>from jaxley.io.graph import to_swc_graph, build_compartment_graph\n\nswc_graph = to_swc_graph(fname)\ncomp_graph = build_compartment_graph(swc_graph, ncomp=1)\n</code></pre> <p>Let\u2019s visualize it:</p> <pre><code>from jaxley.io.graph import vis_compartment_graph\n\nfig, ax = plt.subplots(1, 1, figsize=(3, 7))\nvis_compartment_graph(comp_graph, ax=ax)\n</code></pre> <p></p> <p>Next, we loop over all nodes. We want to keep nodes only if they made any of the following conditions: - if a node has more than one neighbor (<code>degree &gt; 1</code>), - if its compartment length is &gt; 250 \\(\\mu\\)m, or - if it is a soma.</p> <pre><code>import networkx as nx\n\nnodes_to_keep = []\nfor node in comp_graph.nodes:\n    degree = comp_graph.in_degree(node) + comp_graph.out_degree(node)\n\n    condition1 = degree &gt; 1\n    condition2 = comp_graph.nodes[node][\"length\"] &gt; 250.0\n    condition3 = \"soma\" in comp_graph.nodes[node][\"groups\"]\n    if condition1 or condition2 or condition3:\n        nodes_to_keep.append(node)\n\ncomp_graph = nx.subgraph(comp_graph, nodes_to_keep)\n</code></pre> <p>We can again visualize the trimmed graph:</p> <pre><code>fig, ax = plt.subplots(1, 1, figsize=(3, 7))\nvis_compartment_graph(comp_graph, ax=ax)\n</code></pre> <p></p> <p>After we are done, we can import the graph as a <code>jx.Cell</code> with the <code>from_graph</code> method:</p> <pre><code>from jaxley.io.graph import from_graph\ncell = from_graph(comp_graph)\n</code></pre> <p>\u2026and we can also visualize the remaining morphology:</p> <pre><code>fig, ax = plt.subplots(1, 1, figsize=(3, 7))\n_ = cell.vis(ax=ax)\n</code></pre> <p></p> <p>\u2026and we can run simulations as always:</p> <pre><code>import jaxley as jx\n\ncell.delete_recordings()\ncell.delete_stimuli()\ncell.soma.branch(0).comp(0).record()\ncell.soma.branch(0).comp(0).stimulate(jx.step_current(10.0, 20.0, 0.1, 0.025, 100.0))\n</code></pre> <pre><code>Added 1 recordings. See `.recordings` for details.\nAdded 1 external_states. See `.externals` for details.\n</code></pre> <pre><code>v = jx.integrate(cell)\n</code></pre> <pre><code>fig, ax = plt.subplots(1, 1, figsize=(5, 2))\n_ = ax.plot(v.T)\n</code></pre> <p></p>"},{"location":"tutorial/13_graph_backend/#building-graphs-from-scratch","title":"Building graphs from scratch","text":"<p>Finally, you can even build graphs completely from scratch in <code>networkX</code> and then import them as a <code>Jaxley</code> module:</p> <pre><code>import networkx as nx\n\nnodes = {\n    0: {\"id\":1, \"x\": -1, \"y\": 0, \"z\": 0, \"r\": 0.2},\n    1: {\"id\":1, \"x\": 0, \"y\": 0, \"z\": 0, \"r\": 0.2},\n    2: {\"id\":1, \"x\": 1, \"y\": 0, \"z\": 0, \"r\": 0.2},\n    3: {\"id\":1, \"x\": 2, \"y\": 1, \"z\": 0, \"r\": 0.1},\n    4: {\"id\":1, \"x\": 3, \"y\": 2, \"z\": 0, \"r\": 0.1},\n    5: {\"id\":1, \"x\": 2, \"y\": -1, \"z\": 0, \"r\": 0.1},\n    6: {\"id\":1, \"x\": 3, \"y\": -2, \"z\": 0, \"r\": 0.1},\n} \nedges = ((0, 1),(1, 2),(2, 3),(3, 4),(2, 5),(5, 6))\n\ng = nx.DiGraph()\ng.add_nodes_from(nodes.items())\ng.add_edges_from(edges, l=1)\n\n# Setting any of these attributes is optional. It is sufficient to define the \n# connectivity and simply do g = nx.DiGraph(edges). In this case, r and l will \n# be set to default values and x, y, z can be computed using Cell.compute_xyz().\n\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 1, figsize=(5, 3))\nnx.draw(g.to_undirected(), pos={k: (v[\"x\"], v[\"y\"]) for k, v in nodes.items()}, with_labels=True, ax=ax)\n</code></pre> <p></p> <p>We can then use <code>io.graph</code> to import such a graph into a <code>jx.Module</code> using the <code>from_graph</code> method:</p> <pre><code>from jaxley.io.graph import build_compartment_graph, from_graph\n\ncomp_graph = build_compartment_graph(g, ncomp=2)\ncell = from_graph(comp_graph)\n\ncell.vis()\nplt.show()\n</code></pre> <p></p> <p>Congrats! You have now learned how to interface with <code>networkX</code> to further customize and manipulate imported morphologies.</p>"}]}