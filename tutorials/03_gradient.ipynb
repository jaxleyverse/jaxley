{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc77dd53",
   "metadata": {},
   "source": [
    "# Obtaining the gradient and training (optimizing) the parameters\n",
    "\n",
    "In this tutorial, we will describe how you can use JAX's automatic differentiation to obtain gradients through `jaxley` simulations and how you can use optimize the parameters with the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e596286c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad69e152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have experienced stability issues with float32.\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "config.update(\"jax_platform_name\", \"cpu\")\n",
    "\n",
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\".8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d09b991a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jit, value_and_grad\n",
    "\n",
    "import jaxley as jx\n",
    "from jaxley.channels import HHChannel\n",
    "from jaxley.synapses import GlutamateSynapse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6a8517",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b4f07eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of segments per branch.\n",
    "nseg_per_branch = 8\n",
    "\n",
    "# Stimulus.\n",
    "i_delay = 3.0  # ms\n",
    "i_amp = 0.05  # nA\n",
    "i_dur = 2.0  # ms\n",
    "\n",
    "# Duration and step size.\n",
    "dt = 0.025  # ms\n",
    "t_max = 50.0  # ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bce08e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_vec = jnp.arange(0.0, t_max+dt, dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be214d52",
   "metadata": {},
   "source": [
    "### Let's define a network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba9f5f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = jx.Compartment()\n",
    "branch = jx.Branch([comp for _ in range(nseg_per_branch)])\n",
    "cell = jx.Cell([branch for _ in range(5)], parents=jnp.asarray([-1, 0, 0, 1, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "588688d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = np.random.seed(0)\n",
    "conn_builder = jx.ConnectivityBuilder([cell.total_nbranches for _ in range(5)])\n",
    "connectivities = [jx.Connectivity(GlutamateSynapse(), conn_builder.fc(np.arange(0, 2), np.arange(2, 5)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff784bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = jx.Network([cell for _ in range(5)], connectivities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "222f9a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "network.insert(HHChannel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90affb0c-dc77-47c7-be3c-18e2829cc820",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cell_ind in range(5):\n",
    "    network.cell(cell_ind).branch(1).comp(0.0).record()\n",
    "    \n",
    "current = jx.step_current(i_delay, i_dur, i_amp, dt, t_max)\n",
    "for stim_ind in range(2):\n",
    "    network.cell(stim_ind).branch(1).comp(0.0).stimulate(current)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a4effc",
   "metadata": {},
   "source": [
    "### Defining trainable parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bb1f52",
   "metadata": {},
   "source": [
    "This follows the same API as `.set_params()` seen in the previous tutorial. If you want to use a single parameter for all `radius`es in the entire network, do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10cb5b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of newly added trainable parameters: 1. Total number of trainable parameters: 1\n"
     ]
    }
   ],
   "source": [
    "network.make_trainable(\"radius\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded765bf",
   "metadata": {},
   "source": [
    "We can also define parameters for individual compartments. To do this, use the `\"all\"` key. The following defines a separate parameter the sodium conductance for every compartment in the entire network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c90be7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of newly added trainable parameters: 200. Total number of trainable parameters: 201\n"
     ]
    }
   ],
   "source": [
    "network.cell(\"all\").branch(\"all\").comp(\"all\").make_trainable(\"gNa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d0ab89",
   "metadata": {},
   "source": [
    "### Making synaptic parameters trainable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5811b8",
   "metadata": {},
   "source": [
    "Synaptic parameters can be made trainable in the exact same way. To use a single parameter for all syanptic conductances in the entire network, do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f31901bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of newly added trainable parameters: 1. Total number of trainable parameters: 202\n"
     ]
    }
   ],
   "source": [
    "network.make_trainable(\"gS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84527ee4",
   "metadata": {},
   "source": [
    "and to use a different syanptic conductance for all syanpses, do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12fe7828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of newly added trainable parameters: 6. Total number of trainable parameters: 208\n"
     ]
    }
   ],
   "source": [
    "network.GlutamateSynapse(\"all\").make_trainable(\"gS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfc1c6b",
   "metadata": {},
   "source": [
    "### Running the simulation again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d8a610",
   "metadata": {},
   "source": [
    "Once all parameters are defined, you have to use `.get_parameters()` to obtain all trainable parameters. This is also the time to check how many trainable parameters your network has:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40a48eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = network.get_parameters()\n",
    "# print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf68cf64",
   "metadata": {},
   "source": [
    "You can now run the simulation with the trainable parameters by passing them to the `jx.integrate` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4eb3f8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = jx.integrate(network, delta_t=dt, params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00946772",
   "metadata": {},
   "source": [
    "### Defining a loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9808bb05",
   "metadata": {},
   "source": [
    "Let us define a loss function to be optimized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a29f1ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(params):\n",
    "    s = jx.integrate(network, delta_t=dt, params=params)\n",
    "    return jnp.sum(s[0, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef18ca2",
   "metadata": {},
   "source": [
    "And we can use `JAX`'s inbuilt functions to take the gradient through the entire ODE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f38d61a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "jitted_grad = jit(value_and_grad(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ac97e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "value, gradient = jitted_grad(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dbde13",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "We will use the ADAM optimizer from the [optax library](https://optax.readthedocs.io/en/latest/) to optimize the free parameters (you have to install the package with `pip install optax` first):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9ccf1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61dab27",
   "metadata": {},
   "source": [
    "Before training, however, we will enforce for all parameters to be within a prespecified range (such that, e.g., conductances can not become negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "710a1545",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = jx.ParamTransform(\n",
    "    lowers={\"gNa\": 0.05, \"gK\": 0.01, \"gLeak\": 0.0001, \"radius\": 0.1, \"length\": 1.0, \"axial_resistivity\": 500.0, \"gS\": 0.01}, \n",
    "    uppers={\"gNa\": 1.1, \"gK\": 0.3, \"gLeak\": 0.001, \"radius\": 5.0, \"length\": 20.0, \"axial_resistivity\": 5500.0, \"gS\": 5.0}, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df80cc24",
   "metadata": {},
   "source": [
    "Let's modify the loss function acocrdingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "800f959e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(params):\n",
    "    params = transform.forward(params)\n",
    "    s = jx.integrate(network, delta_t=dt, params=params)\n",
    "    return jnp.sum(s[0, -1])\n",
    "\n",
    "jitted_grad = jit(value_and_grad(loss))\n",
    "value, gradient = jitted_grad(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9de29a",
   "metadata": {},
   "source": [
    "Then we define the optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d639efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_params = transform.inverse(params)\n",
    "optimizer = optax.adam(learning_rate=1e-1)\n",
    "opt_state = optimizer.init(opt_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e4aebd0-283e-4165-8c24-4b6fb811135e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss -64.97740512171988\n",
      "epoch 1, loss -65.03050878143252\n",
      "epoch 2, loss -65.07820463321355\n",
      "epoch 3, loss -65.12091871011332\n",
      "epoch 4, loss -65.15909222980945\n"
     ]
    }
   ],
   "source": [
    "epoch_losses = []\n",
    "\n",
    "for epoch in range(5):\n",
    "    loss_val, gradient = jitted_grad(opt_params)\n",
    "    updates, opt_state = optimizer.update(gradient, opt_state)\n",
    "    opt_params = optax.apply_updates(opt_params, updates)\n",
    "\n",
    "    print(f\"epoch {epoch}, loss {loss_val}\")\n",
    "    epoch_losses.append(loss_val)\n",
    "    \n",
    "final_params = transform.forward(opt_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa2db31",
   "metadata": {},
   "source": [
    "Indeed, the loss goes down."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neurax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
