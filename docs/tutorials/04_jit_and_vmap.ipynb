{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edec0bef",
   "metadata": {},
   "source": [
    "# Speeding up simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69c7e69",
   "metadata": {},
   "source": [
    "In this tutorial, you will learn how to:\n",
    "\n",
    "- make parameter sweeps in `Jaxley`  \n",
    "- use `jit` to compile your simulations and make them faster  \n",
    "- use `vmap` to parallelize simulations on GPUs  \n",
    "\n",
    "Here is a code snippet which you will learn to understand in this tutorial:\n",
    "```python\n",
    "from jax import jit, vmap\n",
    "\n",
    "\n",
    "cell = ...  # See tutorial on Basics of Jaxley.\n",
    "\n",
    "def simulate(params):\n",
    "    param_state = None\n",
    "    param_state = cell.data_set(\"Na_gNa\", params[0], param_state)\n",
    "    param_state = cell.data_set(\"K_gK\", params[1], param_state)\n",
    "    return jx.integrate(cell, param_state=param_state, delta_t=0.025)\n",
    "\n",
    "# Define 100 sets of sodium and potassium conductances.\n",
    "all_params = jnp.asarray(np.random.rand(100, 2))\n",
    "\n",
    "# Fast for-loops with jit compilation.\n",
    "jitted_simulate = jit(simulate)\n",
    "voltages = [jitted_simulate(params) for params in all_params]\n",
    "\n",
    "# Using vmap for parallelization.\n",
    "vmapped_simulate = vmap(jitted_simulate, in_axes=(0,))\n",
    "voltages = vmapped_simulate(all_params)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961add1b",
   "metadata": {},
   "source": [
    "In the previous tutorials, you learned how to build single cells or networks and how to change their parameters. In this tutorial, you will learn how to speed up such simulations by many orders of magnitude. This can be achieved in to ways:\n",
    "\n",
    "- by using JIT compilation  \n",
    "- by using GPU parallelization  \n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4634d043",
   "metadata": {},
   "source": [
    "### Using GPU or CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b3da0b",
   "metadata": {},
   "source": [
    "In `Jaxley` you can set whether you want to use `gpu` or `cpu` with the following lines at the beginning of your script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eecdb54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import config\n",
    "config.update(\"jax_platform_name\", \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93b182d",
   "metadata": {},
   "source": [
    "`JAX` (and `Jaxley`) also allow to choose between `float32` and `float64`. Especially on GPUs, `float32` will be faster, but we have experienced stability issues when simulating morphologically detailed neurons with `float32`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c211ae58",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.update(\"jax_enable_x64\", True)  # Set to false to use `float32`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c19485",
   "metadata": {},
   "source": [
    "Next, we will import relevant libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4771ff43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from jax import jit, vmap\n",
    "\n",
    "import jaxley as jx\n",
    "from jaxley.channels import Na, K, Leak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a134be2",
   "metadata": {},
   "source": [
    "### Building the cell or network\n",
    "\n",
    "We first build a cell (or network) in the same way as we showed in the previous tutorials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "184a8f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 1 external_states. See `.externals` for details.\n",
      "Added 1 recordings. See `.recordings` for details.\n"
     ]
    }
   ],
   "source": [
    "dt = 0.025\n",
    "t_max = 10.0\n",
    "\n",
    "comp = jx.Compartment()\n",
    "branch = jx.Branch(comp, ncomp=4)\n",
    "cell = jx.Cell(branch, parents=[-1, 0, 0, 1, 1, 2, 2])\n",
    "\n",
    "cell.insert(Na())\n",
    "cell.insert(K())\n",
    "cell.insert(Leak())\n",
    "\n",
    "cell.delete_stimuli()\n",
    "current = jx.step_current(i_delay=1.0, i_dur=1.0, i_amp=0.1, delta_t=dt, t_max=t_max)\n",
    "cell.branch(0).loc(0.0).stimulate(current)\n",
    "\n",
    "cell.delete_recordings()\n",
    "cell.branch(0).loc(0.0).record()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a60a0f3",
   "metadata": {},
   "source": [
    "### Parameter sweeps\n",
    "\n",
    "Assume you want to run the same cell with many different values for the sodium and potassium conductance, for example for genetic algorithms or for parameter sweeps. To do this efficiently in `Jaxley`, you have to use the `data_set()` method (in combination with `jit` and `vmap`, as shown later):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f90ef994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(params):\n",
    "    param_state = None\n",
    "    param_state = cell.data_set(\"Na_gNa\", params[0], param_state)\n",
    "    param_state = cell.data_set(\"K_gK\", params[1], param_state)\n",
    "    return jx.integrate(cell, param_state=param_state, delta_t=dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39beb756",
   "metadata": {},
   "source": [
    "The `.data_set()` method takes three arguments: \n",
    "\n",
    "1) the name of the parameter you want to set. `Jaxley` allows to set the following parameters: \"radius\", \"length\", \"axial_resistivity\", as well as all parameters of channels and synapses.  \n",
    "2) the value of the parameter.  \n",
    "3) a `param_state` which is initialized as `None` and is modified by `.data_set()`. This has to be passed to `jx.integrate()`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e1fd58",
   "metadata": {},
   "source": [
    "Having done this, the simplest (but least efficient) way to perform the parameter sweep is to run a for-loop over many parameter sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c032cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voltages.shape (5, 1, 402)\n"
     ]
    }
   ],
   "source": [
    "# Define 5 sets of sodium and potassium conductances.\n",
    "all_params = jnp.asarray(np.random.rand(5, 2))\n",
    "\n",
    "voltages = jnp.asarray([simulate(params) for params in all_params])\n",
    "print(\"voltages.shape\", voltages.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cdf353",
   "metadata": {},
   "source": [
    "The resulting voltages have shape `(num_simulations, num_recordings, num_timesteps)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c22f857",
   "metadata": {},
   "source": [
    "### Stimulus sweeps\n",
    "\n",
    "In addition to running sweeps across multiple parameters, you can also run sweeps across multiple stimuli (e.g. step current stimuli of different amplitudes. You can achieve this with the `data_stimulate()` method:\n",
    "```python\n",
    "def simulate(i_amp):\n",
    "    current = jx.step_current(1.0, 1.0, i_amp, 0.025, 10.0)\n",
    "\n",
    "    data_stimuli = None\n",
    "    data_stimuli = cell.branch(0).comp(0).data_stimulate(current, data_stimuli)\n",
    "    return jx.integrate(cell, data_stimuli=data_stimuli)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21da118c",
   "metadata": {},
   "source": [
    "### Speeding up for loops via `jit` compilation\n",
    "\n",
    "We can speed up such parameter sweeps (or stimulus sweeps) with `jit` compilation. `jit` compilation will compile the simulation when it is run for the first time, such that every other simulation will be must faster. This can be achieved by defining a new function which uses `JAX`'s `jit()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1744ed7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "jitted_simulate = jit(simulate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9ff8393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First run, will be slow.\n",
    "voltages = jitted_simulate(all_params[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b919baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voltages.shape (5, 1, 402)\n"
     ]
    }
   ],
   "source": [
    "# More runs, will be much faster.\n",
    "voltages = jnp.asarray([jitted_simulate(params) for params in all_params])\n",
    "print(\"voltages.shape\", voltages.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba75b9e",
   "metadata": {},
   "source": [
    "`jit` compilation can be up to 10k times faster, especially for small simulations with few compartments. For very large models, the gain obtained with `jit` will be much smaller (`jit` may even provide no speed up at all)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8467051",
   "metadata": {},
   "source": [
    "### Speeding up with GPU parallelization via `vmap`\n",
    "\n",
    "Another way to speed up parameter sweeps is with GPU parallelization. Parallelization in `Jaxley` can be achieved by using `vmap` of `JAX`. To do this, we first create a new function that handles __multiple__ parameter sets directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "189befd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using vmap for parallelization.\n",
    "vmapped_simulate = vmap(jitted_simulate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1fc4f8",
   "metadata": {},
   "source": [
    "We can then run this method on __all__ parameter sets (`all_params.shape == (100, 2)`), and `Jaxley` will automatically parallelize across them. Of course, you will only get a speed-up if you have a GPU available and you specified `gpu` as device in the beginning of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70f63764",
   "metadata": {},
   "outputs": [],
   "source": [
    "voltages = vmapped_simulate(all_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a78465a",
   "metadata": {},
   "source": [
    "GPU parallelization with `vmap` can give a large speed-up, which can easily be 2-3 orders of magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af06c28",
   "metadata": {},
   "source": [
    "### Combining `jit` and `vmap`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5120a2",
   "metadata": {},
   "source": [
    "Finally, you can also combine using `jit` and `vmap`. For example, you can run multiple batches of many parallel simulations. Each batch can be parallelized with `vmap` and simulating each batch can be compiled with `jit`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45c832db",
   "metadata": {},
   "outputs": [],
   "source": [
    "jitted_vmapped_simulate = jit(vmap(simulate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74fa4e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in range(10):\n",
    "    all_params = jnp.asarray(np.random.rand(5, 2))\n",
    "    voltages_batch = jitted_vmapped_simulate(all_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c5609a",
   "metadata": {},
   "source": [
    "That's all you have to know about `jit` and `vmap`! If you have worked through this and the previous tutorials, you should be ready to set up your first network simulations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b6cbce",
   "metadata": {},
   "source": [
    "### Next steps\n",
    "\n",
    "If you want to learn more, we recommend you to read the [tutorial on building channel and synapse models](https://jaxley.readthedocs.io/en/latest/tutorials/05_channel_and_synapse_models.html).\n",
    "\n",
    "Alternatively, you can also directly jump ahead to the [tutorial on training biophysical networks](https://jaxley.readthedocs.io/en/latest/tutorials/07_gradient_descent.html) which will teach you how you can optimize parameters of biophysical models with gradient descent.\n",
    "\n",
    "Finally, if you want to learn more about JAX, check out their [tutorial on jit](https://jax.readthedocs.io/en/latest/jit-compilation.html) or their [tutorial on vmap](https://jax.readthedocs.io/en/latest/automatic-vectorization.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
